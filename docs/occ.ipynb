{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Occupancy models\n",
        "description: 'Static, single-species occupancy models in PyMC'\n",
        "author:\n",
        "  name: Philip T. Patton\n",
        "  affiliation:\n",
        "    - Marine Mammal Research Program\n",
        "    - HawaiÊ»i Institute of Marine Biology\n",
        "date: 'January 18, 2023'\n",
        "format:\n",
        "  html:\n",
        "    html-math-method: mathjax\n",
        "    code-overflow: wrap\n",
        "bibliography: refs.bib\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are many valuable tools for fitting occupancy models. These tools are typically R libraries, such as [unmarked](https://rbchan.github.io/unmarked/), or programs called from R, such as [JAGS](https://www.mbr-pwrc.usgs.gov/pubanalysis/roylebook/) or [Stan](https://fukamilab.github.io/BIO202/09-C-occupancy-models.html). There are relatively fewer examples of how to fit these models in Python. While most ecologists, and arguably statisticians, use R, there are some benefits to using Python generally. For example, despite ecology being a lucrative industry, some of us might have to pivot to another field where Python may be more common. Besides, Python is widely used for machine learning, which is increasingly applied in ecology.  \n",
        "\n",
        "In this notebook, I demonstrate how to fit static site-occupancy models in [PyMC](https://www.pymc.io/welcome.html), a Python library for doing Bayesian data analysis. PyMC has many virtues, including a user-friendly interface, and a direct linkage to the ArviZ library, which has functions for summarizing MCMC output. \n",
        "\n",
        "# Simulated examples\n",
        "\n",
        "The standard site-occupancy model models binary detection/non-detection data $y_{j,k}$ for repeated surveys $k=1,2,\\dots,K$ at sites $j=1,2,\\dots,J.$ The species is present at the sites when $z_j=1,$ and absent otherwise. We assume that our probability of detecting the species given that the site is occupied is  $P(y_{j,k}|z_j=1)=p,$ and zero when the site is unoccupied. The probability of occurrence, which is typically the parameter of interest, is $P(z_{j}=1)=\\psi.$\n",
        "\n",
        "To start, I demonstrate how to simulate detection/non-detection data using numpy. In this first example, I simulate the simplest possible case, where $\\psi$ is constant across all sites and $p$ is constant across all sites and visits. Using . "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import pandas as pd\n",
        "\n",
        "def scale(x):\n",
        "    return (x - np.nanmean(x)) / np.nanstd(x)\n",
        "\n",
        "def invlogit(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sim_y(p, z, site_count, visit_count):\n",
        "    \n",
        "    ones = np.ones((site_count, visit_count))\n",
        "    p_array = p * ones \n",
        "\n",
        "    flips = rng.binomial(1, p_array)\n",
        "    y = (flips.T * z_true).T\n",
        "    \n",
        "    return y \n",
        "\n",
        "## simulation\n",
        "\n",
        "SEED = 808\n",
        "rng = np.random.default_rng(seed=SEED)\n",
        "\n",
        "# sampling characteristics\n",
        "site_count = 200\n",
        "visit_count = 3\n",
        "\n",
        "## ecological model\n",
        "\n",
        "# true parameter values\n",
        "psi_true = 0.8\n",
        "\n",
        "# simulate occurrence state\n",
        "z_true = rng.binomial(1, psi_true, size=site_count)\n",
        "\n",
        "## detection model\n",
        "\n",
        "# true parameter values\n",
        "p_true = 0.5\n",
        "\n",
        "# simulate detection\n",
        "y = sim_y(p_true, z_true, site_count, visit_count)\n",
        "\n",
        "# number of detections at each site \n",
        "y_summarized = y.sum(axis=1)\n",
        "\n",
        "# detection data at the first five sites \n",
        "y[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating parameters with PyMC\n",
        "\n",
        "Next, I use PyMC to train the occupancy model with the simulated data. First, similar to JAGS and Stan, the model must be specified using the PyMC syntax. This is done using a [context manager](https://peps.python.org/pep-0343/) in Python, essentially, a `with` statement. This creates a `Model` object. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with pm.Model() as constant:\n",
        "\n",
        "    # priors for the detetion and occurrence probabilities\\\n",
        "    psi = pm.Uniform('psi', 0, 1)\n",
        "    p = pm.Uniform('p', 0, 1)\n",
        "\n",
        "    # likelihood for the summarized data\n",
        "    pm.ZeroInflatedBinomial('y', p=p, psi=psi, n=visit_count, \n",
        "                            observed=y_summarized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In JAGS, the prior for $p$ would be specified as `p ~ dunif(0, 1).` The PyMC equivalent is `p = pm.Uniform('p', 0, 1)`. This could, alternatively, be specified as `p = pm.Uniform('detection probability', 0, 1)`.\n",
        "\n",
        "For the likelihood, I use PyMC's built-in `ZeroInflatedBinomial` distribution. We tell PyMC that this is an observed random variable by supplying data to the `observed` argument. \n",
        "\n",
        "PyMC has handy tools for visualizing the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'The simplest possible occupancy model. Random variables appear as unshaded ellipses while data appears shaded. Notice the ''200'', which states that there are 200 observations (sites) of this random variable.'\n",
        "pm.model_to_graphviz(constant)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now I can sample from the posterior. Again, I use the context manager, this time referring to the model by name. It's typical to name the output with `idata` because, by default, PyMC returns an object of class `InferenceData` from the Arviz package. Arviz is similar to the coda package for R."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "with constant:\n",
        "    constant_idata = pm.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyMC will try to use the No-U-Turn Sampler (NUTS) whenever possible. As you can see, it samples the posterior quickly.\n",
        "\n",
        "I plot the output using the `plot_trace` function from the Arviz package, supplying the true values for $p$ and $\\psi$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Estimated parameter values with the true values represented by vertical and horizontal lines.\n",
        "az.plot_trace(\n",
        "    constant_idata,\n",
        "    lines=[(\"psi\", {}, [psi_true]), (\"p\", {}, [p_true])] \n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Arviz also produces tabular summaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(constant_idata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding site covariates\n",
        "\n",
        "Next, I add in some realism by simulating a site-level covariate $x$ that affects the occurrence probability. I model this effect with a logit-linear model, i.e., $\\psi_j=\\text{logit}^{-1}(\\beta_0 + \\beta_1 x_j).$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## ecological model\n",
        "\n",
        "# true parameter values\n",
        "beta0_true = -1\n",
        "beta1_true = 3\n",
        "\n",
        "# covariates \n",
        "x = scale(rng.uniform(size=site_count))\n",
        "\n",
        "# linear model\n",
        "mu_true = beta0_true + beta1_true * x\n",
        "psi_true = invlogit(mu_true)\n",
        "\n",
        "# simulate occurrence state\n",
        "z_true = rng.binomial(1, psi_true)\n",
        "\n",
        "## detection model\n",
        "\n",
        "# true parameter values\n",
        "p_true = 0.75\n",
        "\n",
        "# simulate detection\n",
        "y = sim_y(p_true, z_true, site_count, visit_count)\n",
        "\n",
        "# vector with the number of detections at each site \n",
        "y_summarized = y.sum(axis=1)\n",
        "\n",
        "# detection data at the first five sites \n",
        "y[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, I specify the model with PyMC. Like JAGS, the random variables can be manipulated, as in a linear model with $x_j.$ These behave like numpy arrays, meaning that vectorized operations and broadcasting are available. To monitor the output of these manipulations, use the `pm.Deterministic` class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with pm.Model() as psix:\n",
        "\n",
        "    # occurrence process \n",
        "    # priors \n",
        "    beta0 = pm.Normal(\"beta0\", mu=0, sigma=2)\n",
        "    beta1 = pm.Normal(\"beta1\", mu=0, sigma=2)\n",
        "    \n",
        "    # linear model\n",
        "    mu = beta0 + beta1 * x\n",
        "    psi = pm.Deterministic(\"psi\", pm.math.invlogit(mu))\n",
        "\n",
        "    # detection process\n",
        "    # prior\n",
        "    p = pm.Uniform('p', 0, 1)\n",
        "\n",
        "    # likelihood for the summarized data\n",
        "    pm.ZeroInflatedBinomial('y', p=p, psi=psi, n=visit_count, \n",
        "                            observed=y_summarized)\n",
        "\n",
        "pm.model_to_graphviz(psix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These visualizations become handier as the models get more complex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with psix:\n",
        "    psix_idata = pm.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.plot_trace(\n",
        "    psix_idata,\n",
        "    var_names=['beta0', 'beta1', 'p'],\n",
        "    lines=[(\"beta0\", {}, [beta0_true]), (\"beta1\", {}, [beta1_true]), \n",
        "           ('p', {}, [p_true])]\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(psix_idata, var_names=['beta0', 'beta1', 'p'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding visit covariates\n",
        "\n",
        "Finally, I add in visit-level covariate $w_{j,k}$ that affects detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## ecological model\n",
        "\n",
        "# true parameter values\n",
        "beta0_true = -1\n",
        "beta1_true = 3\n",
        "\n",
        "# covariates \n",
        "x = scale(rng.uniform(size=site_count))\n",
        "\n",
        "# linear model\n",
        "mu_true = beta0_true + beta1_true * x\n",
        "psi_true = invlogit(mu_true)\n",
        "\n",
        "# simulate occurrence state\n",
        "z_true = rng.binomial(1, psi_true)\n",
        "\n",
        "# true parameter values\n",
        "alpha0_true = 1\n",
        "alpha1_true = -3\n",
        "\n",
        "# covariates\n",
        "w = rng.uniform(size=site_count * visit_count).reshape(site_count, visit_count)\n",
        "w = scale(w)\n",
        "\n",
        "# linear model\n",
        "nu_true = alpha0_true + alpha1_true * w\n",
        "p_true = invlogit(nu_true)\n",
        "\n",
        "# simulate detection\n",
        "y = sim_y(p_true, z_true, site_count, visit_count)\n",
        "\n",
        "y[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our PyMC code will need to be a little uglier now. I could write the model in terms of the latent occurrence state $z_j.$ The NUTS sampler, however, does not jive with discrete latent states. As such, PyMC will assign it to a binary Gibbs sampler by default, which works, albeit painfully slowly.\n",
        "\n",
        "Since I am impatient, I instead use the [marginalized version of the model](https://mbjoseph.github.io/posts/2020-04-28-a-step-by-step-guide-to-marginalizing-over-discrete-parameters-for-ecologists-using-stan/), that is, a model that does not include the discrete latent states. To do this in PyMC, I use the `CustomDist` class. This requires, first, defining the log probability of the distribution, `logp`, given the data and it's parameters. We can write `logp` using the likelihood of the occupancy model, \n",
        "$$\n",
        "P(\\mathbf{y}_j)= \n",
        "\\begin{cases}\n",
        "    P(\\mathbf{y}_j | z_j = 1)\\; \\psi_j \\; + \\; (1 - \\psi_j),   & \\text{if } \\mathbf{y}_j = \\mathbf{0}\\\\\n",
        "    P(\\mathbf{y}_j | z_j = 1)\\; \\psi_j,  & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "where $P(\\mathbf{y}_j | z_j = 1) = \\prod_j p_{j,k}^{y_{j,k}} (1-p_{j,k})^{(1-y_{j,k})}.$ To do this in PyMC, I rely on the `pm.math.switch` function, which is similar to `ifelse()` in R or `np.where()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# likelihood for y data\n",
        "def logp(x, p, psi):\n",
        "    '''Computes the log-likelihood for an occupancy model\n",
        "\n",
        "    Args: \n",
        "        x: (site_count x visit_count) array with binary detection data\n",
        "        p: (site_count x visit_count) array of probabilities\n",
        "        p: site_count vector of probabilities\n",
        "    '''\n",
        "    \n",
        "    bern = (p ** x) * ((1 - p) ** (1 - x))\n",
        "    bern_prod = pm.math.prod(bern, axis=1)\n",
        "    \n",
        "    res = pm.math.switch(\n",
        "        x.sum(axis=1) > 0,\n",
        "        bern_prod * psi,\n",
        "        bern_prod * psi + (1 - psi)\n",
        "    )\n",
        "    \n",
        "    return pm.math.log(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, I simply provide this function as an argument to the `CustomDist` class in our PyMC model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with pm.Model() as marginal:\n",
        "\n",
        "    # occurrence process \n",
        "    # priors \n",
        "    beta0 = pm.Normal(\"beta0\", mu=0, sigma=2)\n",
        "    beta1 = pm.Normal(\"beta1\", mu=0, sigma=2)\n",
        "    \n",
        "    # linear model\n",
        "    mu = beta0 + beta1 * x\n",
        "    psi = pm.Deterministic(\"psi\", pm.math.invlogit(mu))\n",
        "\n",
        "    # detection process\n",
        "    # priors\n",
        "    alpha0 = pm.Normal('alpha0', mu=0, sigma=2)\n",
        "    alpha1 = pm.Normal('alpha1', mu=0, sigma=2)\n",
        "\n",
        "    # linear model\n",
        "    nu = alpha0 + alpha1 * w\n",
        "    p = pm.Deterministic('p', pm.math.invlogit(nu))\n",
        "\n",
        "    # likelihood\n",
        "    pm.CustomDist(\n",
        "        'y',\n",
        "        p,\n",
        "        psi,\n",
        "        logp=logp,\n",
        "        observed=y,\n",
        "    )\n",
        "\n",
        "pm.model_to_graphviz(marginal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with marginal:\n",
        "    marginal_idata = pm.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.plot_trace(\n",
        "    marginal_idata,\n",
        "    var_names=['beta0', 'beta1', 'alpha0', 'alpha1'],\n",
        "    lines=[(\"beta0\", {}, [beta0_true]), (\"beta1\", {}, [beta1_true]), \n",
        "           ('alpha0', {}, [alpha0_true]), ('alpha1', {}, [alpha1_true])]\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(marginal_idata, var_names=['beta0', 'beta1', 'alpha0', 'alpha1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Real data example\n",
        "\n",
        "Finally, I demonstrate the model using a real data example. These data come from @henden2013, and were used as a demonstration in @hooten2019, Chapter 23. They represent detection/non-detection data of Willow Warblers from Finnmark, Norway. The $J=27$ sites were sampled $K=3$ times. Replicating the analysis in Box 23.7 in @hooten2019, I use two covariates for site: site area and willow tree height. Further, I use two covariates for visit: an indicator for the visit and willow tree height. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# read in the data\n",
        "data = pd.read_csv('PlosOne-DataFinnmark.csv')\n",
        "\n",
        "# subset the data to select willow warbler\n",
        "is_warbler = data.Species == \"Willow Warbler\"\n",
        "Y = data.loc[is_warbler, ['Y05.1', 'Y05.2', 'Y05.3']].to_numpy()\n",
        "n, J = Y.shape\n",
        "\n",
        "# generate site covariate matrix\n",
        "site_intercept = np.ones(n)\n",
        "pland = scale(data.loc[is_warbler, 'Pland']).to_numpy()\n",
        "wheight = scale(data.loc[is_warbler, 'wheight']).to_numpy()\n",
        "\n",
        "X = np.c_[site_intercept, pland, wheight]\n",
        "\n",
        "# generate visit covariate array\n",
        "visit_int = np.ones_like(Y)\n",
        "visit_wheight = np.repeat(wheight, repeats=J).reshape(n, J)\n",
        "\n",
        "# indicates which visit this is [0, 1, 2, 0, ...]\n",
        "_, visit_indicator = np.indices(Y.shape)\n",
        "visit_indicator = scale(visit_indicator)\n",
        "\n",
        "W = np.stack((visit_int, visit_indicator, visit_wheight), axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example uses an extremely handy feature of PyMC: coordinates. This allows us to specify a prior for each $\\alpha$ and $\\beta$ value in one line of code, using the `dims` argument in our prior distribution. The length of vector is implied by length of the list in `coords`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "coords = {\"beta_coefs\": [\"Intercept\", \"Pland\", 'Wheight'],\n",
        "          \"alpha_coefs\": [\"Intercept\", \"Visit\", 'Wheight']}\n",
        "\n",
        "with pm.Model(coords=coords) as warbler:\n",
        "\n",
        "    # occurrence process priors \n",
        "    Beta = pm.Normal(\"Beta\", mu=0, sigma=2, dims=\"beta_coefs\")\n",
        "    \n",
        "    # linear model\n",
        "    mu = pm.math.dot(X, Beta)\n",
        "    psi = pm.Deterministic(\"psi\", pm.math.invlogit(mu))\n",
        "\n",
        "    # detection process priors\n",
        "    Alpha = pm.Normal('Alpha', mu=0, sigma=2, dims='alpha_coefs')\n",
        "\n",
        "    # linear model\n",
        "    nu = pm.math.dot(W, Alpha)\n",
        "    p = pm.Deterministic('p', pm.math.invlogit(nu))\n",
        "\n",
        "    # likelihood\n",
        "    pm.CustomDist(\n",
        "        'y',\n",
        "        p,\n",
        "        psi,\n",
        "        logp=logp,\n",
        "        observed=Y,\n",
        "    )\n",
        "\n",
        "pm.model_to_graphviz(warbler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the dimensionality of the prior distributions is clear, with (3) different priors specified for each random variable in the vectors $\\alpha$ and $\\beta$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with warbler:\n",
        "    warbler_idata = pm.sample(4000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I upped the number of draw iterations to 4,000 per chain, 16,000 total, since this dataset includes real-world messiness. Nevertheless, sampling the posterior took only 6 seconds!  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(warbler_idata, var_names=['Alpha', 'Beta'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I compare the parameter estimates to the ones estimated by @hooten2019, Chapter 23. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "alpha_hat_hooten =  [0.04, 0.47, 0.68]\n",
        "beta_hat_hooten = [0.56, 1.92, 0.93]\n",
        "\n",
        "az.plot_trace(\n",
        "    warbler_idata,\n",
        "    var_names=['Alpha', 'Beta'],\n",
        "    lines=[(\"Alpha\", {}, [alpha_hat_hooten]), \n",
        "           (\"Beta\", {}, [beta_hat_hooten])]\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a high level of agreement between the two methods. While their algorithm was designed for teaching and interpretability, it is noteworthy that the PyMC model is 10x faster. \n",
        "\n",
        "Arviz also produces forest plots for looking at effect sizes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.plot_forest(warbler_idata, var_names=['Alpha', \"Beta\"], \n",
        "               hdi_prob=0.95, ess=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model comparison\n",
        "\n",
        "PyMC also has handy tools for model comparison. I demonstrate these by fitting a model to the warbler data with a constant probability of detection. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Y_sum = Y.sum(axis=1)\n",
        "\n",
        "with pm.Model(coords=coords) as warbler_constantp:\n",
        "\n",
        "    # occurrence process priors \n",
        "    Beta = pm.Normal(\"Beta\", mu=0, sigma=2, dims=\"beta_coefs\")\n",
        "    \n",
        "    # linear model\n",
        "    mu = pm.math.dot(X, Beta)\n",
        "    psi = pm.Deterministic(\"psi\", pm.math.invlogit(mu))\n",
        "\n",
        "    # detection process priors\n",
        "    p = pm.Uniform('p', 0, 1)\n",
        "\n",
        "    # likelihood\n",
        "    pm.ZeroInflatedBinomial('y', p=p, psi=psi, n=J, observed=Y_sum)\n",
        "\n",
        "pm.model_to_graphviz(warbler_constantp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with warbler_constantp:\n",
        "    warbler_constantp_idata = pm.sample(4000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, I caclculate the leave-one-out (loo) cross-validation score for each model [@vehtari2017]. This involves first computing the log likelihood for each model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with warbler:\n",
        "    pm.compute_log_likelihood(warbler_idata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "warbler_loo = az.loo(warbler_idata)\n",
        "\n",
        "warbler_loo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with warbler_constantp:\n",
        "    pm.compute_log_likelihood(warbler_constantp_idata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "warbler_constantp_loo = az.loo(warbler_constantp_idata)\n",
        "\n",
        "warbler_constantp_loo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Arviz has handy tools for comparing the results. First, I generate a tabular summary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_comp_loo = az.compare({r\"$p(visit,wheight)$\": warbler_idata, \n",
        "                          r\"$p(\\cdot)$\": warbler_constantp_idata})\n",
        "df_comp_loo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This indicates that the $p(\\cdot)$ model is favored over the $p(visit,wheight)$ model. \n",
        "\n",
        "Arviz also generates plots for these comparisons. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.plot_compare(df_comp_loo, insample_dev=False);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "mc",
      "language": "python",
      "display_name": "PyMC"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}