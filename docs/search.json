[
  {
    "objectID": "pymc-closed-cmr.html",
    "href": "pymc-closed-cmr.html",
    "title": "Closed capture-recapture",
    "section": "",
    "text": "In this notebook, I explore fitting closed population capture-recapture models in PyMC. Capture-recapture, at least the Lincoln-Peterson estimator, has been around for almost 100 years. Since then, countless varieties of capture-recapture models have been developed for closed populations (Otis et al. 1978).\nThe basic steps in capture-recapture are: capture several individuals–e.g., via trapping–from the population of interest, mark these animals, then release them. We repeat this process several times, each time noting when we recapture individuals.\nThis produces a capture history for each individual, which allows us to estimate the probability of capture and the number of individuals in the population \\(N\\).",
    "crumbs": [
      "Code",
      "PyMC",
      "Closed capture-recapture"
    ]
  },
  {
    "objectID": "pymc-closed-cmr.html#model-m_0",
    "href": "pymc-closed-cmr.html#model-m_0",
    "title": "Closed capture-recapture",
    "section": "Model \\(M_0\\)",
    "text": "Model \\(M_0\\)\nI explore fitting the simplest closed capture-recapture model, Model \\(M_0,\\) through parameter-expanded data-augmentation (PX-DA, Royle and Dorazio 2008). The idea with PX-DA is to augment the capture histories with \\(M-n\\) all zero capture-histories, where \\(M\\) is a hyperparameter that should be much greater than the true population size \\(N,\\) and \\(n\\) is the total number of individuals that were captured during the study. This allows us to treat the data as a zero-inflated binomial distribution (see below). In this case, we augment the history by setting \\(M=1500\\)\n\ndef augment_history(history, M):\n    '''Augment a capture history with all-zero histories.'''\n\n    animals_captured, T = history.shape\n\n    # create M - n all zero histories\n    zero_history_count = M - animals_captured\n    zero_history = np.zeros((zero_history_count, T))\n\n    # tack those on to the capture history\n    augmented = np.vstack((history, zero_history))\n\n    return augmented\n\ndef get_histories():\n    '''Read, augment, and recombine the salamander histories.'''\n\n    # read in salamander data\n    sal_data = pd.read_csv('sal_data.csv')\n\n    # labels for capture history columns\n    col_labs = [f'y{t}' for t in range(1, 5)]\n\n    # subset each dataset before augmenting\n    is_pyg = sal_data.spp == 1\n    is_red = sal_data.spp == 0\n\n    pyg = sal_data.loc[is_pyg, col_labs].to_numpy()\n    red = sal_data.loc[is_red, col_labs].to_numpy()\n\n    return {'pyg': pyg, 'red': red}\n\ndef augment_histories(histories, M):\n\n    pyg_augmented = augment_history(histories['pyg'], M=M)\n    red_augmented = augment_history(histories['red'], M=M)\n\n    # recombine into one history\n    history = np.concatenate((pyg_augmented, red_augmented))\n\n    return history\n\nhistories = get_histories()\n\nn_red, T = histories['red'].shape\nn_pyg, T = histories['pyg'].shape\n\n# # summarize into binomial data\nM = 1500\nhistory_augmented = augment_histories(histories, M=M)\nhistory_summarized = history_augmented.sum(axis=1)\n\nFor this model, I use the pm.ZeroInflatedBinomial class, just as I did in the occupancy notebook. That said, the parameters here are different. First, \\(p\\) represents the probability of capturing a given individual during the survey. Second, \\(\\psi\\) represents a mysterious entity known as the inclusion probability. That is, the probability that an individual from the hypothetical superpopulation \\(M\\) is included in the population of interest \\(N.\\) Then, we can simulate the posterior distribution for \\(N\\) using \\(M\\) and the posterior distributions of \\(\\psi.\\)\nIn this example, I combine the two species into one pm.Model object, making use of coords. That said, the parameters for each species are treated as independent. In other words, this is a “no-pooling” model.\n\n# index for each species\nspecies_idx = np.repeat([0, 1], M)\n\n# coordinates identifying parameter each species\ncoords = {'species': ['pygmy', 'red_cheeked']}\n\nwith pm.Model(coords=coords) as M0:\n\n    # priors for the capture and inclusion probabilities\n    psi = pm.Beta('psi', 0.001, 1, dims='species')\n    p = pm.Uniform('p', 0, 1, dims='species')\n\n    # likelihood for the summarized data\n    pm.ZeroInflatedBinomial(\n        'history',\n        p=p[species_idx],\n        psi=psi[species_idx],\n        n=T,\n        observed=history_summarized\n    )\n\npm.model_to_graphviz(M0)\n\n\n\n\n\n\n\nFigure 1: Visual representation of model \\(M_0.\\) MarginalMixture refers to the zero-inflated binomial distribution.\n\n\n\n\n\n\nwith M0:\n    M0_idata = pm.sample()\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [psi, p]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 4 seconds.\n\n\n\nax = az.plot_trace(M0_idata, figsize=(8,4), var_names=['psi', 'p']);\nplt.subplots_adjust(hspace=0.4)\n\n\n\n\n\n\n\nFigure 2: Traceplots for the salamander \\(M_0\\) model. The red-cheeked salamander is in blue while the pygmy salamander is in red.\n\n\n\n\n\nFor faster sampling, it’s better to separate the two species into two separate models. On my machine, the individual species models finish sampling in 2-3 seconds, compared to 15-20 seconds for the two species model. That said, the two species model is somewhat more convenient.\nOf course, the trace plots lack our true parameter of interest: the population size \\(N.\\) We can simulate the posterior of \\(N\\) as a derived quantity, using \\(M\\) and the posterior distribution of \\(\\psi\\).\n\n# az.extract flattens the chains\nposterior = az.extract(M0_idata)\npsi_samps = posterior.psi.values\np_samps = posterior.p.values\n\n# posterior probabilities of being present in the population but not detected\np_if_present = psi_samps * binom.pmf(0, n=T, p=p_samps)\np_total = p_if_present + (1 - psi_samps)\n\n# simulate the number of undetected animals in each population\nnumber_undetected_pyg = RNG.binomial(M - n_pyg, p_if_present[0] / p_total[0])\nnumber_undetected_red = RNG.binomial(M - n_red, p_if_present[1] / p_total[1])\n\n# simulate N\nN_pyg = n_pyg + number_undetected_pyg\nN_red = n_red + number_undetected_red\n\nBelow I plotted the posterior distributions of \\(N\\) for both species, adding the estimates from Hooten and Hefley (2019), Chapter 24. Although note that they used a different prior for \\(\\psi.\\)\n\nN_hooten = [229.6, 450.9]\nfig, ax = plt.subplots(figsize=(6,4))\nax.hist(N_pyg, color='C0', edgecolor='white', alpha=0.9, bins=30, label='Pygmy')\nax.hist(N_red, color='C1', edgecolor='white', alpha=0.9, bins=30, label='Red-cheeked')\nax.axvline(N_hooten[0], linestyle='--', color='black', linewidth=2)\nax.axvline(N_hooten[1], linestyle='--', color='black', linewidth=2)\nax.set_title('Posterior distributions of $N$')\nax.set_ylabel('Number of samples')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Posterior distributions of \\(N\\) from the \\(M_0\\) model. Estimates from Hooten and Hefley (2019) are shown by the vertical lines’\n\n\n\n\n\nWe might expect estimates of capture probability \\(p\\) and the abundance \\(N\\) to be somewhat correlated. We can explore this relationship visually by plotting the posterior draws.\n\n# create the plot\nfig, ax = plt.subplots(1, 1, figsize=(4, 4))\n\n# add the scatter for each species\nlabs = ['Pygmy', 'Red-backed']\nax.scatter(p_samps[0], N_pyg, s=10, alpha=0.2, label=labs[0])\nax.scatter(p_samps[1], N_red, s=10, alpha=0.2, label=labs[1])\n\n# this removes the opacity for the dots in the legend\nleg = ax.legend()\nfor lh in leg.legend_handles:\n    lh.set(sizes=[25], alpha=[1])\n\n# update aesthetics\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)\n\nax.set_ylabel(r'$N$')\nax.set_xlabel(r'$p$')\nax.set_title('Posterior draws')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 4: Posterior draws of \\(N\\) and \\(p\\) for both species of salamander.",
    "crumbs": [
      "Code",
      "PyMC",
      "Closed capture-recapture"
    ]
  },
  {
    "objectID": "pymc-closed-cmr.html#model-m_b",
    "href": "pymc-closed-cmr.html#model-m_b",
    "title": "Closed capture-recapture",
    "section": "Model \\(M_b\\)",
    "text": "Model \\(M_b\\)\nNext, I fit model \\(M_b,\\) which accounts for the possibility that the capture probability changes after the animal is first caught. This could be from trap happiness, whereby animals are more likely to be trapped after their first time. Conversely, this could be from subsequent trap avoidance.\n\n# read in the microtus data\nmicrotus = np.loadtxt('microtus.data.txt').astype(int)\n\n# the last column is not relevant\nmicro_hist = microtus[:,:-1]\nn, T = micro_hist.shape\n\n# augment with all zero histories\nM = 100\nmicro_augmented = augment_history(micro_hist, M=M)\n\n# note the occasion when each individual was first seen\nfirst_seen = (micro_hist != 0).argmax(axis=1)\n\n# create the covariate for the behavior effect\nbehavior_effect = np.zeros((M, T))\nfor i, f in enumerate(first_seen):\n    behavior_effect[i, (f + 1):] = 1\n\n# covariate matrix\nx_int = np.ones((M, T))\nX = np.stack((x_int, behavior_effect), axis=2)\n\nJust like in the occupancy notebook, the detection probability will now be a matrix, i.e., it depends on the individual and the occasion. As such, we can no longer rely on pm.ZeroInflatedBinomial. Again, just like the occupancy notebook, we will write the model in terms of the latent \\(z_i\\) state, then marginalize it out with pymc_extras.marginalize().\n\ncoords = {'alpha_coeffs': ['Intercept', 'B_Response']}\nwith pm.Model(coords=coords) as mb:\n\n    # priors for the capture and inclusion probabilities\n    psi = pm.Beta('psi', 0.001, 1)\n    Alpha = pm.Normal('Alpha', 0, 2, dims='alpha_coeffs')\n\n    # linear model for the capture probability\n    nu = pm.math.dot(X, Alpha)\n    p = pm.Deterministic('p', pm.math.invlogit(nu))\n\n    # included / excluded state\n    z = pm.Bernoulli('z', psi, shape=M)\n\n    # likelihood for detection given inclusion\n    mu_y = z[:, None] * p\n    pm.Bernoulli('y', mu_y, observed=micro_augmented)\n\npm.model_to_graphviz(mb)\n\n\n\n\n\n\n\nFigure 5: Visual representation of model \\(M_b.\\)\n\n\n\n\n\n\nmb_marginal = pmx.marginalize(mb, ['z'])\nwith mb_marginal:\n    mb_idata = pm.sample()\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Alpha, psi]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\naz.summary(mb_idata, var_names=['Alpha', 'psi'])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nAlpha[Intercept]\n0.113\n0.242\n-0.352\n0.564\n0.005\n0.004\n2091.0\n2235.0\n1.0\n\n\nAlpha[B_Response]\n0.615\n0.287\n0.033\n1.115\n0.006\n0.005\n2180.0\n2382.0\n1.0\n\n\npsi\n0.570\n0.050\n0.477\n0.663\n0.001\n0.001\n2887.0\n2601.0\n1.0\n\n\n\n\n\n\n\n\naz.plot_forest(mb_idata, var_names=['Alpha'], combined=True, ess=True, figsize=(6,2));\n\n\n\n\n\n\n\nFigure 6: Forest plot showing the catchability parameters from model \\(M_b.\\)\n\n\n\n\n\nThe forest plot indicates that there is some evidence of a weak, positive behavioral response. Although note that the 94% credible intervals between the baseline capture rate and the behavioral effect overlap considerably.\n\n# # simulate draws of N\nidata = pmx.recover_marginals(mb_idata, model=mb_marginal);\nN_hat = az.extract(idata).z.sum(dim='z_dim_0')\n\n# create the plot\nfig, ax = plt.subplots(figsize=(4, 3))\n\n# bar plot looks a little better than a histogram here imo\nN_values, N_counts = np.unique(N_hat, return_counts=True)\nax.bar(N_values, N_counts)\n\nax.annotate(\n    'Number\\ndetected $n$',\n    ha='left',\n    xy=(N_values[0], N_counts[0]),\n    color='black',\n    xytext=(n+5, 700),\n    arrowprops=dict(arrowstyle=\"-&gt;\", color='black', linewidth=1,\n                    connectionstyle=\"angle3,angleA=90,angleB=0\")\n)\n\n# ax.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\nax.set_ylabel('Number of samples')\nax.set_title('Posterior of $N$')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 7: Posterior distribution of \\(N\\) from model \\(M_b.\\) The number voles that were detected \\(n\\) is shown by the vertical red line.\n\n\n\n\n\nMost of the posterior density of \\(N\\) is at \\(n,\\) the number of animals detected. The discovery curve hints at why this may be the case. It seems that all the voles in the population may have been captured by the end of the study.\n\n# how many voles have been seen?\ntotal_seen = micro_hist.sum(axis=0).cumsum()\ntotal_seen = np.insert(total_seen, 0, 0)\n\n# how many new voles have been seen?\nfirst_seen = (micro_hist != 0).argmax(axis=1)\nnewbies = [sum(first_seen == t) for t in range(T)]\ntotal_newbies = np.cumsum(newbies)\ntotal_newbies = np.insert(total_newbies, 0, 0)\n\nfig, ax = plt.subplots(figsize=(5, 3.5))\nax.plot(total_seen, total_newbies)\nax.fill_between(total_seen, total_newbies, alpha=0.2)\nax.set_title('Discovery curve')\nax.set_xlabel('Total voles captured')\nax.set_ylabel('Unique voles captured')\nplt.show()\n\n\n\n\n\n\n\nFigure 8: Discovery curve for the Microtus study.\n\n\n\n\n\nWe can also look at the behavioral effect by visualizing the posterior distributions of \\(p.\\) As we can see, the voles who have been captured before are more likely to be captured again.\n\np_samps = az.extract(mb_idata).p.values\nfirst_detection = X[:, :, 1] == 0\np_first_detection = p_samps[first_detection].flatten()\np_seen_before = p_samps[~first_detection].flatten()\n\nfig, ax = plt.subplots(figsize=(5, 3.5))\naz.plot_dist(p_first_detection, ax=ax, label='First detection', color='C0')\naz.plot_dist(p_seen_before, ax=ax, label='Seen before', color='C1')\nax.set_title('Posterior distributions of $p$')\nax.set_xlim((0,1))\nax.set_yticks([])\nax.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure 9: Posterior distributions for the probability of detection given the behavioral effect.",
    "crumbs": [
      "Code",
      "PyMC",
      "Closed capture-recapture"
    ]
  },
  {
    "objectID": "pymc-closed-cmr.html#model-m_t",
    "href": "pymc-closed-cmr.html#model-m_t",
    "title": "Closed capture-recapture",
    "section": "Model \\(M_t\\)",
    "text": "Model \\(M_t\\)\nWe can also look at time varying effects with model \\(M_t.\\)\n\ncoords = {'occasion': np.arange(T)}\nwith pm.Model(coords=coords) as mt:\n\n    # priors for the capture and inclusion probabilities\n    psi = pm.Beta('psi', 0.001, 1)\n    p = pm.Uniform('p', 0, 1, dims='occasion')\n\n    # included / excluded state\n    z = pm.Bernoulli('z', psi, shape=M)\n\n    # likelihood for detection given inclusion\n    mu_y = z[:, None] * p\n    pm.Bernoulli('y', mu_y, observed=micro_augmented)\n\npm.model_to_graphviz(mt)\n\n\n\n\n\n\n\nFigure 10: Visual representation of model \\(M_b.\\)\n\n\n\n\n\nBroadcasting z across p, now that p is a vector, produces a matrix. The first row of this matrix, mu_y[0], contains the first value of the state variable, z[0], multiplied against every value of the detection probability vector p[0:T], i.e., p.\n\nmt_marginal = pmx.marginalize(mt, ['z'])\nwith mt_marginal:\n    mt_idata = pm.sample()\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p, psi]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\naz.summary(mt_idata, var_names=['p', 'psi'])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\np[0]\n0.565\n0.064\n0.439\n0.675\n0.001\n0.001\n6318.0\n3589.0\n1.0\n\n\np[1]\n0.704\n0.059\n0.589\n0.807\n0.001\n0.001\n4626.0\n3004.0\n1.0\n\n\np[2]\n0.668\n0.062\n0.547\n0.779\n0.001\n0.001\n6251.0\n3135.0\n1.0\n\n\np[3]\n0.616\n0.063\n0.497\n0.732\n0.001\n0.001\n5893.0\n2979.0\n1.0\n\n\np[4]\n0.582\n0.064\n0.464\n0.704\n0.001\n0.001\n5534.0\n3276.0\n1.0\n\n\npsi\n0.558\n0.051\n0.461\n0.654\n0.001\n0.001\n6972.0\n3037.0\n1.0\n\n\n\n\n\n\n\n\naz.plot_forest(mt_idata, var_names=['p'], combined=True, ess=True, figsize=(6,2));\n\n\n\n\n\n\n\nFigure 11: Forest plot showing the variation in the detection probability from model \\(M_b.\\)\n\n\n\n\n\nFor the most part, it doesn’t look as if the detection prbobability varied dramatically across the study.",
    "crumbs": [
      "Code",
      "PyMC",
      "Closed capture-recapture"
    ]
  },
  {
    "objectID": "pymc-closed-cmr.html#model-m_h",
    "href": "pymc-closed-cmr.html#model-m_h",
    "title": "Closed capture-recapture",
    "section": "Model \\(M_{h}\\)",
    "text": "Model \\(M_{h}\\)\nWe can also model individual effects with model \\(M_h\\). Following this PyMC notebook, I use a bounded Pareto distribution, which goes on to inform the individual-level recapture probabilities, which are Beta-distributed. See their notebook for deatils.\n\ncoords = {'individual': np.arange(M)}\nwith pm.Model(coords=coords) as mh:\n\n    # priors for the capture and inclusion probabilities\n    psi = pm.Beta('psi', 0.001, 1)\n\n    # overall average\n    phi = pm.Uniform(\"phi\", lower=0.0, upper=1.0)\n\n    # individual level variation\n    kappa_log = pm.Exponential(\"kappa_log\", lam=1.5)\n    kappa = pm.Deterministic(\"kappa\", pm.math.exp(kappa_log))\n    theta = pm.Beta(\"theta\", alpha=phi * kappa, beta=(1.0 - phi) * kappa, dims=\"individual\")\n\n    pm.ZeroInflatedBinomial(\n        'y',\n        p=theta,\n        psi=psi,\n        n=T,\n        observed=micro_augmented.sum(axis=1)\n    )\n\npm.model_to_graphviz(mh)\n\n\n\n\n\n\n\nFigure 12: Visual representation of model \\(M_h.\\)\n\n\n\n\n\nThis model is a little trickier to fit than the models above. As such, I increased the number of tuning draws and post-tune draws to 2000. Moreover, I increased the target acceptance rate of the sampler to 0.95.\n\nwith mh:\n    mh_idata = pm.sample(tune=2000, draws=2000, target_accept=0.99)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [psi, phi, kappa_log, theta]\n\n\n\n\n\n\n\n\nSampling 4 chains for 2_000 tune and 2_000 draw iterations (8_000 + 8_000 draws total) took 12 seconds.\n\n\n\naz.summary(mh_idata, var_names=['psi', 'phi', 'kappa'])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\npsi\n0.720\n0.096\n0.545\n0.909\n0.003\n0.001\n1254.0\n1986.0\n1.00\n\n\nphi\n0.494\n0.072\n0.355\n0.622\n0.002\n0.001\n1032.0\n2235.0\n1.00\n\n\nkappa\n1.369\n0.365\n1.000\n2.043\n0.012\n0.014\n1045.0\n1631.0\n1.01\n\n\n\n\n\n\n\n\nax = az.plot_trace(mh_idata, var_names=['psi', 'phi', 'kappa'])\nplt.subplots_adjust(hspace=0.4)\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(7, 13), sharey=True)\n\naz.plot_forest(mh_idata, var_names=\"theta\", combined=True, ess=True, ax=axes,\n               coords={'individual': np.arange(50)});\n\n\n\n\n\n\n\n\nWe see that there is notable variation in capture probability among individuals. Further, we see that the inclusion probability has increased considerable, with some samples near the boundary. If we were interested in this model, we would likely have to increase the number of augmented individuals in the analysis.\n\n%load_ext watermark\n\n%watermark -n -u -v -iv -w\n\nLast updated: Wed Jan 14 2026\n\nPython implementation: CPython\nPython version       : 3.13.9\nIPython version      : 9.9.0\n\nnumpy      : 2.3.5\npymc       : 5.27.0\npymc_extras: 0.7.0\nseaborn    : 0.13.2\nmatplotlib : 3.10.8\npandas     : 2.3.3\narviz      : 0.23.0\npytensor   : 2.36.3\n\nWatermark: 2.5.0",
    "crumbs": [
      "Code",
      "PyMC",
      "Closed capture-recapture"
    ]
  },
  {
    "objectID": "pymc-cjs.html",
    "href": "pymc-cjs.html",
    "title": "Cormack-Jolly-Seber",
    "section": "",
    "text": "In this notebook, I explore the Cormack-Jolly-Seber (CJS) model for estimating survival using capture-recapture data in PyMC. I have drawn considerable inspiration from Austin Rochford’s notebook on capture-recapture in PyMC, the second chapter of my dissertation (a work in progress), and McCrea and Morgan (2014).\n\nCormack-Jolly-Seber\nThe goal of the CJS model is to estimate survival, accounting for capture probabilities being less than one. There are many methods for estimating parameters in the model, including state-space formulations that explicitly model the latent alive/dead state \\(z.\\) Following the theme of the previous notebooks, I instead marginalize this variable out of the model by using the so-called \\(M\\)-array. This is an array that contains the sufficient statistics for the CJS models. For example, \\(m_{1,2}\\) is the number of individuals that were released on at \\(t=1\\) and were first recaptured on \\(t=2.\\)\n\nAn example of the M-array from a four year capture-recapture survey. The number recaptured, \\(m_{i,j}\\) refers to the number of individuals released at \\(i\\) who were first recaptured at time \\(j\\).\n\n\n\n\n\n\n\n\n\n\n\nNumber Released\n\nNumber recaptured\n\nNever recaptured\n\n\n\n\n\n\n1982\n1983\n1984\n\n\n\n1981\n\\(R_1\\)\n\\(m_{1,2}\\)\n\\(m_{1,3}\\)\n\\(m_{1,4}\\)\n\\(R_1-m_{1\\cdot}\\)\n\n\n1982\n\\(R_2\\)\n\n\\(m_{2,3}\\)\n\\(m_{2,3}\\)\n\\(R_2-m_{2\\cdot}\\)\n\n\n1983\n\\(R_3\\)\n\n\n\\(m_{3,4}\\)\n\\(R_3-m_{3\\cdot}\\)\n\n\n\n\n%config InlineBackend.figure_format = 'retina'\n\nfrom pymc.distributions.dist_math import factln\nfrom scipy.linalg import circulant\nimport arviz as az\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport pytensor.tensor as pt\nimport seaborn as sns\n\n# only necessary on MacOS Sequoia\n# https://discourse.pymc.io/t/pytensor-fails-to-compile-model-after-upgrading-to-mac-os-15-4/16796/5\nimport pytensor\npytensor.config.cxx = '/usr/bin/clang++'\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\nsns.set_palette(\"tab10\")\n\ndef fill_lower_diag_ones(x):\n    '''Fill the lower diagonal of a matrix with ones.'''\n    return pt.triu(x) + pt.tril(pt.ones_like(x), k=-1)\n\nAs an example, I use the cormorant data from McCrea and Morgan (2014), Table 4.6. These data come from an eleven year capture-recapture study between 1982 and 1993. These were breeding cormorants of unknown age. The data is summarized in the \\(M\\)-array below. The last column is the number that were never recapured. The number released can be calculated from the array.\n\ncormorant = np.array([\n       [ 10,   4,   2,   2,   0,   0,   0,   0,   0,   0,  12],\n       [  0,  42,  12,  16,   1,   0,   1,   1,   1,   0,  83],\n       [  0,   0,  85,  22,   5,   5,   2,   1,   0,   1,  53],\n       [  0,   0,   0, 139,  39,  10,  10,   4,   2,   0,  94],\n       [  0,   0,   0,   0, 175,  60,  22,   8,   4,   2, 199],\n       [  0,   0,   0,   0,   0, 159,  46,  16,   5,   2, 193],\n       [  0,   0,   0,   0,   0,   0, 191,  39,   4,   8, 171],\n       [  0,   0,   0,   0,   0,   0,   0, 188,  19,  23, 284],\n       [  0,   0,   0,   0,   0,   0,   0,   0, 101,  55, 274],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  84,  97]])\n\n\ninterval_count, T = cormorant.shape\n\nnumber_recaptured = cormorant[:,:-1]\nnever_recaputured = cormorant[:,-1]\nnumber_released = number_recaptured.sum(axis=1) + never_recaputured\n\nThis PyMC model will look different than the ones in previous notebooks, simply because it requires many tricks to get the probabilities in the correct format for the \\(m\\)-array, then modeling the \\(m\\)-array as a multinomial with the associated cell probabilities. These probabilities correspond to the situations in the \\(m\\)-array, such as the probability that an animal survived and was not recaptured until a later date. In this example, I model survival as time-varying, i.e., \\(\\phi(t).\\)\n\n# utility vectors for creating arrays and array indices\nintervals = np.arange(interval_count)\nrow_indices = np.reshape(intervals, (interval_count, 1))\ncol_indices = np.reshape(intervals, (1, interval_count))\n\n# matrix indicating the number of intervals between sampling occassions\nintervals_between = np.clip(col_indices - row_indices, 0, np.inf)\n\nwith pm.Model() as phit:\n\n    # priors for catchability and survival\n    p = pm.Uniform('p', 0, 1)\n    phi = pm.Uniform('phi', 0, 1, shape=interval_count)\n\n    # broadcast phi into a matrix\n    phi_mat = pt.ones_like(number_recaptured) * phi\n    phi_mat = fill_lower_diag_ones(phi_mat) # fill irrelevant values\n\n    # probability of surviving between i and j in the m-array\n    p_alive = pt.cumprod(phi_mat, axis=1)\n    p_alive = pt.triu(p_alive) # select relevant (upper triangle) values\n\n    # probability of not being captured between i and j\n    p_not_cap = pt.triu((1 - p) ** intervals_between)\n\n    # probabilities associated with each cell in the m-array\n    nu = p_alive * p_not_cap * p\n\n    # probability for the animals that were never recaptured\n    chi = 1 - nu.sum(axis=1)\n\n    # combine the probabilities into a matrix\n    chi = pt.reshape(chi, (interval_count, 1))\n    marr_probs = pt.horizontal_stack(nu, chi)\n\n    # distribution of the m-array\n    marr = pm.Multinomial(\n        'M-array',\n        n=number_released,\n        p=marr_probs,\n        observed=cormorant\n    )\n\npm.model_to_graphviz(phit)\n\n\n\n\n\n\n\nFigure 1: Visual representation of the CJS model\n\n\n\n\n\n\nwith phit:\n    cjs_idata = pm.sample()\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p, phi]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\nmccrea_p = [0.51]\nmccrea_phi = [0.79, 0.56, 0.83, 0.86, 0.73, 0.70, 0.81, 0.64, 0.46, 0.95]\n\naz.plot_trace(\n    cjs_idata,\n    figsize=(10, 4),\n    lines=[(\"phi\", {}, [mccrea_phi]), (\"p\", {}, [mccrea_p])]\n);\n\n\n\n\n\n\n\nFigure 2: Traceplots for the cormorant CJS model with \\(p(\\cdot)\\phi(t)\\). MLEs from McCrea and Morgan (2014) shown by vertical and horizontal lines.\n\n\n\n\n\nThe model samples fairly quickly in this parameterization. The traceplots above include comparisons to the estimates from McCrea and Morgan (2014). While a bit messy, the plots show a high level of agreement between their estimates and the ones here. To clean things up a bit, I plot the estimates for \\(\\phi\\) over time, along with the 94% credible intervals\n\nfig, ax = plt.subplots(figsize=(6,4))\n\nt = np.arange(1983, 1993)\n\nphi_samps = az.extract(cjs_idata, var_names='phi').values.T\nphi_median = np.median(phi_samps, axis=0)\n\nax.plot(t, phi_median, linestyle='dotted', color='lightgray', linewidth=2)\nax.violinplot(phi_samps, t, showmedians=True, showextrema=False)\n\nax.set_ylim((0,1))\n\nax.set_ylabel(r'Apparent survival $\\phi$')\nax.set_title(r'Cormorant CJS')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Violin plots of the posterior for apparent surival over time from the cormorant CJS. Horizontal lines represent the posterior median.\n\n\n\n\n\n\n%load_ext watermark\n\n%watermark -n -u -v -iv -w\n\nLast updated: Wed Jan 14 2026\n\nPython implementation: CPython\nPython version       : 3.13.9\nIPython version      : 9.9.0\n\narviz     : 0.23.0\nscipy     : 1.17.0\npymc      : 5.27.0\nseaborn   : 0.13.2\npytensor  : 2.36.3\nmatplotlib: 3.10.8\nnumpy     : 2.3.5\n\nWatermark: 2.5.0\n\n\n\n\n\n\n\n\nReferences\n\nMcCrea, Rachel S, and Byron JT Morgan. 2014. Analysis of Capture-Recapture Data. CRC Press.",
    "crumbs": [
      "Code",
      "PyMC",
      "Cormack-Jolly-Seber"
    ]
  },
  {
    "objectID": "comparison.html",
    "href": "comparison.html",
    "title": "Model comparison",
    "section": "",
    "text": "In this notebook, I demonstrate an approach to model selection in PyMC. To do so, follow the lead of King and Brooks (2008), although not nearly as elegantly. They demonstrate an approach to model selection for a typical suite of closed capture-recapture models. These include the effects of behavior \\(b\\), time \\(t,\\) and individual heterogeneity \\(h\\) on capture probabilities \\(p\\). The eight models considered here are combinations of the three: \\(M_{0},\\) \\(M_{t},\\) \\(M_{b},\\) \\(M_{tb},\\) \\(M_{h},\\) \\(M_{th},\\) \\(M_{bh}\\). The full model, \\(M_{tbh}\\), is\n\\[\n\\begin{equation}\n\\text{logit} \\; p_{it} = \\mu + \\alpha_t + \\beta x_{it} + \\gamma_i,\n\\end{equation}\n\\] where \\(\\mu\\) is the average catchability, \\(\\alpha_t\\) is the effect of each occasion on catchability, \\(\\beta\\) is the behavioral effect, \\(x_{it}\\) indicates whether the individual has been previously caught, and \\(\\gamma_i\\) is the individual random effect such that \\(\\gamma_i \\sim \\text{Normal}(0,\\sigma)\\). Formulating the model this way makes the other models nested subsets of the full model.\nLike King and Brooks (2008), I use the the Moray Firth bottlenose dolphin data as a motivating example. Wilson, Hammond, and Thompson (1999) detected \\(n=56\\) dolphins over the course of \\(T=17\\) boat surveys between May and September 1992. They generated the capture-recapture histories by way of photo-identification, which is near and dear to my heart (and my dissertation).\n\n%config InlineBackend.figure_format = 'retina'\n\n# libraries \nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom pymc.distributions.dist_math import binomln, logpow\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\npal = sns.color_palette(\"Set2\")\nsns.set_palette(pal)\n\n# hyperparameters \nSEED = 808\nRNG = np.random.default_rng(SEED)\n\ndef augment_history(history):\n    '''Augment a capture history with all-zero histories.'''\n    \n    animals_captured, T = history.shape\n\n    # create M - n all zero histories\n    zero_history_count = M - animals_captured\n    zero_history = np.zeros((zero_history_count, T))\n\n    # tack those on to the capture history\n    augmented = np.row_stack((history, zero_history))\n\n    return augmented \n\ndef get_behavior_covariate(history):\n    \n    # note the occasion when each individual was first seen\n    first_seen = (history != 0).argmax(axis=1)\n    \n    # create the covariate for the behavior effect\n    behavior_covariate = np.zeros_like(history)\n    for i, f in enumerate(first_seen):\n        behavior_covariate[i, (f + 1):] = 1\n\n    return behavior_covariate\n\ndef get_occasion_covariate(history):\n\n    _, T = history.shape\n    l = []\n    for t in range(T):\n        oc = np.zeros_like(history)\n        oc[:, t] = 1\n        l.append(oc)\n\n    return np.stack(l, axis=2)\n\ndef sim_N(idata):\n    \n    psi_samps = az.extract(idata).psi.values\n    p_samps = az.extract(idata).p.values\n    not_p = (1 - p_samps)\n    \n    if p_samps.ndim == 1:\n        p_included = psi_samps * (not_p) ** T \n        number_undetected = RNG.binomial(M - n, p_included)\n\n    elif p_samps.ndim == 3:\n        p_included = psi_samps * not_p.prod(axis=1)\n        number_undetected = RNG.binomial(1, p_included).sum(axis=0)\n\n    # N = n + number_undetected\n    N = RNG.binomial(M, psi_samps)\n    return N\n\n# convert the dolphin capture history from '1001001' to array\ndolphin = np.loadtxt('firth.txt', dtype=str)\ndolphin = np.array([list(map(int, d)) for d in dolphin])\n\n# augment the capture history with all zero histories\nn, T = dolphin.shape\nM = 500\ndolphin_augmented = augment_history(dolphin)\n\n# covariates for t and b\noccasion_covariate = get_occasion_covariate(dolphin_augmented)\nbehavior_covariate = get_behavior_covariate(dolphin_augmented)\n\nThe discovery curve, the number of unique dolphins encountered as a function of the total number of dolphins encountered, may be flattening. This suggests that, at this point in the study, Wilson, Hammond, and Thompson (1999) may have encountered many of the unique individuals in the population.\n\n\n\n# how many dolphins have been seen?\ntotal_seen = dolphin.sum(axis=0).cumsum()\n\n# how many new dolphins have been seen?\nfirst_seen = (dolphin != 0).argmax(axis=1)\nnewbies = [sum(first_seen == t) for t in range(T)]\ntotal_newbies = np.cumsum(newbies)\n\nfig, ax = plt.subplots(figsize=(5, 3.5))\nax.plot(total_seen, total_newbies)\nax.fill_between(total_seen, total_newbies, alpha=0.2)\nax.set_title('Discovery curve')\nax.set_xlabel('Total dolphins')\nax.set_ylabel('Unique dolphins')\nplt.show()\n\n\nFigure 1\n\n\n\n\nTraining each model\nThis notebook looks messier than the others, in that I train several models with little commentary along the way. In practice, it would probably be better to wrap these up into a function or a class. To complete the model, I used the following priors, \\[\n\\begin{align}\n\\psi &\\sim \\text{Uniform}(0, 1)\\\\\n\\mu &\\sim \\text{Logistic}(0, 1) \\\\\n\\alpha_t &\\sim \\text{Normal}(0, \\sigma_{\\alpha}) \\\\\n\\beta &\\sim \\text{Normal}(0, \\sigma_{\\beta}) \\\\\n\\gamma_i &\\sim \\text{Normal}(0, \\sigma_{\\gamma}) \\\\\n\\sigma_{\\alpha} &\\sim \\text{InverseGamma}(4, 3) \\\\\n\\sigma_{\\beta} &\\sim \\text{InverseGamma}(4, 3) \\\\\n\\sigma_{\\gamma} &\\sim \\text{InverseGamma}(4, 3),\n\\end{align}\n\\] which were also used by King and Brooks (2008). Although note that I used an informative \\(\\text{Beta}(1, 5)\\) prior for \\(\\psi\\) in the full model (see below). I use the same logp seen in the occupancy and closed capture-recapture notebooks, which accounts for row-level zero-inflation. Unlike other notebooks, I did not look at the summaries or the trace plots unless the sampler indicated that it had issues during training.\nThroughout the notebook, I use the nutpie sampler within PyMC. Nutpie is a NUTS sampler written in Rust, and is often faster than PyMC. Also, I have tweaked the sampling keyword arguments for each model, since they are a little finicky.\n\ndef logp(value, n, p, psi):\n    \n    binom = binomln(n, value) + logpow(p, value) + logpow(1 - p, n - value)\n    bin_sum = pm.math.sum(binom, axis=1)\n    bin_exp = pm.math.exp(bin_sum)\n\n    res = pm.math.switch(\n        value.sum(axis=1) &gt; 0,\n        bin_exp * psi,\n        bin_exp * psi + (1 - psi)\n    )\n    \n    return pm.math.log(res)\n\n\n\n\nwith pm.Model() as m0:\n\n    # Priors\n    # inclusion\n    psi = pm.Uniform('psi', 0, 1)  \n\n    # mean catchability \n    mu = pm.Logistic('mu', 0, 1)\n\n    # Linear model\n    mu_matrix = (np.ones((T, M)) * mu).T\n    p = pm.Deterministic('p', pm.math.invlogit(mu_matrix))\n\n    # Likelihood \n    pm.CustomDist(\n        'y',\n        1,\n        p,\n        psi,\n        logp=logp,\n        observed=dolphin_augmented\n    )\n    \npm.model_to_graphviz(m0)\n\n\nFigure 2\n\n\n\n\nwith m0:\n    m0_idata = pm.sample()\n\n\n\n\nwith pm.Model() as mt:\n\n    # Priors\n    # inclusion\n    psi = pm.Uniform('psi', 0, 1)  \n\n    # mean catchability \n    mu = pm.Logistic('mu', 0, 1)\n\n    # time effect\n    sigma_alpha = pm.InverseGamma('sigma_alpha', 4, 3)\n    alpha = pm.Normal('alpha', 0, pm.math.sqrt(sigma_alpha), shape=T)\n\n    # Linear model\n    # nu = mu + pm.math.dot(occasion_covariate, alpha)\n    nu = mu + alpha\n    p = pm.Deterministic('p', pm.math.invlogit(nu))\n\n    # Likelihood \n    pm.CustomDist(\n        'y',\n        1,\n        p,\n        psi,\n        logp=logp,\n        observed=dolphin_augmented\n    )\n    \npm.model_to_graphviz(mt)\n\n\nFigure 3\n\n\n\n\nwith mt:\n    mt_idata = pm.sample()\n    # pass\n\n\n\n\nwith pm.Model() as mb:\n\n    # Priors\n    # inclusion\n    psi = pm.Uniform('psi', 0, 1)  \n\n    # mean catchability \n    mu = pm.Logistic('mu', 0, 1)\n    \n    # behavior effect\n    sigma_beta = pm.InverseGamma('sigma_beta', 4, 3)\n    beta = pm.Normal('beta', 0, pm.math.sqrt(sigma_beta))\n\n    # Linear model\n    nu = mu + behavior_covariate * beta \n    p = pm.Deterministic('p', pm.math.invlogit(nu))\n\n    # Likelihood \n    pm.CustomDist(\n        'y',\n        1,\n        p,\n        psi,\n        logp=logp,\n        observed=dolphin_augmented\n    )\n    \npm.model_to_graphviz(mb)\n\n\nFigure 4\n\n\n\n\nwith mb:\n    mb_idata = pm.sample()\n\n\n\n\nwith pm.Model() as mtb:\n\n    # Priors\n    # inclusion\n    psi = pm.Uniform('psi', 0, 1)  \n\n    # mean catchability \n    mu = pm.Logistic('mu', 0, 1)\n\n    # time effect\n    sigma_alpha = pm.InverseGamma('sigma_alpha', 4, 3)\n    alpha = pm.Normal('alpha', 0, pm.math.sqrt(sigma_alpha), shape=T)\n\n    # behavior effect\n    sigma_beta = pm.InverseGamma('sigma_beta', 4, 3)\n    beta = pm.Normal('beta', 0, pm.math.sqrt(sigma_beta))\n\n    # Linear model\n    nu = mu + alpha + behavior_covariate * beta\n    p = pm.Deterministic('p', pm.math.invlogit(nu))\n\n    # Likelihood \n    pm.CustomDist(\n        'y',\n        1,\n        p,\n        psi,\n        logp=logp,\n        observed=dolphin_augmented\n    )\n    \npm.model_to_graphviz(mtb)\n\n\nFigure 5\n\n\n\n\nwith mtb:\n    mtb_idata = pm.sample()\n\n\n\n\nwith pm.Model() as mh:\n\n    # Priors\n    # inclusion\n    psi = pm.Uniform('psi', 0, 1)  \n\n    # mean catchability \n    mu = pm.Logistic('mu', 0, 1)\n\n    # individual effect\n    sigma_gamma = pm.InverseGamma('sigma_gamma', 4, 3)\n    gamma = pm.Normal('gamma', 0, pm.math.sqrt(sigma_gamma), shape=M)\n\n    # Linear model\n    individual_effect = (np.ones((T, M)) * gamma).T\n    nu = mu + individual_effect\n    p = pm.Deterministic('p', pm.math.invlogit(nu))\n\n    # Likelihood \n    pm.CustomDist(\n        'y',\n        1,\n        p,\n        psi,\n        logp=logp,\n        observed=dolphin_augmented\n    )\n    \npm.model_to_graphviz(mh)\n\n\nFigure 6\n\n\n\n\nwith mh:\n    mh_idata = pm.sample(3000, target_accept=0.99, )\n\n\naz.summary(mh_idata, var_names=['psi', 'mu', 'sigma_gamma'])\n\n\naz.plot_trace(mh_idata, figsize=(8, 6), var_names=['psi', 'mu', 'sigma_gamma']);\n\n\n\n\nwith pm.Model() as mth:\n\n    # Priors\n    # inclusion\n    psi = pm.Beta('psi', 1, 1)  \n\n    # mean catchability \n    mu = pm.Logistic('mu', 0, 1)\n\n    # time effect\n    sigma_alpha = pm.InverseGamma('sigma_alpha', 4, 3)\n    alpha = pm.Normal('alpha', 0, pm.math.sqrt(sigma_alpha), shape=T)\n\n    # individual effect\n    sigma_gamma = pm.InverseGamma('sigma_gamma', 4, 3)\n    gamma = pm.Normal('gamma', 0, pm.math.sqrt(sigma_gamma), shape=M)\n\n    # Linear model\n    individual_effect = (np.ones((T, M)) * gamma).T\n    nu = mu + alpha + individual_effect\n    p = pm.Deterministic('p', pm.math.invlogit(nu))\n\n    # Likelihood \n    pm.CustomDist(\n        'y',\n        1,\n        p,\n        psi,\n        logp=logp,\n        observed=dolphin_augmented\n    )\n    \npm.model_to_graphviz(mth)\n\n\nFigure 7\n\n\n\n\nwith mth:\n    mth_idata = pm.sample(draws=3000, target_accept=0.95, )\n\n\naz.summary(mth_idata, var_names=['psi', 'mu', 'sigma_alpha', 'sigma_gamma', 'alpha'])\n\n\n\n\naz.plot_trace(mth_idata, figsize=(8, 10),\n              var_names=['psi', 'mu', 'sigma_alpha', 'sigma_gamma', 'alpha']);\n\n\nFigure 8\n\n\n\n\n\n\nwith pm.Model() as mbh:\n\n    # Priors\n    # inclusion\n    psi = pm.Beta('psi', 1, 1)  \n\n    # mean catchability \n    mu = pm.Logistic('mu', 0, 1)\n\n    # behavior effect\n    sigma_beta = pm.InverseGamma('sigma_beta', 4, 3)\n    beta = pm.Normal('beta', 0, pm.math.sqrt(sigma_beta))\n    \n    # individual effect\n    sigma_gamma = pm.InverseGamma('sigma_gamma', 4, 3)\n    gamma = pm.Normal('gamma', 0, pm.math.sqrt(sigma_gamma), shape=M)\n\n    # Linear model\n    individual_effect = (np.ones((T, M)) * gamma).T\n    nu = mu + behavior_covariate * beta + individual_effect\n    p = pm.Deterministic('p', pm.math.invlogit(nu))\n\n    # Likelihood \n    pm.CustomDist(\n        'y',\n        1,\n        p,\n        psi,\n        logp=logp,\n        observed=dolphin_augmented\n    )\n    \npm.model_to_graphviz(mbh)\n\n\nFigure 9\n\n\n\n\nwith mbh:\n    mbh_idata = pm.sample(draws=3000, target_accept=0.95, )\n\n\naz.summary(mbh_idata, var_names=['psi', 'mu', 'beta', 'sigma_gamma'])\n\n\naz.plot_trace(mbh_idata, figsize=(8, 10),\n              var_names=['psi', 'mu', 'beta', 'sigma_beta', 'sigma_gamma']);\n\n\n\n\nwith pm.Model() as mtbh:\n\n    # Priors\n    # inclusion\n    psi = pm.Beta('psi', 1, 5)  \n\n    # mean catchability \n    mu = pm.Logistic('mu', 0, 1)\n\n    # time effect\n    sigma_alpha = pm.InverseGamma('sigma_alpha', 4, 3)\n    alpha = pm.Normal('alpha', 0, pm.math.sqrt(sigma_alpha), shape=T)\n\n    # behavior effect\n    sigma_beta = pm.InverseGamma('sigma_beta', 4, 3)\n    beta = pm.Normal('beta', 0, pm.math.sqrt(sigma_beta))\n\n    # individual effect\n    sigma_gamma = pm.InverseGamma('sigma_gamma', 4, 3)\n    gamma = pm.Normal('gamma', 0, pm.math.sqrt(sigma_gamma), shape=M)\n\n    # Linear model\n    individual_effect = (np.ones((T, M)) * gamma).T\n    nu = mu + alpha + behavior_covariate * beta + individual_effect\n    p = pm.Deterministic('p', pm.math.invlogit(nu))\n\n    # Likelihood \n    pm.CustomDist(\n        'y',\n        1,\n        p,\n        psi,\n        logp=logp,\n        observed=dolphin_augmented\n    )\n    \npm.model_to_graphviz(mtbh)\n\n\nFigure 10\n\n\n\n\nwith mtbh:\n    mtbh_idata = pm.sample(draws=2000, )\n\n\naz.summary(mtbh_idata, \n           var_names=['psi', 'mu', 'alpha', 'beta', 'sigma_alpha', 'sigma_beta', 'sigma_gamma'])\n\n\n\n\naz.plot_trace(mtbh_idata, figsize=(8,14),\n           var_names=['psi', 'mu', 'alpha', 'beta', 'sigma_alpha', 'sigma_beta', 'sigma_gamma']);\n\n\nFigure 11\n\n\n\nThe trace plots and summary statistics show convergence issues for many of the individual heterogeneity models. The variance parameter, \\(\\sigma_{\\gamma},\\) seems to sample poorly. Further, models with both behavioral and individual effects lead to extremely large estimates of \\(\\psi\\). This appears to happen regardless of the size of the data augmentation \\(M.\\)\nNote that I upped the target_accept value for some models. This slows the sampler, but lowers the risk of divergence.\n\n\nModel comparison\nNext, I select a model for inference using an approximation of leave-one-out (loo) cross-validation (Vehtari, Gelman, and Gabry 2017). This approximation can be calculated using PyMC. To do so, I calculate the log-likelihood for each model, which is added to the InferenceData object. This makes it possible to compare the models using loo and az.compare.\n\nwith m0:\n    pm.compute_log_likelihood(m0_idata)\n\nwith mt:\n    pm.compute_log_likelihood(mt_idata)\n\nwith mb:\n    pm.compute_log_likelihood(mb_idata)\n\nwith mtb:\n    pm.compute_log_likelihood(mtb_idata)\n\nwith mh:\n    pm.compute_log_likelihood(mh_idata)\n\nwith mth:\n    pm.compute_log_likelihood(mth_idata)\n\nwith mbh:\n    pm.compute_log_likelihood(mbh_idata)\n\nwith mtbh:\n    pm.compute_log_likelihood(mtbh_idata)\n\n\nmodel_dict = {\"m0\": m0_idata, \"mt\": mt_idata, \"mb\": mb_idata, \n              \"mtb\": mtb_idata, \"mh\": mh_idata, \"mth\": mth_idata, \n              \"mbh\": mbh_idata, \"mtbh\": mtbh_idata}\n\ncomparison = az.compare(model_dict)\n\nThe comparison tools notes issues with several of the models, suggesting a lack of robustness. Inspection of the comparison table shows that the struggling models all include the individual effect \\(h.\\) A more thorough analysis would consider reparameterizing the model, e.g., through the non-centered parameterization. In lieu of that, I simply discard the models that fail this test and re-do the comparison with the passing models.\n\ncomparison.round(2)\n\n\ngood_dict = {\"m0\": m0_idata, \"mt\": mt_idata, \"mb\": mb_idata, \"mtb\": mtb_idata}\ngood_comparison = az.compare(good_dict)\ngood_comparison.round(2)\n\n\n\n\naz.plot_compare(good_comparison, figsize=(5, 4));\n\n\nFigure 12\n\n\n\nThe comparison shows that all of the model weight belongs to two models: \\(M_t\\) and \\(M_{tb}.\\)\n\n\nModel averaged predictions\nFinally, we can use the model weights to simulate a weighted posterior of \\(N.\\) To do so, I take a weighted sample of each of the posteriors of \\(N,\\) with the weight dictated by the comparison tool.\n\n\n\nposteriors = [sim_N(good_dict[model]) for model in good_dict]\nweights = [good_comparison.loc[model].weight for model in good_dict]\nsample_count = len(posteriors[0])\n\nl = []\nfor w, p in zip(weights, posteriors):\n    weighted_sample = RNG.choice(p, size=int(w * sample_count))\n    l.append(weighted_sample)\n\nweighted_posterior = np.concatenate(l)\n\nfig, (ax0, ax1) = plt.subplots(2, 1, figsize=(7, 6), sharex=True, sharey=True, tight_layout=True)\n\npal = sns.color_palette(\"Set2\")\n\n# labs = [k for k in good_dict.keys()]\nlabs = [r'$M_{0}$', r'$M_{t}$', r'$M_{b}$', r'$M_{tb}$']\nfor i, p in enumerate(posteriors):\n    ax0.hist(p, color=pal[i], edgecolor='white', bins=60, alpha=0.6, label=labs[i])\n\nax0.set_title(r'Posteriors of $N$')\n# ax1.set_title(r'Weighted posterior')\n\nax0.set_xlim((53, 150))\nax0.legend()\n\nax0.set_ylabel('Number of samples')\nax1.set_ylabel('Number of samples')\n\nax1.hist(weighted_posterior, edgecolor='white', bins=60, alpha=0.9, color=pal[6], label='Weighted')\nax1.legend()\n\nplt.show()\n\n\nFigure 13\n\n\n\nWe can also look at the posterior densities of \\(p\\) from Model \\(M_t,\\) the second most weighted model.\n\n\n\np_samps = az.extract(mt_idata).p\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\na = 0.4\n# ax[0].set_title(\"Poisson\")\npal = sns.color_palette('viridis', T)\nfor t in range(T):\n    label_idx = t % 2\n    if label_idx == 0:\n        az.plot_dist(p_samps[t], ax=ax, color=pal[t], label=f'$t_{{{t}}}$',\n                     plot_kwargs={'linewidth':3, 'alpha': a})\n    else:\n        az.plot_dist(p_samps[t], ax=ax, color=pal[t],\n                     plot_kwargs={'linewidth':3, 'alpha': a})\n\nax.set_title(r'Posterior densities of $p$ from $M_t$')\nax.set_xlabel(r'$p$')\n\nplt.show()\n\n\nFigure 14\n\n\n\nThis notebook demonstrates a simple way to compare models using leave one out cross-validation (loo) and a classic example from capture-recapture. This is just one way, however, to perform model comparison using PyMC. Perhaps a more effective solution for this problem would be placing a shrinkage prior on the \\(\\sigma\\) parameters.\n\n%load_ext watermark\n\n%watermark -n -u -v -iv -w  \n\n\n\n\n\n\nReferences\n\nKing, Ruth, and SP2526632 Brooks. 2008. “On the Bayesian Estimation of a Closed Population Size in the Presence of Heterogeneity and Model Uncertainty.” Biometrics 64 (3): 816–24.\n\n\nVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and WAIC.” Statistics and Computing 27: 1413–32.\n\n\nWilson, Ben, Philip S Hammond, and Paul M Thompson. 1999. “Estimating Size and Assessing Trends in a Coastal Bottlenose Dolphin Population.” Ecological Applications 9 (1): 288–300."
  },
  {
    "objectID": "pymc-scr.html",
    "href": "pymc-scr.html",
    "title": "Spatial capture-recapture",
    "section": "",
    "text": "In this notebook, I show how to train spatial capture-recapture (SCR) models in PyMC. SCR expands upon traditional capture-recapture by incorporating the location of the traps in the analysis. This matters because, typically, animals that live near a particular trap are more likely to be caught in it. In doing so, SCR links individual-level processes to the population-level, expanding the scientific scope of simple designs.\nIn this notebook, I train the simplest possible SCR model, SCR0 (Royle et al. 2013, chap. 5), where the goal is estimating the true population size \\(N\\). Similar to the other closed population notebooks, I do so using parameter-expanded data-augmentation (PX-DA). I also borrow the concept of the detection function from the distance sampling notebook.\nAs a motivating example, I use the ovenbird mist netting dataset provided by Murray Efford via the secr package in R. The design of the study is outlined in Efford, Dawson, and Robbins (2004) and Borchers and Efford (2008). In this dataset, ovenbirds were trapped in 44 mist nets over 8 to 10 consecutive days during the summers of 2005 to 2009.\n%config InlineBackend.figure_format = 'retina'\n\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nimport pymc_extras as pmx\nimport pytensor.tensor as pt\nimport seaborn as sns\n\n# only necessary on MacOS Sequoia\n# https://discourse.pymc.io/t/pytensor-fails-to-compile-model-after-upgrading-to-mac-os-15-4/16796/5\nimport pytensor\npytensor.config.cxx = '/usr/bin/clang++'\n\n# hyper parameters\nSEED = 42\nRNG = np.random.default_rng(SEED)\nBUFFER = 100\nM = 200\n\n# plotting defaults\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\nsns.set_palette(\"tab10\")\n\ndef invlogit(x):\n    '''Inverse logit function'''\n    return 1 / (1 + np.exp(-x))\n\ndef euclid_dist(X, S, library='np'):\n    '''Pairwise euclidian distance between points in (M, 2) and (N, 2) arrays'''\n    diff = X[np.newaxis, :, :] - S[:, np.newaxis, :]\n\n    if library == 'np':\n        return np.sqrt(np.sum(diff ** 2, axis=-1))\n\n    elif library == 'pm':\n        return pm.math.sqrt(pm.math.sum(diff ** 2, axis=-1))\n\ndef half_normal(d, s, library='np'):\n    '''Half normal detection function.'''\n    if library == 'np':\n        return np.exp( - (d ** 2) / (2 * s ** 2))\n\n    elif library == 'pm':\n        return pm.math.exp( - (d ** 2) / (2 * s ** 2))\n\ndef exponential(d, s, library='np'):\n    '''Negative exponential detection function.'''\n    if library == 'np':\n        return np.exp(- d / s)\n\n    elif library == 'pm':\n        return pm.math.exp(- d / s)\n\n# coordinates for each trap\novenbird_trap = pd.read_csv('ovenbirdtrap.txt', delimiter=' ')\ntrap_count, _ = ovenbird_trap.shape\n\n# information about each trap\ntrap_x = ovenbird_trap.x\ntrap_y = ovenbird_trap.y\nX = ovenbird_trap[['x', 'y']].to_numpy()\n\n# define the state space around the traps\nx_max = trap_x.max() + BUFFER\ny_max = trap_y.max() + BUFFER\nx_min = trap_x.min() - BUFFER\ny_min = trap_y.min() - BUFFER\n\n# scale for plotting\nscale = (y_max - y_min) / (x_max - x_min)\n\n# plot the trap locations\nplot_width = 2\nplot_height = plot_width * scale\nfig, ax = plt.subplots(figsize=(plot_width, plot_height))\n\n# plot the traps\nax.scatter(trap_x, trap_y, marker='x', s=40, linewidth=1.5, color='C1')\nax.set_ylim((y_min, y_max))\nax.set_xlim((x_min, x_max))\n\nax.annotate(\n    '44 nets\\n30m apart', ha='center',\n    xy=(55, -150), xycoords='data', color='black',\n    xytext=(40, 30), textcoords='offset points',\n    arrowprops=dict(arrowstyle=\"-&gt;\", color='black', linewidth=1,\n                    connectionstyle=\"angle3,angleA=90,angleB=0\"))\n\n# aesthetics\nax.set_aspect('equal')\nax.set_title('Mist net locations')\nax.grid(False)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Locations of the mist nets in the ovenbird dataset (Efford, Dawson, and Robbins 2004)\nOne difference between spatial and traditional (non-spatial) capture is the addition of the trap identifier in the capture history. Whereas a traditional capture history is [individual, occasion], a spatial capture history might be [individual, occasion, trap].\nIn the ovenbird example, I ignore the year dimension, pooling parameters across years, which allows for better estimation of the detection parameters. My hack for doing so is treating every band/year combination as a unique individual in a combined year capture history. This is easy to implement, creates an awkward interpretation of \\(N\\) (see below).\n# ovenbird capture history\noven_ch = pd.read_csv('ovenbirdcapt.txt', delimiter=' ')\n\n# create a unique bird/year identifier for each individual\noven_ch['ID'] = oven_ch.groupby(['Year','Band']).ngroup()\noccasion_count = oven_ch.Day.max()\n\n# merge the datasets, making sure that traps with no detections are included\novenbird = (\n    ovenbird_trap.merge(oven_ch[['ID', 'Net', 'Day']], how='left')\n      [['ID', 'Day', 'Net', 'x', 'y']]\n      .sort_values('ID')\n      .reset_index(drop=True)\n)\n\novenbird.head(10)\n\n\n\n\n\n\n\n\nID\nDay\nNet\nx\ny\n\n\n\n\n0\n0.0\n1.0\n2\n-50.0\n-255.0\n\n\n1\n1.0\n9.0\n20\n-50.0\n285.0\n\n\n2\n1.0\n1.0\n15\n-50.0\n135.0\n\n\n3\n2.0\n6.0\n17\n-50.0\n195.0\n\n\n4\n2.0\n9.0\n27\n49.0\n165.0\n\n\n5\n2.0\n1.0\n15\n-50.0\n135.0\n\n\n6\n2.0\n1.0\n14\n-50.0\n105.0\n\n\n7\n3.0\n1.0\n41\n49.0\n-255.0\n\n\n8\n3.0\n3.0\n39\n49.0\n-195.0\n\n\n9\n3.0\n1.0\n42\n49.0\n-285.0",
    "crumbs": [
      "Code",
      "PyMC",
      "Spatial capture-recapture"
    ]
  },
  {
    "objectID": "pymc-scr.html#nuts-sampler",
    "href": "pymc-scr.html#nuts-sampler",
    "title": "Spatial capture-recapture",
    "section": "NUTS sampler",
    "text": "NUTS sampler\nThroughout these notebooks, I have been estimating parameters with the default NUTS sampler in PyMC. One of PyMC’s great strengths is that it allows you to select different samplers with the same model code. We saw one version of this in the occupancy notebook, where we had a Binary Gibbs Sampler for the discrete \\(z_i\\) state and a NUTS sampler for the continuous parameters. But PyMC goes one step further, allowing you to choose different NUTS samplers. These include numpyro, which is itself a package for MCMC that can run on GPUs. Additionally, there is the nutpie sampler that is ultimately written in Rust. In most cases, nutpie is much faster than the default PyMC sampler. That said, nutpie can slightly less flexible.\nSince the SCR models in this notebook take a little longer to run, I will use the nutpie sampler throughout. The difference can be substantial. For instance, the unknown \\(N\\) model below takes 39 seconds with the default sampler, and 13 seconds with the nutpie sampler.\n\nwith known:\n    known_idata = pm.sample(nuts_sampler='nutpie')\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.31\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.32\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.31\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.34\n                    15\n                \n            \n            \n        \n    \n\n\n\n\naz.summary(known_idata, var_names=['g0', 'sigma'])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\ng0\n0.022\n0.003\n0.017\n0.028\n0.000\n0.000\n1026.0\n1637.0\n1.00\n\n\nsigma\n94.726\n7.652\n80.702\n109.081\n0.303\n0.195\n645.0\n1171.0\n1.01\n\n\n\n\n\n\n\n\naz.plot_trace(\n    known_idata,\n    var_names=['g0', 'sigma'],\n    figsize=(8,4),\n    lines=[(\"g0\", {}, [g0_true]), (\"sigma\", {}, [sigma_true])]\n);\n\n\n\n\n\n\n\nFigure 3: Trace plots for model where \\(N\\) is known. The true parameter values are shown by vertical and horizontal lines.\n\n\n\n\n\nThe trace plots show reasonable agreement between the true parameter values and the estimated values, although \\(g_0\\) appears to be overestimated.",
    "crumbs": [
      "Code",
      "PyMC",
      "Spatial capture-recapture"
    ]
  },
  {
    "objectID": "msom-pr.html",
    "href": "msom-pr.html",
    "title": "Community occupancy",
    "section": "",
    "text": "In this notebook, I explore fitting community occupancy models in PyMC. Community occupancy models are a multi-species extension of standard occupancy models. The benefit of these models is that they estimate occupancy and detection better that single species models by treating each species as a random effect. Further, through data augmentation, they can estimate the richness of the supercommunity, that is, the total number of species that use the study area during the surveys."
  },
  {
    "objectID": "msom-pr.html#known-n",
    "href": "msom-pr.html#known-n",
    "title": "Community occupancy",
    "section": "Known \\(N\\)",
    "text": "Known \\(N\\)\nFirst, I fit the the known \\(N\\) version of the model. The goal of this version is to estimate occurrence and detection for each species, without estimating species richness.\nThis notebook makes extensive use of the coords feature in PyMC. Coords makes it easier to incorporate the species-level effects via the multivariate normal. I use a \\(\\text{Normal}(0, 2)\\) prior for both \\(\\mu\\) parameters, and a LKJ Cholesky covariance prior for \\(\\mathbf{\\Sigma}.\\)\n\ncoords = {'process': ['detection', 'occurrence'], \n          'process_bis': ['detection', 'occurrence'],\n          'species': lookup}\n\nwith pm.Model(coords=coords) as known:\n\n    # priors for community-level means for detection and occurrence\n    mu = pm.Normal('mu', 0, 2, dims='process')\n\n    # prior for covariance matrix for occurrence and detection\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"chol\", n=2, eta=2.0, sd_dist=pm.Exponential.dist(1.0, shape=2)\n    )\n    cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=(\"process\", \"process_bis\"))\n\n    # species-level occurrence and detection probabilities on logit-scale  \n    ab = pm.MvNormal(\"ab\", mu, chol=chol, dims=(\"species\", \"process\"))\n\n    # probability of detection. newaxis allows for broadcasting\n    a = ab[:, 0][:, np.newaxis]\n    p = pm.Deterministic(\"p\", pm.math.invlogit(a))\n\n    # probability of detection. newaxis allows for broadcasting\n    b = ab[:, 1][:, np.newaxis]\n    psi = pm.Deterministic(\"psi\", pm.math.invlogit(b))\n\n    # likelihood\n    pm.ZeroInflatedBinomial('Y', p=p, psi=psi, n=K, observed=Y)\n\npm.model_to_graphviz(known)\n\n\n\n\n\n\n\nFigure 2: Visual representation of the known \\(N\\) version of the community occupancy model.\n\n\n\n\n\n\nwith known:\n    known_idata = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, chol, ab]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:43&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n/Users/philtpatton/miniforge3/envs/pymc/lib/python3.11/site-packages/pytensor/compile/function/types.py:970: RuntimeWarning: invalid value encountered in accumulate\n  self.vm()\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 43 seconds.\n\n\n\nmu_hat_royle = [-1.11, -1.7]\naz.plot_trace(known_idata, var_names=['mu'], figsize=(8,2),\n              lines=[(\"mu\", {}, [mu_hat_royle])]);\n\n\n\n\n\n\n\nFigure 3: Trace plots for the community level means of occupancy and abundance in the known \\(N\\) version of the BBS model. The estimates from Royle and Dorazio (2008) are shown by vertical and horizontal lines.\n\n\n\n\n\n\nsamps = az.extract(known_idata, var_names='ab')\nab_mean = samps.mean(axis=2)\n\nfig, ax = plt.subplots(figsize=(5,4))\nax.scatter(invlogit(ab_mean[:, 1]), invlogit(ab_mean[:, 0]), alpha=0.5)\nax.set_xlim((0, 1))\nax.set_xlabel('Occupancy probability')\nax.set_ylim((0, 0.8))\nax.set_ylabel('Detection probability')\nplt.show()\n\n\n\n\n\n\n\nFigure 4: Species-level probabilities of detection and occupancy.\n\n\n\n\n\nThe estimates of the community-level means is quite close to the estimates from Royle and Dorazio (2008). We can visualize the species-level probabilities of detection and occupancy. Compare with Figure 12.3 in Royle and Dorazio (2008)."
  },
  {
    "objectID": "msom-pr.html#unknown-n",
    "href": "msom-pr.html#unknown-n",
    "title": "Community occupancy",
    "section": "Unknown \\(N\\)",
    "text": "Unknown \\(N\\)\nNext, I train the unknown \\(N\\) version of the model. Like many other notebooks in this series, it relies on augmenting the detection histories with all-zero histories. These represent the detection histories for species that may use the study site, but were not detected over the \\(K=11\\) surveys. I also augment the species names in the coords dict, such that we can still use the dims argument in the multivariate normal. Mirroring Royle and Dorazio (2008), I augment the history \\(M - n\\) all-zero histories, where \\(M=250\\) and \\(n\\) is the number of species detected during the survey.\nSimilar to the occupancy notebook, I use a CustomDist to model the augmented history. This accounts for the “row-level” zero-inflation, whereby we know that the species is included in the super community if it was detected along the BBS route. The only difference with this logp is that it uses a ZeroInflatedBinomial distribution under the hood, rather than a Bernoulli, and uses the parameter \\(\\Omega\\) to account for the row-level inflation.\n\nM = 250\nall_zero_history = np.zeros((M - n, J))\nY_augmented = np.row_stack((Y, all_zero_history))\n\naug_names = [f'aug{i}' for i in np.arange(M - n)]\nspp_aug = np.concatenate((lookup, aug_names))\n\ncoords = {'process': ['detection', 'occurrence'], \n          'process_bis': ['detection', 'occurrence'],\n          'species_aug': spp_aug}\n\ndef logp(x, psi, n, p, omega):\n    '''A zero-inflated, zero-inflated model.'''\n    rv = pm.ZeroInflatedBinomial.dist(psi=psi, n=n, p=p)\n    lp = pm.logp(rv, x)\n    lp_sum = lp.sum(axis=1)\n    lp_exp = pm.math.exp(lp_sum)\n    \n    res = pm.math.switch(\n        x.sum(axis=1) &gt; 0,\n        lp_exp * omega,\n        lp_exp * omega + (1 - omega)\n    )\n    \n    return pm.math.log(res)\n\nwith pm.Model(coords=coords) as unknown:\n\n    # priors for inclusion\n    omega = pm.Uniform('omega', 0, 1)\n    \n    # priors for community-level means for detection and occurrence\n    mu = pm.Normal('mu', 0, 2, dims='process')\n\n    # prior for covariance matrix for occurrence and detection\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"chol\", n=2, eta=2.0, sd_dist=pm.Exponential.dist(1.0, shape=2)\n    )\n    cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=(\"process\", \"process_bis\"))\n\n    # species-level occurrence and detection probabilities on logit-scale  \n    ab = pm.MvNormal(\"ab\", mu, chol=chol, dims=(\"species_aug\", \"process\"))\n\n    # probability of detection\n    a = ab[:, 0]\n    p = pm.Deterministic(\"p\", pm.math.invlogit(a))\n\n    # probability of occurrence\n    b = ab[:, 1]\n    psi = pm.Deterministic(\"psi\", pm.math.invlogit(b))\n\n    # likelihood\n    pm.CustomDist(\n        'Y',\n        psi[:, np.newaxis],\n        K,\n        p[:, np.newaxis],\n        omega,\n        logp=logp,\n        observed=Y_augmented\n    )\n    \npm.model_to_graphviz(unknown)\n\n\n\n\n\n\n\nFigure 5: Visual representation of the unknown \\(N\\) version of the BBS model.\n\n\n\n\n\n\nwith unknown:\n    unknown_idata = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [omega, mu, chol, ab]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 03:15&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 196 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n\n\n\nomega_hat_royle = [0.55]\naz.plot_trace(unknown_idata, var_names=['omega'], figsize=(8,2), \n              lines=[(\"omega\", {}, [omega_hat_royle])]);\n\n\n\n\n\n\n\nFigure 6: Trace plots for the inclusion parameter for the unknown \\(N\\) version of the BBS model. The estimate from Royle and Dorazio (2008) are shown by vertical and horizontal lines.\n\n\n\n\n\nI can plot the posterior distribution of species richness \\(N.\\) This is slightly more complicated than before sinc there is an additional level of zero-inflation (included and never detected or not-included) in this model compared to the occupancy model (present and never detection or not present).\n\n# relevant posterior samples\npost = az.extract(unknown_idata)\no_samps = post.omega.to_numpy()\npsi_samps = post.psi.to_numpy()[n:, :]\np_samps = post.p.to_numpy()[n:, :]\n\n# probability that the animal was never detected during the survey if present\np_not_detected = (1 - p_samps) ** K\n\n# probability of a zero detection history \np_zero_hist = psi_samps * p_not_detected + (1 - psi_samps)\n\n# probability that the species was included in the given the all-zero history\np_included = (o_samps * p_zero_hist ** J) / (o_samps * p_zero_hist ** J + (1 - o_samps))\n\n# posterior samples of N\nnumber_undetected = RNG.binomial(1, p_included).sum(axis=0)\nN_samps = n + number_undetected\n\np_samps.shape\n\n# posterior distribution \n# N_hat_royle = 138\n# fig, ax = plt.subplots(figsize=(6, 4))\n# ax.hist(N_samps, edgecolor='white', bins=25)\n# ax.set_xlabel('Species richness $N$')\n# ax.set_ylabel('Posterior samples')\n# ax.axvline(N_hat_royle, linestyle='--', color='C1')\n# ax.axvline(N_samps.mean(), linestyle='--', color='C2')\n# plt.show()\n\n\n\n(151, 1, 4000)\n\n\nFigure 7: Posterior distribution of species richness from the BBS model. The yellow dashed line is \\(\\hat{N}\\) from this model, while the red dashed line shows \\(\\hat{N}\\) from (royledorazio2008?)."
  },
  {
    "objectID": "scr.html",
    "href": "scr.html",
    "title": "Spatial capture-recapture",
    "section": "",
    "text": "In this notebook, I show how to train spatial capture-recapture (SCR) models in PyMC. SCR expands upon traditional capture-recapture by incorporating the location of the traps in the analysis. This matters because, typically, animals that live near a particular trap are more likely to be caught in it. In doing so, SCR links individual-level processes to the population-level, expanding the scientific scope of simple designs.\nIn this notebook, I train the simplest possible SCR model, SCR0 (Royle et al. 2013, chap. 5), where the goal is estimating the true population size \\(N\\). Similar to the other closed population notebooks, I do so using parameter-expanded data-augmentation (PX-DA). I also borrow the concept of the detection function from the distance sampling notebook.\nAs a motivating example, I use the ovenbird mist netting dataset provided by Murray Efford via the secr package in R. The design of the study is outlined in Efford, Dawson, and Robbins (2004) and Borchers and Efford (2008). In this dataset, ovenbirds were trapped in 44 mist nets over 8 to 10 consecutive days during the summers of 2005 to 2009.\n%config InlineBackend.figure_format = 'retina'\n\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nimport pymc_extras as pmx\nimport pytensor.tensor as pt\nimport seaborn as sns\n\n# only necessary on MacOS Sequoia\n# https://discourse.pymc.io/t/pytensor-fails-to-compile-model-after-upgrading-to-mac-os-15-4/16796/5\nimport pytensor\npytensor.config.cxx = '/usr/bin/clang++'\n\n# hyper parameters\nSEED = 42\nRNG = np.random.default_rng(SEED)\nBUFFER = 100\nM = 200\n\n# plotting defaults\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\nsns.set_palette(\"tab10\")\n\ndef invlogit(x):\n    '''Inverse logit function'''\n    return 1 / (1 + np.exp(-x))\n\ndef euclid_dist(X, S, library='np'):\n    '''Pairwise euclidian distance between points in (M, 2) and (N, 2) arrays'''\n    diff = X[np.newaxis, :, :] - S[:, np.newaxis, :]\n\n    if library == 'np':\n        return np.sqrt(np.sum(diff ** 2, axis=-1))\n\n    elif library == 'pm':\n        return pm.math.sqrt(pm.math.sum(diff ** 2, axis=-1))\n\ndef half_normal(d, s, library='np'):\n    '''Half normal detection function.'''\n    if library == 'np':\n        return np.exp( - (d ** 2) / (2 * s ** 2))\n\n    elif library == 'pm':\n        return pm.math.exp( - (d ** 2) / (2 * s ** 2))\n\ndef exponential(d, s, library='np'):\n    '''Negative exponential detection function.'''\n    if library == 'np':\n        return np.exp(- d / s)\n\n    elif library == 'pm':\n        return pm.math.exp(- d / s)\n\n# coordinates for each trap\novenbird_trap = pd.read_csv('ovenbirdtrap.txt', delimiter=' ')\ntrap_count, _ = ovenbird_trap.shape\n\n# information about each trap\ntrap_x = ovenbird_trap.x\ntrap_y = ovenbird_trap.y\nX = ovenbird_trap[['x', 'y']].to_numpy()\n\n# define the state space around the traps\nx_max = trap_x.max() + BUFFER\ny_max = trap_y.max() + BUFFER\nx_min = trap_x.min() - BUFFER\ny_min = trap_y.min() - BUFFER\n\n# scale for plotting\nscale = (y_max - y_min) / (x_max - x_min)\n\n# plot the trap locations\nplot_width = 2\nplot_height = plot_width * scale\nfig, ax = plt.subplots(figsize=(plot_width, plot_height))\n\n# plot the traps\nax.scatter(trap_x, trap_y, marker='x', s=40, linewidth=1.5, color='C1')\nax.set_ylim((y_min, y_max))\nax.set_xlim((x_min, x_max))\n\nax.annotate(\n    '44 nets\\n30m apart', ha='center',\n    xy=(55, -150), xycoords='data', color='black',\n    xytext=(40, 30), textcoords='offset points',\n    arrowprops=dict(arrowstyle=\"-&gt;\", color='black', linewidth=1,\n                    connectionstyle=\"angle3,angleA=90,angleB=0\"))\n\n# aesthetics\nax.set_aspect('equal')\nax.set_title('Mist net locations')\nax.grid(False)\nplt.show()\n\n\nFigure 1\nOne difference between spatial and traditional (non-spatial) capture is the addition of the trap identifier in the capture history. Whereas a traditional capture history is [individual, occasion], a spatial capture history might be [individual, occasion, trap].\nIn the ovenbird example, I ignore the year dimension, pooling parameters across years, which allows for better estimation of the detection parameters. My hack for doing so is treating every band/year combination as a unique individual in a combined year capture history. This is easy to implement, creates an awkward interpretation of \\(N\\) (see below).\n# ovenbird capture history\noven_ch = pd.read_csv('ovenbirdcapt.txt', delimiter=' ')\n\n# create a unique bird/year identifier for each individual\noven_ch['ID'] = oven_ch.groupby(['Year','Band']).ngroup()\noccasion_count = oven_ch.Day.max()\n\n# merge the datasets, making sure that traps with no detections are included\novenbird = (\n    ovenbird_trap.merge(oven_ch[['ID', 'Net', 'Day']], how='left')\n      [['ID', 'Day', 'Net', 'x', 'y']]\n      .sort_values('ID')\n      .reset_index(drop=True)\n)\n\novenbird.head(10)"
  },
  {
    "objectID": "scr.html#simulation",
    "href": "scr.html#simulation",
    "title": "Spatial capture-recapture",
    "section": "Simulation",
    "text": "Simulation\nBefore estimating the parameters, I perform a small simulation. The simulation starts with a core idea of SCR: the activity center. The activity center \\(\\mathbf{s}_i\\) is the most likely place that you’d find an individual \\(i\\) over the course of the trapping study. In this case, I assume that activity centers are uniformly distributed across the sample space.\nI compute the probability of detection for individual \\(i\\) at trap \\(j\\) as \\(p_{i,j}=g_0 \\exp(-d_{i,j}^2/2\\sigma^2),\\) where \\(g_0\\) is the probability of detecting an individual when it’s activity center is at the trap, \\(d_{i,j}\\) is the euclidean distance between the trap and the activity center, and \\(\\sigma\\) is the detection range parameter.\n\n# true population size\nN = 150\n\n# simulate activity centers\nS_true = RNG.uniform((x_min, y_min), (x_max, y_max), (N, 2))\n\n# true distance between the trap and the activity centers\nd_true = euclid_dist(X, S_true)\n\n# detection parameters\ng0_true = 0.025\nsigma_true = 73\n\n# simulate the number of captures at each trap for each individual\ncapture_probability = g0_true * half_normal(d_true, sigma_true)\nsim_Y = RNG.binomial(occasion_count, capture_probability)\n\n# filter out undetected individuals\nwas_detected = sim_Y.sum(axis=1) &gt; 0\nsim_Y_det = sim_Y[was_detected]\nn_detected = int(was_detected.sum())\n\nFollowing Royle et al. (2013), Chapter 5, I first fit the version of the model where we assume that we know the true population size. In this case, I’m only estimating the detection parameters and the activity center locations.\n\n\n\n# upper bound for the uniform prior on sigma\nU_SIGMA = 150\n\nwith pm.Model() as known:\n\n    # priors for the activity centers\n    S = pm.Uniform('S', (x_min, y_min), (x_max, y_max), shape=(n_detected, 2))\n\n    # priors for the detection parameters\n    g0 = pm.Uniform('g0', 0, 1)\n    sigma = pm.Uniform('sigma', 0, U_SIGMA)\n\n    # probability of capture for each individual at each trap\n    distance = euclid_dist(X, S, 'pm')\n    p = pm.Deterministic('p', g0 * half_normal(distance, sigma))\n\n    # likelihood\n    pm.Binomial(\n        'y',\n        p=p,\n        n=occasion_count,\n        observed=sim_Y_det\n    )\n\npm.model_to_graphviz(known)\n\n\nFigure 2\n\n\n\n\nwith known:\n    known_idata = pm.sample(nuts_sampler='nutpie')\n\n\naz.summary(known_idata, var_names=['g0', 'sigma'])\n\n\n\n\naz.plot_trace(\n    known_idata,\n    var_names=['g0', 'sigma'],\n    figsize=(8,4),\n    lines=[(\"g0\", {}, [g0_true]), (\"sigma\", {}, [sigma_true])]\n);\n\n\nFigure 3\n\n\n\nThe trace plots show reasonable agreement between the true parameter values and the estimated values, although \\(g_0\\) appears to be overestimated."
  },
  {
    "objectID": "scr.html#ovenbird-density",
    "href": "scr.html#ovenbird-density",
    "title": "Spatial capture-recapture",
    "section": "Ovenbird density",
    "text": "Ovenbird density\nNow, I estimate the density \\(D\\) for the ovenbird population. Like distance sampling, SCR can robustly estimate the density of the population, regardless of the size of the state space. The difference between the model above and this one is that we use PX-DA to estimate the inclusion probability \\(\\psi,\\) and subsequently \\(N.\\) First, I convert the DataFrame to a (n_detected, n_traps) array of binomial counts.\n\ndef get_Y(ch):\n    '''Get a (individual_count, trap_count) array of detections.'''\n\n    # count the number of detections per individual per trap\n    detection_counts = pd.crosstab(ch.ID, ch.Net, dropna=False)\n\n    # remove the ghost nan individual\n    detection_counts = detection_counts.loc[~detection_counts.index.isna()]\n\n    Y = detection_counts.to_numpy()\n    return Y\n\nY = get_Y(ovenbird)\ndetected_count, trap_count = Y.shape\n\n# augmented spatial capture histories with all zero histories\nall_zero_history = np.zeros((M - detected_count, trap_count))\nY_augmented = np.vstack((Y, all_zero_history))\n\nAs with the other closed models in this series, I will write the model in terms of the latent inclusion state \\(z_i\\). Interestingly, .\n\n\n\nwith pm.Model() as oven:\n\n    # Priors\n    # activity centers\n    S = pm.Uniform('S', (x_min, y_min), (x_max, y_max), shape=(M, 2))\n\n    # capture parameters\n    g0 = pm.Uniform('g0', 0, 1)\n    sigma = pm.Uniform('sigma', 0, U_SIGMA)\n\n    # inclusion probability\n    psi = pm.Beta('psi', 0.001, 1)\n\n    # compute the capture probability\n    distance = euclid_dist(X, S, 'pm')\n    p = pm.Deterministic('p', g0 * half_normal(distance, sigma))\n\n    # inclusion state\n    z = pm.Bernoulli('z', psi, shape=M)\n\n    # likelihood\n    mu_y = z[:, None] * p\n    pm.Binomial('y', p=mu_y, n=occasion_count, observed=Y_augmented)\n\npm.model_to_graphviz(oven)\n\n\nFigure 4\n\n\n\n\noven_marginal = pmx.marginalize(oven, ['z'])\nwith oven_marginal:\n    oven_idata = pm.sample(nuts_sampler='nutpie')\n\n\naz.summary(oven_idata, var_names=['g0', 'sigma', 'psi'])\n\n\n\n\ng0_mle = [0.025]\nsigma_mle = [73]\n\naz.plot_trace(\n    oven_idata,\n    var_names=['g0', 'sigma'],\n    figsize=(8,4),\n    lines=[(\"g0\", {}, [g0_mle]), (\"sigma\", {}, [sigma_mle])]\n);\n\n\nFigure 5\n\n\n\nThe estimates are quite close to the maximum likelihood estimates, which I estimated with the secr package in R.\nFinally, I estimate density \\(D\\) using the results. As in the closed capture-recapture and distance sampling notebooks, I use the posterior samples of \\(\\psi\\) and \\(M\\) to sample the posterior of \\(N.\\) This \\(N,\\) however, has an awkward interpretation because I pooled across the years by combining all the detection histories. To get around this, I compute the average annual abundance by dividing by the total number of years in the sample. Then, I divide by the area of the state space.\n\ndef sim_N(idata, n, K):\n\n    psi_samps = az.extract(idata).psi.to_numpy()\n    p_samps = az.extract(idata).p\n    p_samps_undet = p_samps[n:, :, :]\n\n    bin_probs = (1 - p_samps_undet) ** K\n    bin_prod = bin_probs.prod(axis=1)\n    p_included = (bin_prod * psi_samps) / (bin_prod * psi_samps  + (1 - psi_samps))\n\n    number_undetected = RNG.binomial(1, p_included).sum(axis=0)\n    N_samps = n + number_undetected\n\n    return N_samps\n\n\n\n\nN_samps = sim_N(oven_idata, detected_count, occasion_count)\n\n# kludgy way of calculating avergage abundance\nyear_count = 5\naverage_annual_abundance = N_samps // year_count\n\n# area of the state space in terms of hectares\nha = 100 * 100\nmask_area = (x_max - x_min) * (y_max - y_min) / ha\n\n# density\nD_samples = average_annual_abundance / mask_area\nD_mle = 1.262946\n\nfig, ax = plt.subplots(figsize=(4,4))\nax.hist(D_samples, edgecolor='white', bins=13)\nax.axvline(D_mle, linestyle='--',color='C1')\nax.set_xlabel('Ovenbirds per hectare')\nax.set_ylabel('Number of samples')\nax.text(1.4, 800, rf'$\\hat{{D}}$={D_samples.mean():.2f}', va='bottom', ha='left')\nplt.show()\n\n\nFigure 6\n\n\n\nSometimes, the location of the activity centers is of interest. Below, I plot the posterior median for the activity centers for the detected individuals.\n\n\n\ns_samps = az.extract(oven_idata).S\ns_mean = np.median(s_samps[:detected_count], axis=2)\n\n# plot the trap locations\nplot_width = 3\nplot_height = plot_width * scale\nfig, ax = plt.subplots(figsize=(plot_width, plot_height))\n\n# plot the traps\nax.scatter(trap_x, trap_y, marker='x', s=40, linewidth=1.5, color='C1')\nax.set_ylim((y_min, y_max))\nax.set_xlim((x_min, x_max))\n\n# plot the mean activity centers\nax.scatter(s_mean[:, 0], s_mean[:, 1], marker='o', s=4, color='C0')\n\n# aesthetics\nax.set_aspect('equal')\nax.set_title('Estimated activity centers')\nax.grid(False)\n\n\nFigure 7\n\n\n\nWe can also look at the uncertainty around those estimates. Below, I plot the posterior distribution of the activity centers for two individuals.\n\n\n\none = 49\none_samps = s_samps[one]\n\ntwo = 2\ntwo_samps = s_samps[two]\n\nfig, ax = plt.subplots(figsize=(plot_width, plot_height))\n\n# plot the traps\nax.scatter(trap_x, trap_y, marker='x', s=40, linewidth=1.5, color='tab:cyan')\nax.set_ylim((y_min, y_max))\nax.set_xlim((x_min, x_max))\n\n# plot the distributions of the activity centers\nax.scatter(one_samps[0], one_samps[1], marker='o', s=1, color='tab:pink', alpha=0.4)\nax.scatter(two_samps[0], two_samps[1], marker='o', s=1, color='tab:purple', alpha=0.4)\n\n# plot the mean\nax.scatter(one_samps[0].mean(), one_samps[1].mean(), marker='o', s=40, color='w')\nax.scatter(two_samps[0].mean(), two_samps[1].mean(), marker='o', s=40, color='w')\n\n# add the label\nax.text(one_samps[0].mean(), one_samps[1].mean() + 5, f'{one}', ha='center', va='bottom')\nax.text(two_samps[0].mean(), two_samps[1].mean() + 5, f'{two}', ha='center', va='bottom')\n\n# aesthetics\nax.set_aspect('equal')\nax.set_title('Posterior of two activity centers')\nax.grid(False)\nplt.show()\n\n\nFigure 8\n\n\n\nFinally, I plot the posterior distribution of the detection function.\n\n\n\nxx = np.arange(BUFFER * 2)\n\nsigma_samps = az.extract(oven_idata).sigma.values.flatten()\ng0_samps = az.extract(oven_idata).g0.values.flatten()\n\np_samps = np.array(\n    [g * half_normal(xx, s) for g, s in zip(g0_samps, sigma_samps)]\n)\n\np_mean = p_samps.mean(axis=0)\np_low = np.quantile(p_samps, 0.02, axis=0)\np_high = np.quantile(p_samps, 0.98, axis=0)\n\nfig, ax = plt.subplots(figsize=(5,4))\n\nax.plot(xx, p_mean, '-')\nax.fill_between(xx, p_low, p_high, alpha=0.2)\n\nax.set_title('Detection function')\nax.set_ylabel(r'$p$')\nax.set_xlabel(r'Distance (m)')\n\nplt.show()\n\n\nFigure 9\n\n\n\n\n%load_ext watermark\n\n%watermark -n -u -v -iv -w"
  },
  {
    "objectID": "pymc-jssa.html",
    "href": "pymc-jssa.html",
    "title": "Jolly-Seber",
    "section": "",
    "text": "In this notebook, I explore the Jolly-Seber model (technically the JS-Schwarz-Arnason, JSSA or POPAN, parameterization) for estimating survival and abundance using capture recapture data. JSSA is very similar to the CJS framework, except that it also models entry into the population, permitting esimation of the superpopulation size. Like the CJS notebook, I have drawn considerable inspiration from Austin Rochford’s notebook on capture-recapture in PyMC, the second chapter of my dissertation, and McCrea and Morgan (2014).\nAs a demonstration of the JSSA framework, I use the classic European dipper data of Lebreton et al. (1992). I first convert the dataset into the \\(M\\)-array, since the data is in capture history format.\n\n%config InlineBackend.figure_format = 'retina'\n\nfrom pymc.distributions.dist_math import factln\nfrom scipy.linalg import circulant\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc as pm\nimport pytensor.tensor as pt\nimport seaborn as sns\n\n# only necessary on MacOS Sequoia\n# https://discourse.pymc.io/t/pytensor-fails-to-compile-model-after-upgrading-to-mac-os-15-4/16796/5\nimport pytensor\npytensor.config.cxx = '/usr/bin/clang++'\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\nsns.set_palette(\"tab10\")\n\ndef create_recapture_array(history):\n    \"\"\"Create the recapture array from a capture history.\"\"\"\n    _, occasion_count = history.shape\n    interval_count = occasion_count - 1\n\n    recapture_array = np.zeros((interval_count, interval_count), int)\n    for occasion in range(occasion_count - 1):\n\n        # which individuals, captured at t, were later recaptured?\n        captured_this_time = history[:, occasion] == 1\n        captured_later = (history[:, (occasion + 1):] &gt; 0).any(axis=1)\n        now_and_later = captured_this_time & captured_later\n\n        # when were they next recaptured?\n        remaining_history = history[now_and_later, (occasion + 1):]\n        next_capture_occasion = (remaining_history.argmax(axis=1)) + occasion\n\n        # how many of them were there?\n        ind, count = np.unique(next_capture_occasion, return_counts=True)\n        recapture_array[occasion, ind] = count\n\n    return recapture_array.astype(int)\n\ndef create_m_array(history):\n    '''Create the m-array from a capture history.'''\n\n    # leftmost column of the m-array\n    number_released = history.sum(axis=0)\n\n    # core of the m-array\n    recapture_array = create_recapture_array(history)\n    number_recaptured = recapture_array.sum(axis=1)\n\n    # no animals that were released on the last occasion are recaptured\n    number_recaptured = np.append(number_recaptured, 0)\n    never_recaptured = number_released - number_recaptured\n\n    # add a dummy row at the end to make everything stack\n    zeros = np.zeros(recapture_array.shape[1])\n    recapture_array = np.row_stack((recapture_array, zeros))\n\n    # stack the relevant values into the m-array\n    m_array = np.column_stack((number_released, recapture_array, never_recaptured))\n\n    return m_array.astype(int)\n\ndef fill_lower_diag_ones(x):\n    '''Fill the lower diagonal of a matrix with ones.'''\n    return pt.triu(x) + pt.tril(pt.ones_like(x), k=-1)\n\ndipper = np.loadtxt('dipper.csv', delimiter=',').astype(int)\ndipper[:5]\n\narray([[1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 0, 0, 0],\n       [1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 0, 0, 0, 0, 0]])\n\n\n\ndipper_m = create_m_array(dipper)\ndipper_m\n\n/var/folders/y8/cz021w550rbb072f7qhxyylh0000gq/T/ipykernel_21805/2348532828.py:65: DeprecationWarning: `row_stack` alias is deprecated. Use `np.vstack` directly.\n  recapture_array = np.row_stack((recapture_array, zeros))\n\n\narray([[22, 11,  2,  0,  0,  0,  0,  9],\n       [60,  0, 24,  1,  0,  0,  0, 35],\n       [78,  0,  0, 34,  2,  0,  0, 42],\n       [80,  0,  0,  0, 45,  1,  2, 32],\n       [88,  0,  0,  0,  0, 51,  0, 37],\n       [98,  0,  0,  0,  0,  0, 52, 46],\n       [93,  0,  0,  0,  0,  0,  0, 93]])\n\n\nThe JSSA model requires modeling the number of unmarked animals that were released during an occasion. We can calculate this using the \\(m\\)-array by subtracting the number of marked animals who were released from the total number of released animals.\n\nrecapture_array = create_recapture_array(dipper)\n\nnumber_released = dipper_m[:,0]\nnumber_marked_released = recapture_array.sum(axis=0)\n\n# shift number_released to get the years to align\nnumber_unmarked_released = number_released[1:] - number_marked_released\n\n# add the number released on the first occasion\nnumber_unmarked_released = np.concatenate(\n    ([number_released[0]], number_unmarked_released)\n)\n\nnumber_unmarked_released\n\narray([22, 49, 52, 45, 41, 46, 39])\n\n\nSimilar to the CJS model, this model requires a number of tricks to vectorize the operations. Many pertain to the distribution of the unmarked individuals. Similar to occupancy notebook, I use a custom distribution to model the entrants into the population. Austin Rochford refers to this as an incomplete multinomial distribution.\n\nn, occasion_count = dipper.shape\ninterval_count = occasion_count - 1\n\n# generate indices for the m_array\nintervals = np.arange(interval_count)\nrow_indices = np.reshape(intervals, (interval_count, 1))\ncol_indices = np.reshape(intervals, (1, interval_count))\n\n# matrix indicating the number of intervals between sampling occassions\nintervals_between = np.clip(col_indices - row_indices, 0, np.inf)\n\n# index for generating sequences like [[0], [0,1], [0,1,2]]\nalive_yet_unmarked_index = circulant(np.arange(occasion_count))\n\n\ndef logp(x, n, p):\n\n    x_last = n - x.sum()\n\n    # calculate thwe logp for the observations\n    res = factln(n) + pt.sum(x * pt.log(p) - factln(x)) \\\n            + x_last * pt.log(1 - p.sum()) - factln(x_last)\n\n    # ensure that the good conditions are met.\n    good_conditions = pt.all(x &gt;= 0) & pt.all(x &lt;= n) & (pt.sum(x) &lt;= n) & \\\n                        (n &gt;= 0)\n    res = pm.math.switch(good_conditions, res, -np.inf)\n\n    return res\n\ndef random(n, p, rng=None, size=None):\n    \"\"\"Minimal random function for observed-only CustomDist\"\"\"\n    if size is None:\n        return np.zeros(len(p), dtype=np.int64)\n    return np.zeros(size + (len(p),), dtype=np.int64)\n\n\n# m-array for the CJS portion of the likelihood\ncjs_marr = dipper_m[:-1,1:]\ncjs_marr\n\narray([[11,  2,  0,  0,  0,  0,  9],\n       [ 0, 24,  1,  0,  0,  0, 35],\n       [ 0,  0, 34,  2,  0,  0, 42],\n       [ 0,  0,  0, 45,  1,  2, 32],\n       [ 0,  0,  0,  0, 51,  0, 37],\n       [ 0,  0,  0,  0,  0, 52, 46]])\n\n\nAside from the unmarked portion of the model, the JSSA model is essentially identical to the CJS model above. In this version, I also model survival as time-varying, holding other parameters constant \\(p(\\cdot)\\phi(t)b_0(\\cdot)\\)\n\n# JSSA produces this warning. it's unclear why since it samples well\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Failed to infer_shape from Op AdvancedSubtensor\"\n)\n\nwith pm.Model() as jssa:\n\n    ## Priors\n\n    # catchability, survival, and pent\n    p = pm.Uniform('p', 0., 1.)\n    phi = pm.Uniform('phi', 0., 1., shape=interval_count)\n    b0 = pm.Uniform('b0', 0., 1.)\n    # beta = pm.Dirichlet('beta', np.ones(interval_count))\n\n    # # only estimate first beta, others constant\n    b_other = (1 - b0) / (interval_count)\n    beta = pt.concatenate(\n        ([b0], pt.repeat(b_other, interval_count))\n    )\n\n    # improper flat prior for N\n    total_captured = number_unmarked_released.sum()\n    N = pm.Uniform(\"N\", lower=total_captured, upper=1e4)\n\n    ## Entry\n\n    # add [1] to ensure the addition of the raw beta_0\n    p_alive_yet_unmarked = pt.concatenate(\n        ([1], pt.cumprod((1 - p) * phi))\n    )\n\n    # tril produces the [[0], [0,1], [0,1,2]] patterns for the recursion\n    psi = pt.tril(\n        beta * p_alive_yet_unmarked[alive_yet_unmarked_index]\n    ).sum(axis=1)\n\n    # distribution for the unmarked animals\n    unmarked = pm.CustomDist(\n        'Unmarked captures',\n        N,\n        psi * p,\n        logp=logp,\n        random=random,\n        observed=number_unmarked_released\n    )\n\n    ## CJS\n\n    # broadcast phi into a matrix\n    phi_mat = pt.ones_like(recapture_array) * phi\n    phi_mat = fill_lower_diag_ones(phi_mat) # fill irrelevant values\n\n    # probability of surviving between i and j in the m-array\n    p_alive = pt.cumprod(phi_mat, axis=1)\n    p_alive = pt.triu(p_alive) # select relevant (upper triangle) values\n\n    # p_not_cap: probability of not being captured between i and j\n    p_not_cap = pt.triu((1 - p) ** intervals_between)\n\n    # nu: probabilities associated with each cell in the m-array\n    nu = p_alive * p_not_cap * p\n\n    # probability for the animals that were never recaptured\n    chi = 1 - nu.sum(axis=1)\n\n    # combine the probabilities into a matrix\n    chi = pt.reshape(chi, (interval_count, 1))\n    marr_probs = pt.horizontal_stack(nu, chi)\n\n    # distribution of the m-array\n    marr = pm.Multinomial(\n        'M-array',\n        n=number_released[:-1], # last count irrelevant for CJS\n        p=marr_probs,\n        observed=cjs_marr\n    )\n\npm.model_to_graphviz(jssa)\n\n\n\n\n\n\n\nFigure 1: Visual representation of the JSSA model\n\n\n\n\n\n\nwith jssa:\n    jssa_idata = pm.sample()\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p, phi, b0, N]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\nphi_mle = [0.63, 0.46, 0.48, 0.62, 0.61, 0.58]\np_mle = [0.9]\nb0_mle = [0.079]\nN_mle = [310]\n\naz.plot_trace(\n    jssa_idata,\n    figsize=(10, 8),\n    lines=[(\"phi\", {}, phi_mle), (\"p\", {}, [p_mle]), (\"N\", {}, [N_mle]), (\"b0\", {}, [b0_mle])]\n);\n\n\n\n\n\n\n\nFigure 2: Traceplots for the dipper JSSA model. MLEs from the openCR package shown by vertical and horizontal lines.\n\n\n\n\n\nThe traceplots include the maximum likelihood estimates from the model, which I estimated usingthe openCR package in R. Again, there is high level of agreement between the two methods. I plot the survival estimates over time, and the posterior draws of \\(N\\), \\(p\\), and \\(b\\).\n\nfig, ax = plt.subplots(figsize=(6,4))\n\nt = np.arange(1981, 1987)\n\nphi_samps = az.extract(jssa_idata, var_names='phi').values.T\nphi_median = np.median(phi_samps, axis=0)\n\nax.plot(t, phi_median, linestyle='dotted', color='lightgray', linewidth=2)\nax.violinplot(phi_samps, t, showmedians=True, showextrema=False)\n\nax.set_ylim((0,1))\n\nax.set_ylabel(r'Apparent survival $\\phi$')\nax.set_title(r'Dipper JSSA')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Violin plots of the posterior for apparent surival over time from the cormorant CJS. Horizontal lines represent the posterior median.\n\n\n\n\n\n\npost = jssa_idata.posterior\n\n# stack the draws for each chain, creating a (n_draws, n_species) array\np_samps = post.p.to_numpy().flatten()\nN_samps = post.N.to_numpy().flatten()\nb_samps = post.b0.to_numpy().flatten()\n\n# create the plot\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n\n# add the scatter for each species\nalph = 0.2\nax0.scatter(p_samps, N_samps, s=5, alpha=alph)\n\nax0.spines.right.set_visible(False)\nax0.spines.top.set_visible(False)\n\nax0.set_ylabel(r'$N$')\nax0.set_xlabel(r'$p$')\n\nax1.scatter(b_samps, N_samps, s=5, alpha=alph)\n\nax1.set_xlabel(r'$b_0$')\n\nfig.suptitle('Dipper JSSA posterior draws')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 4: Posterior draws of \\(N,\\) \\(b_0,\\) and \\(p\\) from the dipper JSSA model.\n\n\n\n\n\n\n%load_ext watermark\n\n%watermark -n -u -v -iv -w\n\nLast updated: Wed Jan 14 2026\n\nPython implementation: CPython\nPython version       : 3.13.9\nIPython version      : 9.9.0\n\npymc      : 5.27.0\nnumpy     : 2.3.5\nseaborn   : 0.13.2\npytensor  : 2.36.3\narviz     : 0.23.0\nmatplotlib: 3.10.8\nscipy     : 1.17.0\n\nWatermark: 2.5.0\n\n\n\n\n\n\n\nReferences\n\nLebreton, Jean-Dominique, Kenneth P Burnham, Jean Clobert, and David R Anderson. 1992. “Modeling Survival and Testing Biological Hypotheses Using Marked Animals: A Unified Approach with Case Studies.” Ecological Monographs 62 (1): 67–118.\n\n\nMcCrea, Rachel S, and Byron JT Morgan. 2014. Analysis of Capture-Recapture Data. CRC Press.",
    "crumbs": [
      "Code",
      "PyMC",
      "Jolly-Seber"
    ]
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Code",
    "section": "",
    "text": "Please find here any jupyter notebooks that I may have thought would be of general interest. For now, these are all about Bayesian data analysis in Python. The majority of these are on how to fit ecological models in PyMC. There are also a handful of notebooks on NumPyro, another Python library,",
    "crumbs": [
      "Code"
    ]
  },
  {
    "objectID": "notebooks.html#pymc",
    "href": "notebooks.html#pymc",
    "title": "Code",
    "section": "PyMC",
    "text": "PyMC\nThere are many valuable tools for fitting hierarchical models in ecology, including unmarked, JAGS, NIMBLE and Stan. These are, for the most part, R libraries or programs called from R. There are relatively fewer examples of how to fit these models in Python. While most ecologists use R, they may find some benefit from using Python. For example, despite ecology being a lucrative industry, some of us might have to pivot to another field where Python may be more common. Also, Python is widely used for machine learning, which is increasingly applied in ecology.\nIn the PyMC jupyter notebooks, I try to demonstrate how to use PyMC to train the most common hierarchical models in ecology. For this, I have drawn considerable inspiration from Royle and Dorazio (2008), Kéry and Schaub (2011), McCrea and Morgan (2014), and Hooten and Hefley (2019), oftentimes simply porting their code, ideas, and analyses. In doing so, I hope to demonstrate PyMC’s core features, and highlight its strengths and weaknesses. The PyMC notebooks are somewhat sequential, with earlier notebooks explaining more basic features.",
    "crumbs": [
      "Code"
    ]
  },
  {
    "objectID": "notebooks.html#numpyro",
    "href": "notebooks.html#numpyro",
    "title": "Code",
    "section": "NumPyro",
    "text": "NumPyro\nNumPyro is another Python library for Bayesian data analysis. NumPyro runs on JAX, meaning that it utilizes your GPU for sampling. As such, NumPyro can be very fast for large datasets. PyMC conveniently allows users to sample models written in PyMC with other backends, including NumPyro (see this notebook for details). This is especially handy because NumPyro’s model syntax is different than PyMC, meaning that theoretically you only have to learn one syntax to reap the benefits!\nSo, why learn NumPyro at all? NumPyro has powerful tools for marginalizing discrete latent states in Hidden Markov Model (HMMs). HMMs form the theoretical background for many Bayesian population modeling frameworks, including Cormack-Jolly-Seber, Jolly-Seber, multistate, dynamic occupancy, and open-SCR. From what I can tell, PyMC lacks these features. As such, I think it is worth learning NumPyro if you are inerested in “open” models broadly. Moreover, NumPyro can fit closed models as well. Nevertheless, the NumPyro community seems less active than the PyMC community.\nAlso, I must plug the incredible biolith package in Python, which is written in NumPyro and contains many of the most common models in ecology.",
    "crumbs": [
      "Code"
    ]
  },
  {
    "objectID": "pymc-distance.html",
    "href": "pymc-distance.html",
    "title": "Distance sampling",
    "section": "",
    "text": "In this notebook, I explore how to fit distance sampling models for estimating the size of a closed population. Similar to the occupancy and closed capture-recapture notebooks, I use parameter-expanded data-augmentation (PX-DA) and the zero-inflated binomial model in this notebook.\nThe idea with distance sampling, also known as line-transect sampling, is that a surveyer traverses a transect, typically in a boat or a plane. As they survey, they note when they detect an individual, or a group, from the species of interest, and further note the distance from the transect to the animal. Further, they note the angle to the animal(s), such that they can calculate the perpendicular distance from the animal to the transect. We assume that probability of detecting an animal \\(p\\) decreases monotonically as the distance from the transect grows, e.g., \\(p=\\exp(-x^2/\\sigma^2),\\) where \\(x\\) is the distance and \\(\\sigma\\) is a scale parameter to be estimated. These simple assumptions permit the estimation of the population size \\(N\\) as well as density \\(D.\\)\nFollowing Hooten and Hefley (2019), Chapter 24 and Royle and Dorazio (2008), Chapter 7, I use the impala data from Burnham, Anderson, and Laake (1980), who credits P. Hemingway with the dataset. In this dataset, 73 impalas were observed along a 60km transect. The distance values below are the perpendicular distances, in meters, from the transect.\n\n%config InlineBackend.figure_format = 'retina'\nfrom scipy.stats import binom\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc as pm\nimport pytensor.tensor as pt\nimport seaborn as sns\n\n# plotting defaults\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\nsns.set_palette(\"tab10\")\n\n# hyper parameters\nM = 500\nU_X = 400\nU_SIGMA = 400\n\n# burnham impala dataset with distances in m\nx_observed = np.array(\n    [71.933980, 26.047227, 58.474341, 92.349221, 163.830409, 84.523652\n    ,163.830409, 157.330098, 22.267696, 72.105330, 86.986979, 50.795047\n    ,0.000000, 73.135370,  0.000000, 128.557522, 163.830409,  71.845104\n    ,30.467336, 71.073909, 150.960702, 68.829172, 90.000000, 64.983827\n    ,165.690874, 38.008322, 378.207430, 78.146226, 42.127052, 0.000000\n    ,400.000000, 175.386612, 30.467336, 35.069692, 86.036465, 31.686029\n    ,200.000000, 271.892336, 26.047227, 76.604444, 41.042417, 200.000000\n    ,86.036465, 0.000000, 93.969262, 55.127471, 10.458689, 84.523652\n    ,0.000000, 77.645714, 0.000000, 96.418141, 0.000000, 64.278761\n    ,187.938524, 0.000000, 160.696902, 150.453756, 63.603607, 193.185165\n    ,106.066017, 114.906666, 143.394109, 128.557522, 245.745613, 123.127252\n    ,123.127252, 153.208889, 143.394109, 34.202014, 96.418141, 259.807621\n    ,8.715574]\n)\n\n# plot the distances\nfig, ax = plt.subplots(figsize=(4,4))\n\nax.hist(x_observed, edgecolor='white')\n\nax.set_title('Hemingway Impala Data')\nax.set_ylabel('Number of detections')\nax.set_xlabel('Distance (m)')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Histogram of the number of detected impalas at varying distances.\n\n\n\n\n\nAgain, we treat this as a zero-inflated binomial model using PX-DA. The trick for doing so is to create a binary vector of length \\(M\\), \\(y,\\) that represents whether the individual was detected during the study. Then, combine the indicator with the distance vector \\(x\\) to create a the full dataset \\((x,y).\\)\n\nn = len(x_observed)\nunobserved_count = M - n\nzeros = np.zeros(unobserved_count)\n\ny = np.ones(n)\ny_augmented = np.concatenate((y, zeros))\n\nThe issue is that \\(x\\) is unobserved for the undetected individuals. To work around this, we put a uniform prior on the unobserved \\(x,\\) i.e., \\(x \\sim \\text{Uniform}(0, U_x).\\) With this “complete” \\(x,\\) we can construct the detection function \\(p\\) for the unobserved individuals.\n\nwith pm.Model() as distance:\n\n    psi = pm.Beta('psi', 0.001, 1)\n    sigma = pm.Uniform('sigma', 0, U_SIGMA)\n\n    x_unobserved = pm.Uniform('x_unobserved', 0, U_X, shape=unobserved_count)\n    x_complete = pt.concatenate((x_observed, x_unobserved))\n\n    p = pm.Deterministic('p', pm.math.exp(- x_complete ** 2 / sigma ** 2))\n\n    pm.ZeroInflatedBinomial('y', p=p, psi=psi, n=1, observed=y_augmented)\n\npm.model_to_graphviz(distance)\n\n\n\n\n\n\n\nFigure 2: Visual representation of the distance sampling model.\n\n\n\n\n\n\nwith distance:\n    distance_idata = pm.sample()\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [psi, sigma, x_unobserved]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\n\n\n\naz.plot_trace(\n    distance_idata,\n    figsize=(8,4),\n    var_names=['psi', 'sigma']\n)\nplt.subplots_adjust(hspace=0.4)\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Traceplots for the distance sampling model.\n\n\n\n\n\nThis model samples slower than the models in the other notebooks, presumably because of the unobserved \\(x.\\) As in the closed capture-recapture notebook, we will have to simulate the posterior for \\(N\\) using the posterior distribution of \\(\\psi\\) and \\(M.\\)\n\nRNG = np.random.default_rng()\n\n# extract the posterior samples into numpy arrays\nposterior = az.extract(distance_idata)\npsi_samples = posterior.psi.to_numpy()\n\n# only interested in the unobserved animals\np_samples = posterior.p.to_numpy()[n:]\n\n# posterior probabilities of being present in the population but not detected\np_if_present = psi_samples * binom.pmf(0, n=1, p=p_samples)\np_total = p_if_present + (1 - psi_samples)\n\n# simulate the latent inclusion state\nZ = RNG.binomial(1, p_if_present / p_total)\nnumber_undetected = Z.sum(axis=0)\n\n# # # compute the total number of animals in the population\nN_samples = n + number_undetected\n\n\n# plot the posterior distributions\nsigma_samples = posterior.sigma.to_numpy()\nfig, (ax0, ax1) = plt.subplots(1, 2, sharey=True, figsize=(8,4))\n\n# histograms of the posteriors\nax0.hist(N_samples, edgecolor='white', bins=30)\nax1.hist(sigma_samples, edgecolor='white', bins=30)\n\n# axes labels\nax0.set_xlabel(r'Abundance $N$')\nax0.set_ylabel('Number of samples')\nax1.set_xlabel(r'Detection range $\\sigma$')\n\n# add the point estimates\nN_hat = N_samples.mean()\nsigma_hat = sigma_samples.mean()\nax0.text(200, 350, rf'$\\hat{{N}}$={N_hat:.1f}', ha='left', va='center')\nax1.text(205, 350, rf'$\\hat{{\\sigma}}$={sigma_hat:.1f}', ha='left', va='center')\n\n# the results from royle and dorazio (2008) for comparison\nN_hat_royle = 179.9\nsigma_hat_royle = 187\n\nax0.axvline(N_hat_royle, linestyle='--', linewidth=3, color='C1')\nax1.axvline(sigma_hat_royle, linestyle='--', linewidth=3, color='C1')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 4: Posterior distributions for \\(N\\) and \\(\\sigma.\\) Estimates from Royle and Dorazio (2008) are shown with vertical lines.\n\n\n\n\n\nThe model shows a high level of agreement with Royle and Dorazio (2008), Chapter 7, although note that they reported \\(\\sigma\\) in terms of 100m units. It is also possible to plot the posterior distribution of the detection function.\n\nxx = np.arange(400)\n\ndef det_func(x, s):\n    return np.exp(- (x ** 2) / (s ** 2))\n\np_samps = np.array([det_func(xx, s) for s in sigma_samples])\n\np_mean = p_samps.mean(axis=0)\np_low = np.quantile(p_samps, 0.02, axis=0)\np_high = np.quantile(p_samps, 0.98, axis=0)\n\nfig, ax = plt.subplots(figsize=(5,4))\n\nax.plot(xx, p_mean, '-')\nax.fill_between(xx, p_low, p_high, alpha=0.2)\n\nax.set_title('Detection function')\nax.set_ylabel(r'$p$')\nax.set_xlabel(r'Distance (m)')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 5: Posterior distribution for the detection function. The line represents the posterior mean while the shaded area is the 96% interval.\n\n\n\n\n\n\n%load_ext watermark\n\n%watermark -n -u -v -iv -w\n\nLast updated: Wed Jan 14 2026\n\nPython implementation: CPython\nPython version       : 3.13.9\nIPython version      : 9.9.0\n\npytensor  : 2.36.3\nnumpy     : 2.3.5\narviz     : 0.23.0\nmatplotlib: 3.10.8\npymc      : 5.27.0\nseaborn   : 0.13.2\n\nWatermark: 2.5.0\n\n\n\n\n\n\n\nReferences\n\nBurnham, Kenneth P, David R Anderson, and Jeffrey L Laake. 1980. “Estimation of Density from Line Transect Sampling of Biological Populations.” Wildlife Monographs, no. 72: 3–202.\n\n\nHooten, Mevin B, and Trevor Hefley. 2019. Bringing Bayesian Models to Life. CRC Press.\n\n\nRoyle, J Andrew, and Robert M Dorazio. 2008. Hierarchical Modeling and Inference in Ecology: The Analysis of Data from Populations, Metapopulations and Communities. Elsevier.",
    "crumbs": [
      "Code",
      "PyMC",
      "Distance sampling"
    ]
  },
  {
    "objectID": "numpyro-intro.html",
    "href": "numpyro-intro.html",
    "title": "Introduction to NumPyro",
    "section": "",
    "text": "Before I forced myself to learn it, I found the NumPyro syntax utterly beguiling. In this notebook, I hope to introduce the library in a way that I would have found helpful. Hopefully this will be helpful for you too.\nAs a motivating example, I will use the same occupancy dataset from the PyMC occupancy notebook. These data come from Henden et al. (2013), and were used as a demonstration in Hooten and Hefley (2019), Chapter 23. They represent detection/non-detection data of Willow Warblers from Finnmark, Norway. The \\(J=27\\) sites were sampled \\(K=3\\) times. Replicating the analysis in Box 23.7 in Hooten and Hefley (2019), I use two covariates for site: site area (Pland) and willow tree height (wheight). Further, I use two covariates for visit: an indicator for the visit and willow tree height.\n\nfrom jax import random\nfrom jax.scipy.special import expit\nfrom numpyro.infer import NUTS, MCMC\nimport arviz as az\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport numpyro.distributions as dist\nimport pandas as pd\nimport seaborn as sns\n\n# plotting defaults\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\nsns.set_palette(\"tab10\")\n\n# hyperparameters\nRANDOM_SEED = 17\n\ndef scale(x):\n    '''Scale x: 0 is the mean and 1 is one standard deviation from the mean.'''\n    return (x - np.nanmean(x)) / np.nanstd(x)\n\ndef get_warbler_data():\n\n    # read in the data\n    data = pd.read_csv('PlosOne-DataFinnmark.csv')\n\n    # subset the data to select willow warbler\n    is_warbler = data.Species == \"Willow Warbler\"\n    Y = data.loc[is_warbler, ['Y05.1', 'Y05.2', 'Y05.3']].to_numpy()\n    n, J = Y.shape\n\n    # generate site covariate matrix\n    site_intercept = np.ones(n)\n    pland = scale(data.loc[is_warbler, 'Pland']).to_numpy()\n    wheight = scale(data.loc[is_warbler, 'wheight']).to_numpy()\n\n    X = np.c_[site_intercept, pland, wheight]\n\n    # generate visit covariate array\n    visit_int = np.ones_like(Y)\n    visit_wheight = np.repeat(wheight, repeats=J).reshape(n, J)\n\n    # indicates which visit this is [0, 1, 2, 0, ...]\n    _, visit_indicator = np.indices(Y.shape)\n    visit_indicator = scale(visit_indicator)\n\n    W = np.stack((visit_int, visit_indicator, visit_wheight), axis=2)\n\n    return {\n        'detection_data': Y,\n        'site_covariates': X,\n        'visit_covariates': W\n    }\n\nOur observations are a matrix of of binary detection data, \\(\\mathbf{Y}\\), where the \\(y_{i,j}=1\\) if a warbler was detected at site \\(i\\) during survey \\(j\\), and \\(\\mathbf{Y}\\) has shape \\((J, K)\\). We encode our site level covariates as a matrix \\(\\mathbf{X},\\) with shape \\((J, 3),\\) where each row contains a 1 for the intercept, and the values for pland and wheight at that site. We model the effects of these covariates on the probability that each site is occupied with \\(z_{i} \\sim \\text{Bern}(\\psi)\\) where \\(\\psi = \\text{expit}(\\mathbf{X}\\beta)\\). We assume that warblers can only be detected at occupied sites with probability \\(p_{ij}\\). As such, we model each observation as \\(y_{i,j} \\sim \\text{Bern}(z_i p_{i,j})\\). We encode visit-level covariates was an array \\(\\mathbf{W}\\) with shape \\((J, K, 3)\\), where the last dimension contains a 1 for the intercept, an integer value indicating which visit this is, and the wheight value at that site. We model the effect of these covariates on the probability of detection with \\(\\mathbf{P}=\\text{expit}(\\mathbf{W}\\alpha)\\).\n\nDefining the model\nWe define our NumPyro model as a Python function. Let’s look at one of these functions: an occupancy model with site and visit covariates.\n\ndef occupancy_model(data_dictionary):\n\n    detection_data = data_dictionary['detection_data']\n    site_count, visit_count = detection_data.shape\n\n    # site-level linear model\n    # three beta effects in linear model, each getting its own Normal(0, 2) prior\n    with numpyro.plate('site-effects', 3):\n        beta = numpyro.sample('Beta', dist.Normal(0, 2))\n\n    # we will always use jax.numpy (jnp) for numpy like operations\n    site_covs = data_dictionary['site_covariates']\n    logit_psi = jnp.dot(site_covs, beta)\n\n    # jax.scipy contains scipy like operations, like jax.scipy.special.expit()\n    psi = numpyro.deterministic('psi', expit(logit_psi))\n\n    # visit level linear model\n    # three alpha effects in linear model, each getting its own Normal(0, 2) prior\n    with numpyro.plate('visit-effects', 3):\n        alpha = numpyro.sample('Alpha', dist.Normal(0, 2))\n\n    # linear model for the detection probability\n    visit_covs = data_dictionary['visit_covariates']\n    logit_p = jnp.dot(visit_covs, alpha)\n    p = numpyro.deterministic('p', expit(logit_p))\n\n    # here we let numpyro know that there will be site_count binary z states\n    with numpyro.plate('sites', site_count):\n\n        # the vector of z states will be bernoulli distributed with parameter psi\n        z = numpyro.sample(\n            'z',\n            dist.Bernoulli(psi),\n            infer={'enumerate': 'parallel'} # this is where we marginalize!\n        )\n\n        # NumPyro is finnicky with shapes, especially with enumeration\n        # this helps ensure that the site dimensions are facing each other\n        mu_y = z * p.T\n\n        # encode the likelihood by adding obs (transposed to match above)\n        with numpyro.plate('visits', visit_count):\n            numpyro.sample(\"y\", dist.Bernoulli(mu_y), obs=detection_data.T)\n\nThere are two NumPyro functions that are called repeatedly in the occupancy_model function: numpyro.sample and numpyro.plate. The numpyro.plate function can be thought of as a vectorized loop over conditionally independent dimensions. For example, each one of our site-effects, that is, the \\(\\beta\\) values in the linear model for \\(\\psi\\), is independent conditional on the hyperparameters \\(\\mu=0\\) and \\(\\sigma=2\\). In JAGS, we might write this code as:\nfor (i in 1:3){\n    beta[i] ~ dnorm(0, 0.5)\n}\nIn NumPyro, we can write this as,\n\nwith numpyro.plate('site-effects', 3):\n    beta = numpyro.sample('beta', dist.Normal(0, 2))\n\nwhich creates a vector of beta effects for our linear model.\nThe numpyro.sample sample function is how we define random variables. The first argument is the name, i.e., how we’ll access the variable later. The second argument is the actual distribution we’re assigning to the random variable. These distribution functions reside in NumPyro’s distributions module. This syntax should be familiar with PyMC users. In PyMC, the above statement would be beta = pm.Normal('beta', 0, 2, dims='site-effects').\nThe obs argument in numpyro.sample defines whether each random variable is observed. In the occupancy model, the unobserved random variables include the priors for the coefficients in the linear model, \\(\\alpha\\) and \\(\\beta\\), as well as the discrete latent variable \\(\\mathbf{z}\\). Recall that \\(\\mathbf{z}\\) is a vector of length site_count because it within the plate for sites. The one observed random variable is \\(\\mathbf{Y}\\). As such, the distribution for \\(\\mathbf{Y}\\) encodes the likelihood. Since \\(\\mathbf{Y}\\) is two dimensional, we include another plate for the visits.\nYou might notice that the function occupancy_model() lacks a return statement, which can look a bit odd. That’s because NumPyro registers random variables in the function to a handler, such as an MCMC sampler. This gives the language and syntax flexibility to exchange handlers and models.\n\n\nAutomated marginalization\nThe distribution for the latent occupancy state, \\(\\mathbf{z},\\) looks slightly different than the others because it includes the additional argument, infer={'enumerate': 'parallel'}. This tells NumPyro to marginalize this discrete latent variable. To do so, NumPyro enumerates the possible outcomes for this variable. At sites where the species was never detected, the possible outcomes are that the species is not truly present, or that it is truly present and just never detected. As such, the model enumerates these outcomes as:\n\\[\nP(\\mathbf{y}_j)=\n\\begin{cases}\n    P(\\mathbf{y}_j | z_j = 1)\\; \\psi_j \\; + \\; (1 - \\psi_j),   & \\text{if } \\mathbf{y}_j = \\mathbf{0}\\\\\n    P(\\mathbf{y}_j | z_j = 1)\\; \\psi_j,  & \\text{otherwise}\n\\end{cases}\n\\]\n\n\nInference\nInference is fairly straightforward in NumPyro, although more verbose than PyMC. First, we define which sampler we would like to use on our model. We input this kernel into the MCMC initializer, specifying the configuration (chains, draws, tune, etc.). Finally, we run the MCMC algorithm.\n\nwarbler_data = get_warbler_data()\nrng_key = random.PRNGKey(RANDOM_SEED)\n\n# specify which sampler you want to use\nnuts_kernel = NUTS(occupancy_model)\n\n# configure the MCMC run\nmcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000, num_chains=4)\n\n# run the MCMC then inspect the output\nmcmc.run(rng_key, warbler_data)\nmcmc.print_summary()\n\n/var/folders/y8/cz021w550rbb072f7qhxyylh0000gq/T/ipykernel_21622/1360101409.py:8: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000, num_chains=4)\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1500 [00:00&lt;10:35,  2.36it/s, 3 steps of size 2.34e+00. acc. prob=0.00]sample:  35%|███▌      | 529/1500 [00:00&lt;00:00, 1333.18it/s, 7 steps of size 5.44e-01. acc. prob=0.88]sample:  73%|███████▎  | 1094/1500 [00:00&lt;00:00, 2476.12it/s, 7 steps of size 5.44e-01. acc. prob=0.90]sample: 100%|██████████| 1500/1500 [00:00&lt;00:00, 2156.99it/s, 7 steps of size 5.44e-01. acc. prob=0.90]\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1500 [00:00&lt;09:38,  2.59it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  33%|███▎      | 494/1500 [00:00&lt;00:00, 1333.01it/s, 7 steps of size 7.65e-01. acc. prob=0.79]sample:  70%|██████▉   | 1047/1500 [00:00&lt;00:00, 2500.70it/s, 3 steps of size 5.55e-01. acc. prob=0.88]sample: 100%|██████████| 1500/1500 [00:00&lt;00:00, 2245.52it/s, 7 steps of size 5.55e-01. acc. prob=0.88]\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1500 [00:00&lt;09:51,  2.54it/s, 1 steps of size 2.34e+00. acc. prob=0.00]sample:  35%|███▍      | 521/1500 [00:00&lt;00:00, 1383.82it/s, 7 steps of size 5.19e-01. acc. prob=0.82]sample:  73%|███████▎  | 1101/1500 [00:00&lt;00:00, 2598.06it/s, 15 steps of size 5.19e-01. acc. prob=0.88]sample: 100%|██████████| 1500/1500 [00:00&lt;00:00, 2261.97it/s, 7 steps of size 5.19e-01. acc. prob=0.88] \n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1500 [00:00&lt;10:29,  2.38it/s, 3 steps of size 2.34e+00. acc. prob=0.00]sample:  35%|███▌      | 530/1500 [00:00&lt;00:00, 1344.27it/s, 7 steps of size 4.39e-01. acc. prob=0.93]sample:  71%|███████   | 1061/1500 [00:00&lt;00:00, 2398.99it/s, 15 steps of size 4.39e-01. acc. prob=0.93]sample: 100%|██████████| 1500/1500 [00:00&lt;00:00, 2134.27it/s, 7 steps of size 4.39e-01. acc. prob=0.93] \n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n  Alpha[0]     -0.02      0.37     -0.03     -0.60      0.61   2498.38      1.00\n  Alpha[1]      0.46      0.28      0.45      0.03      0.95   3838.74      1.00\n  Alpha[2]      0.75      0.44      0.75      0.05      1.48   2608.75      1.00\n   Beta[0]      0.84      0.84      0.72     -0.44      2.19   1766.95      1.00\n   Beta[1]      2.28      0.89      2.16      0.89      3.70   1868.30      1.00\n   Beta[2]      0.95      0.69      0.96     -0.17      2.08   2793.53      1.00\n\nNumber of divergences: 0\n\n\nJust like in PyMC, we will rely on the arviz package for visualization of the MCMC samples. The only difference is that we will need to initialize the InferenceData object with az.from_dict().\n\nsamples = mcmc.get_samples(group_by_chain=True)\nwarbler_idata = az.from_dict(samples)\n\nalpha_hat_hooten =  [0.04, 0.47, 0.68]\nbeta_hat_hooten = [0.56, 1.92, 0.93]\n\naz.plot_trace(\n    warbler_idata,\n    figsize=(8,4),\n    var_names=['Alpha', 'Beta'],\n    lines=[(\"Alpha\", {}, [alpha_hat_hooten]),\n           (\"Beta\", {}, [beta_hat_hooten])]\n);\n\n\n\n\n\n\n\nFigure 1: Traceplots from the willow warbler occupancy model. Estimates from Hooten and Hefley (2019) are shown by vertical and horizontal lines.\n\n\n\n\n\n\naz.plot_forest(warbler_idata, var_names=['Alpha', \"Beta\"],\n               figsize=(6,4),\n               hdi_prob=0.95, ess=True, combined=True);\n\n\n\n\n\n\n\nFigure 2: Forest plots from willow warbler occupancy model. ESS refers to the effective sample size.\n\n\n\n\n\nAs we can see, we get virtually the same estimates as the PyMC occupancy notebook.\n\n\nPrediction\nWe can recover the latent states in the model by computing the probability that warblers occupy the site given that one was never detected, \\[\n\\Pr(z_i = 1 | \\mathbf{y}_j=\\mathbf{0}) =\n\\frac{\n    \\prod_{j=1}^{K} (1 - p_{ij}) * \\psi_i\n}{\n    \\prod_{j=1}^{K} (1 - p_{ij}) * \\psi_i + (1 - \\psi_i)\n}\n\\]\nfor all \\(i\\) where warblers were never detected.\n\ndef sim_N(data, samples):\n    '''Simulate the number of occupied sites'''\n\n    # later we'll filter to just the sites without a detection\n    was_detected = data['detection_data'].sum(axis=1) == 0\n    n = was_detected.sum()\n    _, visit_count = data['detection_data'].shape\n\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # extract the relevant samples\n    psi_undet = samples['psi'][:, ~was_detected]\n    p_undet = samples['p'][:, ~was_detected, :]\n\n    # computing Pr(z_i = 0 | y_{j,.} = 0)\n    # that is, probability that the species was never detected given occupancy\n    bin_prod = (1 - p_undet).prod(axis=2)\n    pi = (bin_prod * psi_undet) / (bin_prod * psi_undet  + (1 - psi_undet))\n\n    # sample the number of sites without a detection\n    number_undetected = rng.binomial(1, pi).sum(axis=1)\n\n    # add in the number of sites with a detection\n    N_samps = n + number_undetected\n\n    return N_samps\n\nsamples = mcmc.get_samples()\nN_samples = sim_N(warbler_data, samples)\n\n# bar plot looks a little better than a histogram here imo\nfig, ax = plt.subplots(figsize=(4,3))\nN_values, N_counts = np.unique(N_samples, return_counts=True)\nax.bar(N_values, N_counts)\nax.set_ylabel('Number of samples')\nax.set_title('Posterior of $N$ occupied sites')\n\n# this ensures the x axis labels are only integers\nfrom matplotlib.ticker import MaxNLocator\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Posterior distributions of the number of occupied sites.\n\n\n\n\n\n\n\nConclusion\nIn this notebook, we covered the basic NumPyro syntax using a simple occupancy model as an example. We learned that numpyro.plate can generate a vector (or an array) of random varaibles, making it easy to vectorize operations within our model. We learned that we generate the random variables themselves with numpyro.sample, which requires us to specify a distribution from the numpyro.distributions module. Finally, we learned that we can marginalize out discrete latent states with the infer={'enumerate': 'parallel'} argument to numpyro.sample. In the following notebooks, we will introduce how we can marginalize out discrete latent states in Hidden Markov Models.\n\n\n\n\n\nReferences\n\nHenden, John-André, Nigel G Yoccoz, Rolf A Ims, and Knut Langeland. 2013. “How Spatial Variation in Areal Extent and Configuration of Labile Vegetation States Affect the Riparian Bird Community in Arctic Tundra.” PLoS One 8 (5): e63312.\n\n\nHooten, Mevin B, and Trevor Hefley. 2019. Bringing Bayesian Models to Life. CRC Press.",
    "crumbs": [
      "Code",
      "NumPyro",
      "Introduction"
    ]
  },
  {
    "objectID": "numpyro-scr-temp.html",
    "href": "numpyro-scr-temp.html",
    "title": "Open spatial capture-recapture",
    "section": "",
    "text": "In this notebook, I introduce how to estimate survival and abundance with open spatial capture-recapture (SCR) models. Open SCR models are a combination of closed population SCR models and Jolly-Seber models. The basic open SCR model assumes that animals enter and leave the population according to a Jolly-Seber process, each animal has an activity center, and the probability of encountering the animal declines with distance from its activity center. In this example, we will assume that each animal’s activity center is constant across sampling occasions, and that activity centers are uniformly distributed across the state space.\nIn this example, I will use simulated data to ensure that we’re getting reasonable results. That said, the ovenbird dataset from the PyMC SCR notebook would be a great example here!\n\n%config InlineBackend.figure_format = 'retina'\n\nimport os\n\n# # this suppresses a warning about the cuda backend\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# settings for cpu\nos.environ['XLA_FLAGS'] = '--xla_cpu_multi_thread_eigen=false intra_op_parallelism_threads=4'\nos.environ['JAX_ENABLE_X64'] = 'False'\nimport numpyro\nnumpyro.set_host_device_count(4)\n\nfrom itertools import product\n\nfrom jax import random, debug\nfrom jax.scipy.special import logit\nfrom numpyro.contrib.control_flow import scan\nfrom numpyro.infer import NUTS, MCMC, Predictive\nimport arviz as az\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport numpyro.distributions as dist\nimport pandas as pd\nimport seaborn as sns\n\n# plotting defaults\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\nsns.set_palette(\"tab10\")\n\n# mcmc hyperparameters\nWARMUP_COUNT = 500\nSAMPLE_COUNT = 1000\nCHAIN_COUNT = 4\nRANDOM_SEED = 1792\n\n# simulation hyperparameters\nSESSION_COUNT = 7\nSUPERPOPULATION_SIZE = 400\nAPPARENT_SURVIVAL = 0.7\nINITIAL_PI = 0.34\nBASELINE_RATE = 0.25\nSIGMA = 0.5\nM = 1000\nGRID_WIDTH = 10\nOCCASION_COUNT = 20\nBUFFER = 2\nU_SIGMA = 5\n\n# make the labels on arviz plots nicer\nlabeller = az.labels.MapLabeller(\n    var_name_map={\"psi\": r\"$\\psi$\", 'gamma':  r\"$\\gamma$\", 'alpha': r'$\\alpha$',\n                  'epsilon': r\"$\\epsilon$\", 'p':  r\"$p$\" , 'beta': r'$\\beta$',\n                  'phi': r'$\\phi$', 'alpha_t': r'$\\alpha_t$',}\n)\n\ndef sim_js():\n    \"\"\"Simulate capture histories with a spatial Jolly-Seber model.\n\n    Simulation code adapted from Kery and Schaub (2012), Chapter 10 and Gardner\n    et al (2017) via Olivier Gimenez:\n    https://oliviergimenez.github.io/basics_spatial_capturerecapture/\n    \"\"\"\n\n    rng = np.random.default_rng(RANDOM_SEED)\n    interval_count = SESSION_COUNT - 1\n\n    # simulate entry into the population\n    pi_rest = (1 - INITIAL_PI) / interval_count\n    pi = np.concatenate([[INITIAL_PI], np.full(interval_count, pi_rest)])\n\n    # which occasion did the animal enter in?\n    entry_matrix = rng.multinomial(n=1, pvals=pi, size=SUPERPOPULATION_SIZE)\n    entry_occasion = entry_matrix.nonzero()[1]\n    _, entrant_count = np.unique(entry_occasion, return_counts=True)\n\n    # zero if the animal has not yet entered and one after it enters\n    entry_trajectory = np.maximum.accumulate(entry_matrix, axis=1)\n\n    # flip coins for survival between occasions\n    survival_draws = rng.binomial(\n        1, APPARENT_SURVIVAL, (SUPERPOPULATION_SIZE, interval_count)\n    )\n\n    # add column such that survival between t and t+1 implies alive at t+1\n    survival_draws = np.column_stack([np.ones(SUPERPOPULATION_SIZE), survival_draws])\n\n    # ensure that the animal survives until it enters\n    is_yet_to_enter = np.arange(SESSION_COUNT) &lt;= entry_occasion[:, None]\n    survival_draws[is_yet_to_enter] = 1\n\n    # once the survival_draws flips to zero the remaining row stays 0\n    survival_trajectory = np.cumprod(survival_draws, axis=1)\n\n    # animal has entered AND is still alive\n    state = entry_trajectory * survival_trajectory\n    state = state.astype(np.int8)\n\n    # create the trapping grid\n    grid_height = GRID_WIDTH\n    grid = product(range(GRID_WIDTH), range(grid_height))\n    grid = np.array(list(grid))\n    trap_count = len(grid)\n\n    # define the state space\n    minima = grid.min(axis=0) - BUFFER\n    maxima = grid.max(axis=0) + BUFFER\n\n    # simulate activity centers for every individual in the superpopulation\n    center_coords = rng.uniform(minima, maxima, size=(SUPERPOPULATION_SIZE, 2))\n\n    # true distance between the trap and the activity centers\n    distance = jnp.linalg.norm(\n        center_coords[:, None, :] - grid[None, :, :],\n        axis=2\n    )\n\n    # simulate potential captures\n    p = BASELINE_RATE * np.exp(-(distance**2) / (2 * SIGMA**2))\n    capture = rng.binomial(OCCASION_COUNT, p[..., None],\n                           size=(SUPERPOPULATION_SIZE, trap_count, SESSION_COUNT))\n\n    # calculate actual captures, shape: (super_size, trap_count, session_count)\n    capture_history = capture * state[:, np.newaxis, :]\n\n    # remove the non-detected individuals\n    was_captured = capture_history.sum(axis=(1, 2)) &gt; 0\n    capture_history = capture_history[was_captured]\n\n    # augment the history with nz animals\n    n = was_captured.sum()\n    nz = M - n\n    all_zero_history = np.zeros((nz, trap_count, SESSION_COUNT), dtype=np.int8)\n    capture_history = np.vstack([capture_history, all_zero_history]).astype(np.int8)\n\n    # return a dict with relevant summary stats\n    N_t = state.sum(axis=0)\n\n    # deaths at time t\n    D_t = (state[:, :-1] * (1 - state[:, 1:])).sum(axis=0)\n\n    # Print simulation summary\n    print(f\"Simulation complete: {n}/{SUPERPOPULATION_SIZE} individuals detected\")\n    print(f\"Population trajectory: N = {N_t}\")\n    print(f\"Recruits per session: B = {entrant_count}\")\n    print(f\"Deaths per session: D = {D_t}\")\n\n    capture_history = np.moveaxis(capture_history, 2, 0)\n    return {\n        'capture_history': capture_history,\n        'N_t': N_t,\n        'B': entrant_count,\n        'trapxy': grid,\n        'trap_count': trap_count,\n        'center_coords': center_coords,\n        'z': state,\n        'minima': minima,\n        'maxima': maxima,\n        'super_size': M,\n        'occasion_count': OCCASION_COUNT,\n        'session_count': SESSION_COUNT\n    }\n\nLike the simulation code, the NumPyro model is essentially the same as the NumPyro Jolly-Seber model. The difference is that the detection process is more complex, in that we need to simulate activity centers, compute the distance pairwise distance matrix from the activity centers and the traps, then compute the detection probability for each individual at each trap. Again, we will marginalize out the latent existential state, \\(z\\), and scan over the sampling occasions.\n\ndef spatial_js(data):\n\n    # unpack everything in the data dictionary\n    history = data['capture_history']\n    trapxy = jnp.array(data['trapxy'])\n    trap_count = data['trap_count']\n    xmin, ymin = jnp.array(data['minima'])\n    xmax, ymax = jnp.array(data['maxima'])\n    super_size = data['super_size']\n    occasion_count = data['occasion_count']\n    session_count = data['session_count']\n\n    ### Priors ###\n\n    # specify gamma prior as a function of pi and psi\n    psi = numpyro.sample('psi', dist.Beta(0.001, 1))\n    pi = numpyro.sample('pi', dist.Dirichlet(jnp.ones(session_count)))\n\n    # compute the removal probabilities as a function of psi and pi\n    gamma = jnp.zeros(session_count)\n\n    # the `vector.at[0].set(1)` notation is jax for `vector[0] = 1`\n    gamma = gamma.at[0].set(psi * pi[0])\n    for t in range(1, session_count):\n        denominator = jnp.prod(1 - gamma[:t])\n        gamma = gamma.at[t].set(psi * pi[t] / denominator)\n    gamma = numpyro.deterministic('gamma', gamma)\n\n    # apparent survival\n    phi = numpyro.sample('phi', dist.Beta(2, 2))\n\n    # baseline encounter rate\n    g0 = numpyro.sample('alpha0', dist.Beta(4, 1))\n\n    # detection range\n    sigma = numpyro.sample('sigma', dist.Uniform(0, U_SIGMA))\n\n    # sample activity center locations and compute capture probability at traps\n    with numpyro.plate(\"activity_centers\", super_size):\n        sx = numpyro.sample('sx', dist.Uniform(xmin, xmax))\n        sy = numpyro.sample('sy', dist.Uniform(ymin, ymax))\n        center_coords = jnp.stack([sx, sy], axis=1)\n\n        # pairwise distance between activity centers and the traps\n        x_norm = jnp.sum(center_coords**2, axis=1, keepdims=True)\n        y_norm = jnp.sum(trapxy**2, axis=1, keepdims=True)\n        distance2 = x_norm + y_norm.T - 2 * center_coords @ trapxy.T\n\n        # linear model for scr detection model\n        p = g0 * jnp.exp( - distance2 / (2 * sigma ** 2))\n\n\n    #### Transition existential states and compute scr likelihood ####\n\n\n    def transition_and_capture(carry, y_current):\n\n        # this is what the previous iteration of transition_and_capture returned\n        z_previous, t = carry\n\n        # transition probability matrix\n        trans_probs = jnp.array([\n            [1 - gamma[t], gamma[t],     0.0],  # From not yet entered\n            [         0.0,      phi, 1 - phi],  # From alive\n            [         0.0,      0.0,     1.0]   # From dead\n        ])\n\n        with numpyro.plate(\"animals\", super_size, dim=-1):\n\n            # probability of transitioning depends on the previous state\n            mu_z_current = trans_probs[z_previous]\n            z_current = numpyro.sample(\n                \"state\",\n                dist.Categorical(dist.util.clamp_probs(mu_z_current)),\n                infer={\"enumerate\": \"parallel\"}\n            )\n\n            # probability of encountering the individual based on the state\n            mu_y_current = jnp.where(z_current == 1, 1, 0) * p.T\n            mu_y_current = dist.util.clamp_probs(mu_y_current)\n\n            # likelihood of the scr data\n            with numpyro.plate('traps', trap_count, dim=-2):\n                numpyro.sample(\n                    \"photo_id_data\",\n                    dist.Binomial(occasion_count, mu_y_current),\n                    obs=y_current.T\n                )\n\n        return (z_current, t + 1), None\n\n    # start everyone in the not yet entered state\n    state_init = jnp.zeros(super_size, dtype=jnp.int32)\n    scan(\n        transition_and_capture,\n        (state_init, 0),\n        history\n    )\n\nsim_data = sim_js()\n\nSimulation complete: 300/400 individuals detected\nPopulation trajectory: N = [133 124 128 145 138 162 154]\nRecruits per session: B = [133  33  44  54  37  57  42]\nDeaths per session: D = [42 40 37 44 33 50]\n\n\nThis is the first NumPyro model where the difference in performance between CPU and GPU is vast. As such, I will use the GPU to sample the models. To use GPU in NumPyro, your computer will have to have a CUDA compatible (i.e., NVIDIA) GPU. By default, these come with laptops and PCs designed for gaming. Alternatively, almost all university or governmental high performance computing clusters (HPCs) will have GPUs. Another option would be Google Colab, which allows people to temporarily rent GPU resources.\nThis model takes about 2 minutes to sample on GPU. While I am not quite patient enough to run it on CPU, it likely takes over an hour to sample on CPU.\n\nfrom numpyro.infer import init_to_feasible\n\nrng_key = random.PRNGKey(RANDOM_SEED)\n\n# specify which sampler you want to use\nnuts_kernel = NUTS(spatial_js)\n\n# configure the MCMC run\nmcmc = MCMC(nuts_kernel, num_warmup=WARMUP_COUNT, num_samples=SAMPLE_COUNT,\n            num_chains=CHAIN_COUNT)\n\n# run the MCMC\nmcmc.run(rng_key, sim_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsamples = mcmc.get_samples(group_by_chain=True)\nidata = az.from_dict(samples)\nidata.posterior['N'] = idata.posterior['psi'] * M\n\npi_rest = (1 - INITIAL_PI) / (SESSION_COUNT - 1)\npi = np.concat([[INITIAL_PI], np.full(SESSION_COUNT - 1, pi_rest)])\n\nparameters = ['N', 'phi', 'alpha0', 'pi', 'sigma']\naxes = az.plot_trace(\n    idata,\n    figsize=(8,10),\n    var_names=parameters,\n    labeller=labeller,\n    lines=[(\"phi\", {}, [APPARENT_SURVIVAL]), (\"alpha0\", {}, [BASELINE_RATE]),\n           (\"pi\", {}, pi), (\"N\", {}, [SUPERPOPULATION_SIZE]),\n           ('sigma', {}, SIGMA)],\n)\nplt.subplots_adjust(hspace=0.4)\n[axes[i, 0].grid(False) for i in range(len(parameters))]\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the model returns parameter estimates well, although some of the entry probabilities, \\(pi_t\\) are clearly biased. Again, we can recover discrete latent states with Predictive.\n\ndef sample_z(model, posterior_samples, data):\n    '''Samples the posterior predictive distribution for z given the histories'''\n\n    # initialize the posterior predictive distribution\n    predictive = Predictive(\n        model,\n        posterior_samples=posterior_samples,\n        return_sites=[\"state\"]\n    )\n\n    # sample z\n    rng_key = random.PRNGKey(RANDOM_SEED)\n    latent_samples = predictive(rng_key, data)\n    return latent_samples[\"state\"]\n\n# generate the posterior predictive distribution for N\nsamples = mcmc.get_samples()\nz = sample_z(spatial_js, samples, sim_data)\never_alive = z.max(axis=1) &gt; 0\nsamples['N'] = ever_alive.sum(axis=1)\n\n\nfig, ax = plt.subplots(figsize=(6,4))\n\nt = np.arange(SESSION_COUNT)\n\nN_t_samps = (z == 1).sum(axis=2)\n\nax.scatter(t, sim_data['N_t'], color='C0', marker='d', label='Truth', s=60)\nax.violinplot(N_t_samps, t, showmedians=False, showextrema=False)\n\nax.legend()\nax.set_xlabel(r'Season')\nax.set_ylabel(r'Abundance')\nax.set_title(r'Population trajectory')\n\nplt.show()\n\n\n\n\n\n\n\n\nThe model is able to recreate the population trajectory over time. While some estimates are slightly off, they are all well within the estimates of uncertainty.",
    "crumbs": [
      "Code",
      "NumPyro",
      "Open SCR"
    ]
  },
  {
    "objectID": "numpyro-occ.html",
    "href": "numpyro-occ.html",
    "title": "Dynamic occupancy",
    "section": "",
    "text": "I started exploring NumPyro because of its ability to marginalize discrete latent states in Hidden Markov Models (HMMs). Thankfully, the NumPyro website includes a handy tutorial on how to fit a Cormack-Jolly-Seber model in NumPyro. Unfortunately, this tutorial also assumes familiarity with many NumPyro concepts. As such, it took me some time to grasp everything that was going on within it. This notebook will hopefully be a gentler introduction to ecological HMMs in NumPyro.\nIn this notebook, I will demonstrate how to estimate patch colonization and extinction with dynamic occupancy models in NumPyro. In a dynamic occupancy model, occupied patches can go extinct (\\(\\epsilon\\)) or remain occupied (\\(1-\\epsilon\\)). Additionally, unoccupied patches can be colonized (\\(\\gamma\\)) or remain unoccupied (\\(1-\\gamma\\)) (Figure 1). The proportion of patches occupied at the first time step is dictated by the parameter \\(\\psi\\), i.e., the initial occupancy probability.\n\n\n\n\n\n\n\n\noccupancy\n\n\n\nU1\n\nz₁ = 0\nUnoccupied\n\n\n\nU2\n\nz₂ = 0\nUnoccupied\n\n\n\nU1-&gt;U2\n\n\n1-γ\n\n\n\nO2\n\nz₂ = 1\nOccupied\n\n\n\nU1-&gt;O2\n\n\nγ\n\n\n\nO1\n\nz₁ = 1\nOccupied\n\n\n\nO1-&gt;U2\n\n\nε\n\n\n\nO1-&gt;O2\n\n\n1-ε\n\n\n\nStart\n\nInitial\n\n\n\nStart-&gt;U1\n\n\n1-ψ\n\n\n\nStart-&gt;O1\n\n\nψ\n\n\n\n\n\n\nFigure 1: Dynamic occupancy model with latent state variable z\n\n\n\n\n\n\nfrom jax import random\nfrom numpyro.contrib.control_flow import scan\nfrom numpyro.infer import NUTS, MCMC, Predictive\nimport arviz as az\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport numpyro.distributions as dist\nimport seaborn as sns\n\n# plotting defaults\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\nsns.set_palette(\"tab10\")\n\n# hyperparameters\nRANDOM_SEED = 1792\n\n## true values for colext model\nPSI_TRUE = 0.6\nEPSILON_TRUE = 0.3\nGAMMA_TRUE = 0.15\nP_TRUE = 0.4\nSITE_COUNT = 250\nSURVEY_COUNT = 3\nSEASON_COUNT = 10\n\ndef simulate_data():\n    \"\"\"Simulate detection/non-detection data from a dynamic occupancy model\"\"\"\n\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # empty array to fill in the occupancy states later\n    z = np.zeros((SITE_COUNT, SEASON_COUNT), dtype=int)\n\n    # initial values for the occupancy state\n    z[:, 0] = rng.binomial(n=1, p=PSI_TRUE, size=SITE_COUNT)\n\n    # simulate transitions\n    for t in range(1, SEASON_COUNT):\n\n        # patches can be colonized, go extinct, remain occupied, or remain unoccupied\n        mu_z = z[:, t-1] * (1 - EPSILON_TRUE) + (1 - z[:, t-1]) * GAMMA_TRUE\n        z[:, t] = rng.binomial(n=1, p=mu_z)\n\n    # simulate detection non-detection data\n    mu_x = z * P_TRUE\n    x = rng.binomial(n=1, p=mu_x[:, :, None],\n                    size=(SITE_COUNT, SEASON_COUNT, SURVEY_COUNT))\n    return x\n\nAs with the static occupancy model, we can parameterize the model in terms of its latent \\(z_{i,j}\\) state, which indicates if site \\(i\\) was occupied. The primary difference is that \\(\\mathbf{Z}\\) is now a matrix, where \\(j\\) indicates the “season”, or primary period. States can transition between seasons. States are however, constant between surveys, or secondary periods, which take place within a season. As such, \\(\\mathbf{Y}\\) is now an array, with shape (site_count, season_count, survey_count), where \\(y_{i,j,k} \\sim \\text{Bern}(z_{i,j} \\, p)\\). It’s possible to include covariate effects on any of the four probabilities in the model: \\(\\psi, \\gamma, \\epsilon\\), and \\(p.\\) Nevertheless, we will introduce the NumPyro syntax with a simple model where every parameter is constant across sites, seasons, and visits.\n\nDefining the model\nJust like in the introductory notebook to NumPyro, we will define our dynamic occupancy model with a Python function, dynamic_occupancy(), that contains NumPyro random variables. This model, however, will contain a new character, the scan() function, as well as an additional function within our model transition_and_detect(). The scan() function sequentially applies the transition_and_detect() to our occupancy data, starting with initial values.\n\ndef dynamic_occupancy(detection_history):\n    '''Dynamic occupancy model in NumPyro.'''\n    site_count, season_count, survey_count = detection_history.shape\n\n    # scalar priors for the four probabilistic parameters\n    psi = numpyro.sample(\"psi\", dist.Uniform(0, 1))  # initial occupancy prob\n    gamma = numpyro.sample(\"gamma\", dist.Uniform(0, 1)) # colonization prob\n    epsilon = numpyro.sample(\"epsilon\", dist.Uniform(0, 1)) # extinction prob\n    p = numpyro.sample(\"p\", dist.Uniform(0, 1))  # recapture prob\n\n    def transition_and_detect(carry, y_t):\n        \"\"\"Transitions betweens states and defines the likelihood.\"\"\"\n\n        # unpack the values that are returned from the transition function at\n        # the previous time step\n        z_prev, t = carry\n\n        # transition the latent state at every site\n        with numpyro.plate(\"sites\", site_count):\n\n            # probability of transitioning according to the previous state\n            mu_z_t = z_prev * (1 - epsilon) + (1 - z_prev) * gamma\n\n            # dist.util.clamp_probs() helps the sampler avoid boundary regions\n            z = numpyro.sample(\n                \"z\",\n                dist.Bernoulli(dist.util.clamp_probs(mu_z_t)),\n                infer={\"enumerate\": \"parallel\"}, # this is where we marginalize!\n            )\n\n            # the likelihood of each observation at each site\n            mu_y = z * p\n            with numpyro.plate('surveys', survey_count):\n                numpyro.sample(\n                    \"y\",\n                    dist.Bernoulli(dist.util.clamp_probs(mu_y)),\n                    obs=y_t.T\n                )\n\n            # carry forward the current z state and incremented time index\n            # None indicates we don't return/accumulate any outputs from scan\n            return (z, t + 1), None\n\n    # the initial state only depends on psi\n    with numpyro.plate('sites', site_count):\n        z0 = numpyro.sample(\n            \"z0\",\n            dist.Bernoulli(dist.util.clamp_probs(psi)),\n            infer={\"enumerate\": \"parallel\"},\n        )\n\n        # compute the likelihood of the detection data for just the first season\n        mu_y = z0 * p\n        with numpyro.plate('surveys', survey_count):\n            numpyro.sample(\n                \"y0\",\n                dist.Bernoulli(dist.util.clamp_probs(mu_y)),\n                obs=detection_history[:, 0].T # just the first occasion!\n            )\n\n    # now we scan (or apply) the transition function across the remaining seasons\n    scan(\n        transition_and_detect,                        # function to scan\n        (z0, 0),                                      # initial states\n        jnp.swapaxes(detection_history[:, 1:], 0, 1), # scan across first dimension of data\n    )\n\nAs the name suggests, transition_and_detect() transitions the discrete latent states (the process model) and detects animals during the surveys (the observation model). As such, it contains our unobserved random variable z as well as our observed random variable y. scan() requires that our inner function, transition_and_detect(), return a tuple, where the first element contains the input to the next iteration of transition_and_detect(). The next element is anything we want to report or accumulate (in this case, None, since we don’t need to track anything).\nFor the most part, the transition_and_detect() function is not much different than our model in the NumPyro static occupancy notebook. One difference is the use of dist.util.clamp_probs(). This is a helper function that keeps the sampler out of boundary regions, for example, by keeping \\(\\psi\\) between \\([1 \\times 10^{-7}, 1 - 1 \\times 10^{-7}]\\) rather than \\([0, 1]\\).\nWe begin the scan after the first occasion. The \\(z_{i,j}\\) state at the first occasion only depends on \\(\\psi\\). After this first occasion, we transition the states such that patches can be colonized or go extinct.\n\ndetections = simulate_data()\nrng_key = random.PRNGKey(RANDOM_SEED)\n\n# specify which sampler you want to use\nnuts_kernel = NUTS(dynamic_occupancy)\n\n# configure the MCMC run\nmcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000, num_chains=4)\n\n# run the MCMC then inspect the output\nmcmc.run(rng_key, detections)\nmcmc.print_summary()\n\n/var/folders/y8/cz021w550rbb072f7qhxyylh0000gq/T/ipykernel_15481/1038765192.py:8: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000, num_chains=4)\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1500 [00:00&lt;19:01,  1.31it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   5%|▍         | 70/1500 [00:00&lt;00:12, 110.07it/s, 15 steps of size 5.91e-02. acc. prob=0.76]warmup:  10%|▉         | 147/1500 [00:00&lt;00:05, 231.34it/s, 15 steps of size 4.55e-01. acc. prob=0.78]warmup:  18%|█▊        | 263/1500 [00:01&lt;00:02, 424.34it/s, 3 steps of size 1.29e+00. acc. prob=0.78] warmup:  25%|██▌       | 379/1500 [00:01&lt;00:01, 592.69it/s, 1 steps of size 4.43e-01. acc. prob=0.78]sample:  34%|███▎      | 503/1500 [00:01&lt;00:01, 751.76it/s, 7 steps of size 7.21e-01. acc. prob=0.99]sample:  42%|████▏     | 624/1500 [00:01&lt;00:01, 869.67it/s, 7 steps of size 7.21e-01. acc. prob=0.91]sample:  49%|████▉     | 738/1500 [00:01&lt;00:00, 941.28it/s, 7 steps of size 7.21e-01. acc. prob=0.91]sample:  57%|█████▋    | 850/1500 [00:01&lt;00:00, 989.86it/s, 3 steps of size 7.21e-01. acc. prob=0.91]sample:  64%|██████▍   | 963/1500 [00:01&lt;00:00, 1027.97it/s, 7 steps of size 7.21e-01. acc. prob=0.91]sample:  72%|███████▏  | 1080/1500 [00:01&lt;00:00, 1066.97it/s, 7 steps of size 7.21e-01. acc. prob=0.91]sample:  80%|███████▉  | 1197/1500 [00:01&lt;00:00, 1094.54it/s, 7 steps of size 7.21e-01. acc. prob=0.91]sample:  87%|████████▋ | 1312/1500 [00:01&lt;00:00, 1109.24it/s, 3 steps of size 7.21e-01. acc. prob=0.91]sample:  95%|█████████▌| 1426/1500 [00:02&lt;00:00, 1099.77it/s, 7 steps of size 7.21e-01. acc. prob=0.91]sample: 100%|██████████| 1500/1500 [00:02&lt;00:00, 701.76it/s, 3 steps of size 7.21e-01. acc. prob=0.91] \n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   5%|▍         | 72/1500 [00:00&lt;00:01, 719.46it/s, 15 steps of size 4.38e-02. acc. prob=0.76]warmup:  10%|█         | 155/1500 [00:00&lt;00:01, 783.10it/s, 3 steps of size 1.54e+00. acc. prob=0.78]warmup:  17%|█▋        | 258/1500 [00:00&lt;00:01, 890.94it/s, 7 steps of size 1.64e+00. acc. prob=0.78]warmup:  24%|██▍       | 366/1500 [00:00&lt;00:01, 958.99it/s, 15 steps of size 5.51e-01. acc. prob=0.78]warmup:  32%|███▏      | 484/1500 [00:00&lt;00:00, 1035.53it/s, 7 steps of size 8.45e-01. acc. prob=0.79]sample:  40%|████      | 603/1500 [00:00&lt;00:00, 1087.39it/s, 3 steps of size 7.64e-01. acc. prob=0.89]sample:  48%|████▊     | 722/1500 [00:00&lt;00:00, 1118.97it/s, 3 steps of size 7.64e-01. acc. prob=0.89]sample:  56%|█████▌    | 838/1500 [00:00&lt;00:00, 1130.40it/s, 7 steps of size 7.64e-01. acc. prob=0.90]sample:  64%|██████▎   | 954/1500 [00:00&lt;00:00, 1138.84it/s, 3 steps of size 7.64e-01. acc. prob=0.90]sample:  71%|███████   | 1068/1500 [00:01&lt;00:00, 1134.46it/s, 3 steps of size 7.64e-01. acc. prob=0.90]sample:  79%|███████▉  | 1182/1500 [00:01&lt;00:00, 1135.75it/s, 3 steps of size 7.64e-01. acc. prob=0.90]sample:  87%|████████▋ | 1298/1500 [00:01&lt;00:00, 1140.63it/s, 7 steps of size 7.64e-01. acc. prob=0.90]sample:  94%|█████████▍| 1413/1500 [00:01&lt;00:00, 1120.33it/s, 7 steps of size 7.64e-01. acc. prob=0.90]sample: 100%|██████████| 1500/1500 [00:01&lt;00:00, 1077.67it/s, 7 steps of size 7.64e-01. acc. prob=0.90]\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   4%|▍         | 64/1500 [00:00&lt;00:02, 623.87it/s, 31 steps of size 3.74e-02. acc. prob=0.76]warmup:   9%|▊         | 131/1500 [00:00&lt;00:02, 648.42it/s, 7 steps of size 1.57e+00. acc. prob=0.78]warmup:  16%|█▌        | 241/1500 [00:00&lt;00:01, 851.54it/s, 7 steps of size 8.12e-01. acc. prob=0.78]warmup:  24%|██▍       | 362/1500 [00:00&lt;00:01, 991.13it/s, 3 steps of size 1.11e+00. acc. prob=0.79]warmup:  32%|███▏      | 478/1500 [00:00&lt;00:00, 1045.43it/s, 15 steps of size 5.57e-01. acc. prob=0.79]sample:  39%|███▉      | 590/1500 [00:00&lt;00:00, 1069.00it/s, 7 steps of size 6.97e-01. acc. prob=0.91] sample:  47%|████▋     | 703/1500 [00:00&lt;00:00, 1088.01it/s, 7 steps of size 6.97e-01. acc. prob=0.91]sample:  54%|█████▍    | 816/1500 [00:00&lt;00:00, 1099.40it/s, 7 steps of size 6.97e-01. acc. prob=0.91]sample:  62%|██████▏   | 928/1500 [00:00&lt;00:00, 1105.19it/s, 7 steps of size 6.97e-01. acc. prob=0.91]sample:  69%|██████▉   | 1039/1500 [00:01&lt;00:00, 1099.85it/s, 7 steps of size 6.97e-01. acc. prob=0.92]sample:  77%|███████▋  | 1150/1500 [00:01&lt;00:00, 1090.59it/s, 3 steps of size 6.97e-01. acc. prob=0.92]sample:  84%|████████▍ | 1260/1500 [00:01&lt;00:00, 1090.71it/s, 7 steps of size 6.97e-01. acc. prob=0.92]sample:  91%|█████████▏| 1370/1500 [00:01&lt;00:00, 1083.97it/s, 7 steps of size 6.97e-01. acc. prob=0.92]sample:  99%|█████████▊| 1481/1500 [00:01&lt;00:00, 1091.68it/s, 7 steps of size 6.97e-01. acc. prob=0.92]sample: 100%|██████████| 1500/1500 [00:01&lt;00:00, 1045.06it/s, 7 steps of size 6.97e-01. acc. prob=0.92]\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   6%|▌         | 84/1500 [00:00&lt;00:01, 824.28it/s, 15 steps of size 4.98e-02. acc. prob=0.77]warmup:  11%|█         | 167/1500 [00:00&lt;00:01, 763.35it/s, 3 steps of size 1.12e+00. acc. prob=0.78]warmup:  18%|█▊        | 275/1500 [00:00&lt;00:01, 895.42it/s, 15 steps of size 4.52e-01. acc. prob=0.78]warmup:  26%|██▌       | 391/1500 [00:00&lt;00:01, 994.88it/s, 7 steps of size 8.66e-01. acc. prob=0.79] warmup:  33%|███▎      | 495/1500 [00:00&lt;00:00, 1009.55it/s, 15 steps of size 5.80e-01. acc. prob=0.79]sample:  41%|████      | 612/1500 [00:00&lt;00:00, 1063.06it/s, 3 steps of size 6.78e-01. acc. prob=0.92] sample:  48%|████▊     | 727/1500 [00:00&lt;00:00, 1090.30it/s, 7 steps of size 6.78e-01. acc. prob=0.92]sample:  56%|█████▌    | 841/1500 [00:00&lt;00:00, 1103.25it/s, 7 steps of size 6.78e-01. acc. prob=0.92]sample:  64%|██████▎   | 956/1500 [00:00&lt;00:00, 1116.84it/s, 3 steps of size 6.78e-01. acc. prob=0.92]sample:  71%|███████   | 1068/1500 [00:01&lt;00:00, 1111.09it/s, 3 steps of size 6.78e-01. acc. prob=0.92]sample:  79%|███████▉  | 1184/1500 [00:01&lt;00:00, 1124.60it/s, 7 steps of size 6.78e-01. acc. prob=0.92]sample:  87%|████████▋ | 1300/1500 [00:01&lt;00:00, 1134.10it/s, 7 steps of size 6.78e-01. acc. prob=0.92]sample:  94%|█████████▍| 1414/1500 [00:01&lt;00:00, 1121.02it/s, 3 steps of size 6.78e-01. acc. prob=0.92]sample: 100%|██████████| 1500/1500 [00:01&lt;00:00, 1066.12it/s, 7 steps of size 6.78e-01. acc. prob=0.92]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n   epsilon      0.26      0.02      0.26      0.22      0.29   3840.88      1.00\n     gamma      0.14      0.01      0.14      0.12      0.16   3676.85      1.00\n         p      0.40      0.01      0.40      0.38      0.42   3777.61      1.00\n       psi      0.58      0.04      0.58      0.52      0.64   3388.00      1.00\n\nNumber of divergences: 0\n\n\n\nlabeller = az.labels.MapLabeller(\n    var_name_map={\"psi\": r\"$\\psi$\", 'gamma':  r\"$\\gamma$\",\n                  'epsilon': r\"$\\epsilon$\", 'p':  r\"$p$\" }\n)\n\nsamples = mcmc.get_samples(group_by_chain=True)\nidata = az.from_dict(samples)\n\naz.plot_trace(\n    idata,\n    figsize=(8,8),\n    var_names=['psi', 'gamma', 'epsilon', 'p'],\n    labeller=labeller,\n    lines=[\n        (\"psi\", {}, [PSI_TRUE]),\n        (\"gamma\", {}, [GAMMA_TRUE]),\n        (\"epsilon\", {}, [EPSILON_TRUE]),\n        (\"p\", {}, [P_TRUE]),\n    ]\n)\nplt.subplots_adjust(hspace=0.4)\n\n\n\n\n\n\n\nFigure 2: Traceplots for the dynamic occupancy model.\n\n\n\n\n\nWe see that the model model recovers parameters well. While \\(\\epsilon\\) is underestimated, I suspect that this is simply a function of Monte Carlo error.\n\n\nPrediction\nWe can recover the latent \\(z_{i,j}\\) states with the Predictive class in NumPyro. This allows us to generate posterior predictive samples given the detection data. In this case, we may want to know the total number of occupied sites throughout the study.\n\ndef sample_z(model, posterior_samples, detection_history):\n    '''Samples the posterior predictive distribution for z given the histories'''\n\n    # initialize the posterior predictive distribution\n    predictive = Predictive(\n        model,\n        posterior_samples=posterior_samples,\n        return_sites=[\"z0\", 'z']\n    )\n\n    # sample z\n    rng_key = random.PRNGKey(RANDOM_SEED)\n    latent_samples = predictive(rng_key, detection_history)\n\n    # z has shape (chain_count * sample_count, interval_count, site_count)\n    z_state = jnp.insert(latent_samples[\"z\"], 0, latent_samples[\"z0\"], axis=1)\n    return z_state\n\n# generate the posterior predictive distribution for N\nsamples = mcmc.get_samples()\nz = sample_z(dynamic_occupancy, samples, detections)\ntotal_occupied = z.sum(axis=2)\n\n\nfig, ax = plt.subplots(figsize=(6,4))\n\nt = np.arange(SEASON_COUNT)\n\ntotal_occupied_median = np.median(total_occupied, axis=0)\n\nax.plot(t, total_occupied_median, linestyle='dotted', color='lightgray', linewidth=2)\nax.violinplot(total_occupied, t, showmedians=True, showextrema=False)\n\nax.set_xlabel(r'Season')\nax.set_ylabel(r'Total occupied sites')\nax.set_title(r'Occupied area decreases over time')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Number of occupied sites over time.",
    "crumbs": [
      "Code",
      "NumPyro",
      "Dynamic occupancy"
    ]
  },
  {
    "objectID": "pymc-occupancy.html",
    "href": "pymc-occupancy.html",
    "title": "Occupancy models",
    "section": "",
    "text": "In this notebook, I demonstrate how to fit static site-occupancy models in PyMC (Royle and Dorazio 2008, chap. 3). The standard site-occupancy model models binary detection/non-detection data \\(y_{j,k}\\) for repeated surveys \\(k=1,2,\\dots,K\\) at sites \\(j=1,2,\\dots,J.\\) The species is present at the sites when \\(z_j=1,\\) and absent otherwise. We assume that our probability of detecting the species given that the site is occupied is \\(P(y_{j,k}|z_j=1)=p,\\) and zero when the site is unoccupied. The probability of occurrence, which is typically the parameter of interest, is \\(P(z_{j}=1)=\\psi.\\) As such, we can think of this as a zero-inflated binomial model, where \\[\n\\begin{align}\n&y_j \\sim\n\\begin{cases}\n    0,   & \\text{if } z_j = 0 \\\\\n    \\text{Binomial}(K, p),   & \\text{if } z_j = 1\n\\end{cases} \\\\\n&z_j \\sim \\text{Bernoulli}(\\psi)\n\\end{align},\n\\] which assumes a constant occurrence probability across sites and a constant detection probability. I start with this simple model, then add site- and visit-level covariates later.",
    "crumbs": [
      "Code",
      "PyMC",
      "Occupancy"
    ]
  },
  {
    "objectID": "pymc-occupancy.html#estimating-parameters-with-pymc",
    "href": "pymc-occupancy.html#estimating-parameters-with-pymc",
    "title": "Occupancy models",
    "section": "Estimating parameters with PyMC",
    "text": "Estimating parameters with PyMC\nNext, I use PyMC to train the occupancy model with the simulated data. First, similar to JAGS and Stan, the model must be specified using the PyMC syntax. This is done using a context manager in Python, essentially, a with statement. This creates a Model object.\n\nwith pm.Model() as constant:\n\n    # priors for the detetion and occurrence probabilities\\\n    psi = pm.Uniform('psi', 0, 1)\n    p = pm.Uniform('p', 0, 1)\n\n    # likelihood for the summarized data\n    pm.ZeroInflatedBinomial('y', p=p, psi=psi, n=visit_count,\n                            observed=y_summarized)\n\nIn JAGS, the prior for \\(p\\) would be specified as p ~ dunif(0, 1). The PyMC equivalent is p = pm.Uniform('p', 0, 1). This could, alternatively, be specified as p = pm.Uniform('detection probability', 0, 1). For the likelihood, I use PyMC’s built-in ZeroInflatedBinomial distribution. We tell PyMC that this is an observed random variable by supplying data to the observed argument. PyMC also has handy tools for visualizing the model.\n\npm.model_to_graphviz(constant)\n\n\n\n\n\n\n\nFigure 1: Visual representation of model \\(p(\\cdot)\\psi(\\cdot).\\) MarginalMixture refers to the zero-inflated binomial distribution.\n\n\n\n\n\nNow I can sample from the posterior. Again, I use the context manager, this time referring to the model by name. It’s typical to name the output with idata because, by default, PyMC returns an object of class InferenceData from the Arviz package. Arviz is similar to the coda package for R.\n\nwith constant:\n    constant_idata = pm.sample()\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [psi, p]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 0 seconds.\n\n\nPyMC will try to use the No-U-Turn Sampler (NUTS) whenever possible. As you can see, it samples the posterior quickly. I can plot the output using the az.plot_trace(), supplying the true values for \\(p\\) and \\(\\psi\\) for comparison. I can also look at a tabular summary using az.summary().\n\naz.plot_trace(\n    constant_idata,\n    compact=True,\n    figsize=(8,4),\n    lines=[(\"psi\", {}, [psi_true]), (\"p\", {}, [p_true])]\n);\n\n\n\n\n\n\n\nFigure 2: Traceplots for the \\(p(\\cdot)\\psi(\\cdot)\\) model. The true parameter values are shown by vertical and horizontal lines.\n\n\n\n\n\n\naz.summary(constant_idata)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\npsi\n0.800\n0.043\n0.720\n0.883\n0.001\n0.001\n2097.0\n2125.0\n1.0\n\n\np\n0.499\n0.030\n0.443\n0.554\n0.001\n0.000\n2486.0\n2448.0\n1.0",
    "crumbs": [
      "Code",
      "PyMC",
      "Occupancy"
    ]
  },
  {
    "objectID": "pymc-occupancy.html#adding-site-covariates",
    "href": "pymc-occupancy.html#adding-site-covariates",
    "title": "Occupancy models",
    "section": "Adding site covariates",
    "text": "Adding site covariates\nNext, I add in some realism by simulating a site-level covariate \\(x\\) that affects the occurrence probability. I model this effect with a logit-linear model, i.e., \\(\\psi_j=\\text{logit}^{-1}(\\beta_0 + \\beta_1 x_j).\\)\n\n## ecological model\n\n# true parameter values\nbeta0_true = -1\nbeta1_true = 3\n\n# covariates\nx = scale(rng.uniform(size=site_count))\n\n# linear model\nmu_true = beta0_true + beta1_true * x\npsi_true = invlogit(mu_true)\n\n# simulate occurrence state\nz_true = rng.binomial(1, psi_true)\n\n## detection model\n\n# true parameter values\np_true = 0.75\n\n# simulate detection\ny = sim_y(p_true, z_true, site_count, visit_count)\n\n# vector with the number of detections at each site\ny_summarized = y.sum(axis=1)\n\n# detection data at the first five sites\ny[:5]\n\narray([[0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0]])\n\n\nAgain, I specify the model with PyMC. Like JAGS, the random variables can be manipulated, as in a linear model with \\(x_j.\\) These behave like NumPy arrays, meaning that vectorized operations and broadcasting are available. To monitor the output of these manipulations, use the pm.Deterministic class. In this case, I am monitoring the site level occurrence probability \\(\\psi_j.\\)\n\nwith pm.Model() as psix:\n\n    # occurrence process\n    # priors\n    beta0 = pm.Normal(\"beta0\", mu=0, sigma=2)\n    beta1 = pm.Normal(\"beta1\", mu=0, sigma=2)\n\n    # linear model\n    mu = beta0 + beta1 * x\n    psi = pm.Deterministic(\"psi\", pm.math.invlogit(mu))\n\n    # detection process\n    # prior\n    p = pm.Uniform('p', 0, 1)\n\n    # likelihood for the summarized data\n    pm.ZeroInflatedBinomial('y', p=p, psi=psi, n=visit_count,\n                            observed=y_summarized)\n\npm.model_to_graphviz(psix)\n\n\n\n\n\n\n\nFigure 3: Visual representation of model \\(p(\\cdot)\\psi(x).\\) MarginalMixture refers to the zero-inflated binomial distribution.\n\n\n\n\n\n\nwith psix:\n    psix_idata = pm.sample()\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [beta0, beta1, p]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\naz.plot_trace(\n    psix_idata,\n    figsize=(8,6),\n    var_names=['beta0', 'beta1', 'p'],\n    lines=[(\"beta0\", {}, [beta0_true]), (\"beta1\", {}, [beta1_true]),\n           ('p', {}, [p_true])]\n);\n\n\n\n\nTraceplots for the \\(p(\\cdot)\\psi(x)\\) model. The true parameter values are shown by vertical and horizontal lines\n\n\n\n\n\naz.summary(psix_idata, var_names=['beta0', 'beta1', 'p'])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nbeta0\n-1.241\n0.266\n-1.723\n-0.744\n0.005\n0.004\n2815.0\n2566.0\n1.0\n\n\nbeta1\n2.858\n0.408\n2.156\n3.657\n0.008\n0.006\n2775.0\n3017.0\n1.0\n\n\np\n0.744\n0.032\n0.683\n0.804\n0.000\n0.001\n4635.0\n2970.0\n1.0",
    "crumbs": [
      "Code",
      "PyMC",
      "Occupancy"
    ]
  },
  {
    "objectID": "pymc-occupancy.html#adding-visit-covariates",
    "href": "pymc-occupancy.html#adding-visit-covariates",
    "title": "Occupancy models",
    "section": "Adding visit covariates",
    "text": "Adding visit covariates\nFinally, I add in visit-level covariate \\(w_{j,k}\\) that affects detection.\n\n## ecological model\n\n# true parameter values\nbeta0_true = -1\nbeta1_true = 3\n\n# covariates\nx = scale(rng.uniform(size=site_count))\n\n# linear model\nmu_true = beta0_true + beta1_true * x\npsi_true = invlogit(mu_true)\n\n# simulate occurrence state\nz_true = rng.binomial(1, psi_true)\n\n# true parameter values\nalpha0_true = 1\nalpha1_true = -3\n\n# covariates\nw = rng.uniform(size=site_count * visit_count).reshape(site_count, visit_count)\nw = scale(w)\n\n# linear model\nnu_true = alpha0_true + alpha1_true * w\np_true = invlogit(nu_true)\n\n# simulate detection\ny = sim_y(p_true, z_true, site_count, visit_count)\n\nprint(y.shape)\ny[:5]\n\n(200, 3)\n\n\narray([[0, 0, 0],\n       [0, 0, 0],\n       [0, 1, 1],\n       [1, 1, 0],\n       [0, 0, 0]])\n\n\nNow that we have visit covariates, we can no longer rely on the pm.ZeroInflatedBinomial class for our likelihood because it assumes that all zeros might be come from either process (i.e., not there or not detected). In an occupancy model, however, this is only true at sites where the species was never detected, and we have no way of compelling pm.ZeroInflatedBinomial to only consider zero inflation at certain sites.\nAs such, we will now code the model in terms of the discrete, latent \\(z_i\\) state, where \\(z_i=1\\) if site \\(i\\) is occupied and \\(z_i=0\\) otherwise. This is the canonical way to code occupancy models in WinBUGS, JAGS, and NIMBLE (Royle and Dorazio 2008; Kéry and Schaub 2011). The NUTS sampler, however, does not jive with discrete latent states. As such, PyMC will assign a binary Gibbs sampler to z by default, which works, albeit less efficiently than the NUTS sampler. That said, PyMC will assign the other continuous parameters to the NUTS sampler, which is good!\n\nwith pm.Model() as binary_gibbs:\n\n    # occurrence process\n    # priors\n    beta0 = pm.Normal(\"beta0\", mu=0, sigma=2)\n    beta1 = pm.Normal(\"beta1\", mu=0, sigma=2)\n\n    # linear model\n    mu = beta0 + beta1 * x\n    psi = pm.Deterministic(\"psi\", pm.math.invlogit(mu))\n\n    # detection process\n    # priors\n    alpha0 = pm.Normal('alpha0', mu=0, sigma=2)\n    alpha1 = pm.Normal('alpha1', mu=0, sigma=2)\n\n    # linear model\n    nu = alpha0 + alpha1 * w\n    p = pm.Deterministic('p', pm.math.invlogit(nu))\n\n    # occupied / unoccupied state at each site\n    z = pm.Bernoulli(\"z\", psi)\n\n    # [:, None] allows us to multiply a vector across every column of a matrix\n    mu_y = z[:, None] * p\n\n    # the likelihood is now bernoulli conditional on the z state\n    pm.Bernoulli(\"y\", mu_y, observed=y)\n\npm.model_to_graphviz(binary_gibbs)\n\n\n\n\n\n\n\nFigure 4: Visual representation of the \\(p(w)\\psi(w)\\) model.\n\n\n\n\n\nThe mu_y = z[:, None] * p notation may look strange to an R user. This is a trick that’s related to NumPy’s broadcasting rules. Broadcasting allows us to multiply arrays with different dimensions. In this case, we have a vector z that we would like to multiply against a matrix p, such that the first value in z is multiplied against every value in the first row of p, and so on. R does this naturally since it has slightly different broadcasting rules than NumPy. To do this in NumPy, we need to make z a column vector by adding a dummy dimension, hence the [:, None].\n\nwith binary_gibbs:\n    binary_gibbs_idata = pm.sample()\naz.summary(binary_gibbs_idata, var_names=['beta0', 'beta1', 'alpha0', 'alpha1'])\n\nMultiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;NUTS: [beta0, beta1, alpha0, alpha1]\n&gt;BinaryGibbsMetropolis: [z]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 9 seconds.\n/Users/PattonP/miniforge3/envs/nut/lib/python3.13/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nbeta0\n-0.746\n0.244\n-1.211\n-0.305\n0.004\n0.004\n3237.0\n2897.0\n1.0\n\n\nbeta1\n2.803\n0.401\n2.089\n3.556\n0.008\n0.006\n2680.0\n2614.0\n1.0\n\n\nalpha0\n1.389\n0.270\n0.898\n1.905\n0.006\n0.004\n2052.0\n2161.0\n1.0\n\n\nalpha1\n-3.070\n0.381\n-3.780\n-2.378\n0.008\n0.006\n2432.0\n2462.0\n1.0\n\n\n\n\n\n\n\nWe see that the model recovers the simulated parameters. Nevertheless, it did take 13 seconds to do so on my machine. While this is fairly quick, we can do better.",
    "crumbs": [
      "Code",
      "PyMC",
      "Occupancy"
    ]
  },
  {
    "objectID": "pymc-occupancy.html#automated-marginalization",
    "href": "pymc-occupancy.html#automated-marginalization",
    "title": "Occupancy models",
    "section": "Automated Marginalization",
    "text": "Automated Marginalization\nPyMC has a handy experimental feature called marginalize that automatically marginalizes out discrete latent states. This means that we can sample every parameter in the model with NUTS. This is not only faster than the hybrid sampler above, it also explores the parameter space more efficiently, which can be important for complex models or challenging datasets\nmarginalize is an experimental feature, and as such should be treated with some caution. Nevertheless, I have had success with it in the most common closed Bayesian population models (e.g., occupancy, capture-recapture, and distance sampling). The PyMC team houses these experimental features in the pymc-extras package, which can be installed following the instructions here.\n\nimport pymc_extras as pmx\nmarginal = pmx.marginalize(binary_gibbs, [\"z\"])\n\nwith marginal:\n    marginal_idata = pm.sample()\n\naz.summary(marginal_idata, var_names=['beta0', 'beta1', 'alpha0', 'alpha1'])\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha1, alpha0, beta1, beta0]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nbeta0\n-0.751\n0.240\n-1.175\n-0.284\n0.005\n0.004\n2691.0\n2694.0\n1.0\n\n\nbeta1\n2.796\n0.400\n2.061\n3.542\n0.008\n0.007\n2829.0\n2236.0\n1.0\n\n\nalpha0\n1.403\n0.278\n0.878\n1.922\n0.005\n0.004\n3239.0\n3031.0\n1.0\n\n\nalpha1\n-3.092\n0.382\n-3.785\n-2.349\n0.007\n0.005\n3059.0\n2975.0\n1.0\n\n\n\n\n\n\n\n\naz.plot_trace(\n    binary_gibbs_idata,\n    figsize=(8,6),\n    var_names=['beta0', 'beta1', 'alpha0', 'alpha1'],\n    lines=[(\"beta0\", {}, [beta0_true]), (\"beta1\", {}, [beta1_true]),\n           ('alpha0', {}, [alpha0_true]), ('alpha1', {}, [alpha1_true])]\n);\n\n\n\n\n\n\n\nFigure 5: Tracepots for the \\(p(w)\\psi(x)\\) model. The true parameter values are shown by vertical and horizontal lines\n\n\n\n\n\nAs we can see, the marginal model produces the same estimates in one second. That means that the hybrid sampler took 13 times as long! While 13 seconds doesn’t seem like a lot, remember that this is simulated data and a rather simple model. Imagine if, say, the marginal model took five minutes to fit!",
    "crumbs": [
      "Code",
      "PyMC",
      "Occupancy"
    ]
  },
  {
    "objectID": "numpyro-cjs.html",
    "href": "numpyro-cjs.html",
    "title": "Cormack-Jolly-Seber",
    "section": "",
    "text": "In this notebook, I demonstrate how to estimate survival with Cormack-Jolly-Seber models in NumPyro. This notebook is a near carbon copy of the CJS notebook on the NumPyro documentation page. Nevertheless, I hope that this notebook will be useful in its own right, primarily for folks who are more familiar with CJS models and less familiar with NumPyro.\n\nfrom jax import random\nfrom jax.scipy.special import expit\nfrom numpyro import handlers\nfrom numpyro.contrib.control_flow import scan\nfrom numpyro.infer import NUTS, MCMC\nimport arviz as az\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport numpyro.distributions as dist\nimport seaborn as sns\n\n# plotting defaults\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\nsns.set_palette(\"tab10\")\n\n# make the labels on arviz plots nicer\nlabeller = az.labels.MapLabeller(\n    var_name_map={\"psi\": r\"$\\psi$\", 'gamma':  r\"$\\gamma$\", 'alpha': r'$\\alpha$',\n                  'epsilon': r\"$\\epsilon$\", 'p':  r\"$p$\" , 'beta': r'$\\beta$',\n                  'phi': r'$\\phi$', 'alpha_t': r'$\\alpha_t$',}\n)\n\n# hyperparameters\nRANDOM_SEED = 1792\nCHAIN_COUNT = 4\nWARMUP_COUNT = 500\nSAMPLE_COUNT = 1000\n\n\nModel definition\nThis model is very similar to the one defined in the NumPyro Jolly-Seber notebook. In some ways, this model is actually much simpler because we do not have to worry about recruitment into the population. This model does, however, introduce a new character from the NumPyro-verse: handlers.mask().\nhandlers.mask() tells NumPyro: do not include this the sample of z in the log probability computation when the mask is False. In this case, the mask, has_been_captured, indicates whether the animal has been captured by time t. As such, we essentially ignore the z state until the animal is captured. We include the mask in the carry, and update it with new data after we transition the z state (i.e., after the animal has survived or died between intervals).\nThe model requires one more trick to run. Essentially, we need to ensure that z[t]=1 during the occasion of the animal’s first capture. To do so, we employ the mask: mu_z_t = has_been_captured * phi * z + (1 - has_been_captured). This forces z[t]=1 until the animals first capture. After the animal’s first capture, the z state is included in the log probability calculation, and mu_z_t simplifies to phi * z.\n\ndef p_dot_phi_dot(capture_history):\n\n    capture_count, _ = capture_history.shape\n    phi = numpyro.sample(\"phi\", dist.Uniform(0.0, 1.0))\n    p = numpyro.sample(\"p\", dist.Uniform(0.0, 1.0))\n\n    def transition_fn(carry, y):\n\n        has_been_captured, z = carry\n\n        with numpyro.plate(\"animals\", capture_count, dim=-1):\n\n            # only compute log probs for animals where has_been_captured is True\n            with handlers.mask(mask=has_been_captured):\n\n                # force mu_z_t=1 during the occasion of the animal's first capture\n                mu_z_t = has_been_captured * phi * z + (1 - has_been_captured)\n                z = numpyro.sample(\n                    \"z\",\n                    dist.Bernoulli(dist.util.clamp_probs(mu_z_t)),\n                    infer={\"enumerate\": \"parallel\"},\n                )\n\n                mu_y_t = p * z\n                numpyro.sample(\n                    \"y\", dist.Bernoulli(dist.util.clamp_probs(mu_y_t)), obs=y\n                )\n\n        has_been_captured = has_been_captured | y.astype(bool)\n        return (has_been_captured, z), None\n\n    z = jnp.ones(capture_count, dtype=jnp.int32)\n    has_been_captured = capture_history[:, 0].astype(bool)\n    scan(\n        transition_fn,\n        (has_been_captured, z),\n        jnp.swapaxes(capture_history[:, 1:], 0, 1),\n    )\n\n\n# data\ndipper = np.loadtxt('dipper.csv', delimiter=',', dtype=np.int32)\n\nrng_key = random.PRNGKey(RANDOM_SEED)\n\n# specify which sampler you want to use\nnuts_kernel = NUTS(p_dot_phi_dot) # 11 seconds\n\n# configure the MCMC run\nmcmc = MCMC(nuts_kernel, num_warmup=WARMUP_COUNT, num_samples=SAMPLE_COUNT,\n            num_chains=CHAIN_COUNT)\n\n# run the MCMC then inspect the output\nmcmc.run(rng_key, dipper)\nmcmc.print_summary()\n\n/var/folders/y8/cz021w550rbb072f7qhxyylh0000gq/T/ipykernel_16207/284679213.py:10: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  mcmc = MCMC(nuts_kernel, num_warmup=WARMUP_COUNT, num_samples=SAMPLE_COUNT,\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1500 [00:00&lt;16:08,  1.55it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   8%|▊         | 117/1500 [00:00&lt;00:06, 211.15it/s, 3 steps of size 1.22e+00. acc. prob=0.78]warmup:  17%|█▋        | 260/1500 [00:00&lt;00:02, 460.99it/s, 7 steps of size 9.17e-01. acc. prob=0.78]warmup:  28%|██▊       | 415/1500 [00:00&lt;00:01, 707.54it/s, 7 steps of size 8.39e-01. acc. prob=0.79]sample:  38%|███▊      | 563/1500 [00:01&lt;00:01, 895.60it/s, 3 steps of size 8.91e-01. acc. prob=0.92]sample:  47%|████▋     | 710/1500 [00:01&lt;00:00, 1042.74it/s, 23 steps of size 8.91e-01. acc. prob=0.91]sample:  58%|█████▊    | 869/1500 [00:01&lt;00:00, 1190.21it/s, 3 steps of size 8.91e-01. acc. prob=0.91] sample:  68%|██████▊   | 1022/1500 [00:01&lt;00:00, 1283.62it/s, 3 steps of size 8.91e-01. acc. prob=0.91]sample:  78%|███████▊  | 1169/1500 [00:01&lt;00:00, 1327.50it/s, 7 steps of size 8.91e-01. acc. prob=0.92]sample:  88%|████████▊ | 1315/1500 [00:01&lt;00:00, 1361.39it/s, 3 steps of size 8.91e-01. acc. prob=0.91]sample:  98%|█████████▊| 1470/1500 [00:01&lt;00:00, 1412.15it/s, 7 steps of size 8.91e-01. acc. prob=0.91]sample: 100%|██████████| 1500/1500 [00:01&lt;00:00, 897.14it/s, 3 steps of size 8.91e-01. acc. prob=0.91] \n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   7%|▋         | 109/1500 [00:00&lt;00:01, 1088.55it/s, 15 steps of size 4.07e-01. acc. prob=0.77]warmup:  16%|█▌        | 239/1500 [00:00&lt;00:01, 1206.69it/s, 7 steps of size 9.35e-01. acc. prob=0.78] warmup:  25%|██▌       | 379/1500 [00:00&lt;00:00, 1294.50it/s, 7 steps of size 5.36e-01. acc. prob=0.79]sample:  35%|███▌      | 527/1500 [00:00&lt;00:00, 1362.30it/s, 15 steps of size 8.83e-01. acc. prob=0.87]sample:  46%|████▌     | 687/1500 [00:00&lt;00:00, 1447.22it/s, 3 steps of size 8.83e-01. acc. prob=0.89] sample:  57%|█████▋    | 851/1500 [00:00&lt;00:00, 1512.56it/s, 3 steps of size 8.83e-01. acc. prob=0.90]sample:  69%|██████▊   | 1031/1500 [00:00&lt;00:00, 1604.57it/s, 3 steps of size 8.83e-01. acc. prob=0.90]sample:  80%|████████  | 1205/1500 [00:00&lt;00:00, 1645.05it/s, 7 steps of size 8.83e-01. acc. prob=0.90]sample:  91%|█████████▏| 1370/1500 [00:00&lt;00:00, 1634.17it/s, 3 steps of size 8.83e-01. acc. prob=0.90]sample: 100%|██████████| 1500/1500 [00:00&lt;00:00, 1522.71it/s, 3 steps of size 8.83e-01. acc. prob=0.90]\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   7%|▋         | 101/1500 [00:00&lt;00:01, 988.60it/s, 63 steps of size 1.35e+00. acc. prob=0.78]warmup:  16%|█▋        | 245/1500 [00:00&lt;00:01, 1248.66it/s, 3 steps of size 1.61e+00. acc. prob=0.79]warmup:  26%|██▌       | 390/1500 [00:00&lt;00:00, 1338.13it/s, 7 steps of size 9.76e-01. acc. prob=0.79]sample:  36%|███▌      | 534/1500 [00:00&lt;00:00, 1377.06it/s, 3 steps of size 7.46e-01. acc. prob=0.93]sample:  46%|████▌     | 684/1500 [00:00&lt;00:00, 1420.81it/s, 7 steps of size 7.46e-01. acc. prob=0.93]sample:  55%|█████▌    | 827/1500 [00:00&lt;00:00, 1404.01it/s, 3 steps of size 7.46e-01. acc. prob=0.93]sample:  65%|██████▍   | 968/1500 [00:00&lt;00:00, 1400.69it/s, 3 steps of size 7.46e-01. acc. prob=0.93]sample:  74%|███████▍  | 1109/1500 [00:00&lt;00:00, 1394.77it/s, 7 steps of size 7.46e-01. acc. prob=0.93]sample:  83%|████████▎ | 1249/1500 [00:00&lt;00:00, 1391.84it/s, 3 steps of size 7.46e-01. acc. prob=0.93]sample:  93%|█████████▎| 1389/1500 [00:01&lt;00:00, 1393.28it/s, 3 steps of size 7.46e-01. acc. prob=0.93]sample: 100%|██████████| 1500/1500 [00:01&lt;00:00, 1377.58it/s, 7 steps of size 7.46e-01. acc. prob=0.93]\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   7%|▋         | 109/1500 [00:00&lt;00:01, 1080.08it/s, 7 steps of size 4.53e-01. acc. prob=0.77]warmup:  18%|█▊        | 264/1500 [00:00&lt;00:00, 1352.15it/s, 7 steps of size 1.13e+00. acc. prob=0.78]warmup:  29%|██▊       | 429/1500 [00:00&lt;00:00, 1483.98it/s, 7 steps of size 6.55e-01. acc. prob=0.79]sample:  39%|███▊      | 578/1500 [00:00&lt;00:00, 1381.35it/s, 3 steps of size 8.33e-01. acc. prob=0.92]sample:  49%|████▉     | 735/1500 [00:00&lt;00:00, 1443.02it/s, 7 steps of size 8.33e-01. acc. prob=0.91]sample:  59%|█████▉    | 886/1500 [00:00&lt;00:00, 1463.56it/s, 3 steps of size 8.33e-01. acc. prob=0.91]sample:  69%|██████▉   | 1034/1500 [00:00&lt;00:00, 1444.10it/s, 7 steps of size 8.33e-01. acc. prob=0.91]sample:  79%|███████▉  | 1183/1500 [00:00&lt;00:00, 1458.10it/s, 7 steps of size 8.33e-01. acc. prob=0.91]sample:  89%|████████▉ | 1341/1500 [00:00&lt;00:00, 1493.32it/s, 3 steps of size 8.33e-01. acc. prob=0.91]sample: 100%|█████████▉| 1495/1500 [00:01&lt;00:00, 1505.52it/s, 3 steps of size 8.33e-01. acc. prob=0.91]sample: 100%|██████████| 1500/1500 [00:01&lt;00:00, 1454.42it/s, 3 steps of size 8.33e-01. acc. prob=0.91]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n         p      0.91      0.02      0.91      0.87      0.95   2737.11      1.00\n       phi      0.69      0.03      0.69      0.64      0.73   3004.49      1.00\n\nNumber of divergences: 0\n\n\n\n\n\n\nsamples = mcmc.get_samples(group_by_chain=True)\nidata = az.from_dict(samples)\n\naz.plot_trace(idata, figsize=(8,4), var_names=['p', 'phi'], labeller=labeller)\nplt.subplots_adjust(hspace=0.4)",
    "crumbs": [
      "Code",
      "NumPyro",
      "Cormack-Jolly-Seber"
    ]
  },
  {
    "objectID": "numpyro-js.html",
    "href": "numpyro-js.html",
    "title": "Jolly-Seber",
    "section": "",
    "text": "Jolly-Seber models are a form of open-population capture-recapture models that estimate survival and abundance. They do so by estimating recruitment, broadly defined, at every time step. Contrast this with Cormack-Jolly-Seber models, which ignore recruitment to focus on just estimating survival.\nThere are many parameterizations of the Jolly-Seber model. For example, the Jolly-Seber model can be conceived of as a multi-state model with three states: not yet entered, alive, and dead (Kéry and Schaub 2011) (Figure 1). Those who have not yet entered, \\(z_{i,t}=0\\), can be recruited into the population, e.g., be born or immigrate into it, thus having \\(z_{i, t+1}=1\\). These individuals may be captured with probability \\(p\\). Then, these individuals might die or immigrate out of the population, thus having \\(z_{i, t+1}=2\\). Such individuals can never re-enter the population.\nThis multi-state version of the Jolly-Seber is essentially the same as the restricted occupancy parameterization (Royle and Dorazio 2008, chap. 10). As such, the \\(\\gamma\\) parameter is a nuisance parameter with no biological meaning (Kéry and Schaub 2011) (Figure 1). It represents “removal entry probabilities” (Kéry and Schaub 2011), i.e., the probability that the individual is removed from the superpopulation at time \\(t\\), which contains \\(M\\) individuals. As such, \\(\\gamma\\) tends to grow over time because \\(M\\) is fixed across all periods. In this case, \\(M\\) refers to the size of the augmented dataset, meaning that, as in the closed capture-recapture notebook, we will be augmenting the capture histories with all-zero histories.\njs_multistate\n\n\ncluster_t1\n\nTime t+1\n\n\ncluster_t\n\nTime t\n\n\n\nN1\n\nNot yet\nentered\nz=0\n\n\n\nN2\n\nNot yet\nentered\nz=0\n\n\n\nN1-&gt;N2\n\n\n1-γ\n\n\n\nA2\n\nAlive\nz=1\n\n\n\nN1-&gt;A2\n\n\nγ\n\n\n\nA1\n\nAlive\nz=1\n\n\n\nA1-&gt;A2\n\n\nφ\n\n\n\nD2\n\nDead\nz=2\n\n\n\nA1-&gt;D2\n\n\n1-φ\n\n\n\nD1\n\nDead\nz=2\n\n\n\nD1-&gt;D2\n\n\n1\n\n\n\nobs\n\nyₜ ~ Bernoulli(p·𝟙[z=1])\n\n\n\nA2-&gt;obs\n\n\n\n\n\n\n\n\nFigure 1: Jolly-Seber multistate model: recruitment (γ), survival (φ), detection (p)\nIn this notebook, I will primarily analyze simulated data. This is helpful because, as we will see, these models are sensitive to priors on \\(\\gamma\\). The simulation code is a direct translation of the code from (Kéry and Schaub 2011, chap. 10), although I’ve vectorized the operations and leveraged some NumPy tricks, like accummulate().\n%config InlineBackend.figure_format = 'retina'\n\nfrom jax import random\nfrom jax.scipy.special import expit\nfrom numpyro.contrib.control_flow import scan\nfrom numpyro.infer import NUTS, MCMC, Predictive\nimport arviz as az\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport numpyro.distributions as dist\nimport seaborn as sns\n\n# plotting defaults\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\nsns.set_palette(\"tab10\")\n\n# hyperparameters\nRANDOM_SEED = 89\nMAX_ABUNDANCE = 500 # size of augmented dipper dataset\n\n# mcmc hyperparameters\nCHAIN_COUNT = 4\nWARMUP_COUNT = 500\nSAMPLE_COUNT = 1000\n\n# simulation hyperparameters\nOCCASION_COUNT = 7\nSUPERPOPULATION_SIZE = 400\nAPPARENT_SURVIVAL = 0.7\nINITIAL_PI = 0.34\nRECAPTURE_RATE = 0.5\nM = 1000\n\n# make the labels on arviz plots nicer\nlabeller = az.labels.MapLabeller(\n    var_name_map={\"psi\": r\"$\\psi$\", 'gamma':  r\"$\\gamma$\", 'alpha': r'$\\alpha$',\n                  'epsilon': r\"$\\epsilon$\", 'p':  r\"$p$\" , 'beta': r'$\\beta$',\n                  'phi': r'$\\phi$', 'alpha_t': r'$\\alpha_t$',}\n)\n\ndef augment_history(history):\n    '''Augments a capture histories with all zero histories'''\n    number_encountered, occasion_count = history.shape\n    number_to_augment = MAX_ABUNDANCE - number_encountered\n    all_zero_histories = np.zeros((number_to_augment, occasion_count))\n    y_augmented = np.vstack([history, all_zero_histories])\n    return y_augmented.astype(np.int32)\n\ndef load_dipper_data():\n    '''Loads and augments the classic dipper dataset'''\n    dipper = np.loadtxt('dipper.csv', delimiter=',').astype(int)\n    y_augmented = augment_history(dipper)\n\n    return y_augmented\n\ndef sim_js():\n    \"\"\"Simulation code ported from Kery and Schaub (2012), Chapter 10\"\"\"\n\n    rng = np.random.default_rng(RANDOM_SEED)\n    interval_count = OCCASION_COUNT - 1\n\n    # simulate entry into the population\n    pi_rest = (1 - INITIAL_PI) / interval_count\n    pi = np.concatenate([[INITIAL_PI], np.full(interval_count, pi_rest)])\n\n    # which occasion did the animal enter in?\n    entry_matrix = rng.multinomial(n=1, pvals=pi, size=SUPERPOPULATION_SIZE)\n    entry_occasion = entry_matrix.nonzero()[1]\n    _, entrant_count = np.unique(entry_occasion, return_counts=True)\n\n    # zero if the animal has not yet entered and one after it enters\n    entry_trajectory = np.maximum.accumulate(entry_matrix, axis=1)\n\n    # flip coins for survival between occasions\n    survival_draws = rng.binomial(\n        1, APPARENT_SURVIVAL, (SUPERPOPULATION_SIZE, interval_count)\n    )\n\n    # add column such that survival between t and t+1 implies alive at t+1\n    survival_draws = np.column_stack([np.ones(SUPERPOPULATION_SIZE), survival_draws])\n\n    # ensure that the animal survives until it enters\n    is_yet_to_enter = np.arange(OCCASION_COUNT) &lt;= entry_occasion[:, None]\n    survival_draws[is_yet_to_enter] = 1\n\n    # once the survival_draws flips to zero the remaining row stays 0\n    survival_trajectory = np.cumprod(survival_draws, axis=1)\n\n    # animal has entered AND is still alive\n    state = entry_trajectory * survival_trajectory\n\n    # binary matrix of random possible recaptures\n    capture = rng.binomial(\n        1, RECAPTURE_RATE, (SUPERPOPULATION_SIZE, OCCASION_COUNT)\n    )\n\n    # remove the non-detected individuals\n    capture_history = state * capture\n    was_captured = capture_history.sum(axis=1) &gt; 0\n    capture_history = capture_history[was_captured]\n\n    # augment the history with nz animals\n    n, _ = capture_history.shape\n    nz = M - n\n    all_zero_history = np.zeros((nz, OCCASION_COUNT))\n    capture_history = np.vstack([capture_history, all_zero_history]).astype(int)\n\n    # return a dict with relevant summary stats\n    N_t = state.sum(axis=0)\n    return {\n        'capture_history': capture_history,\n        'N_t': N_t,\n        'B': entrant_count,\n    }",
    "crumbs": [
      "Code",
      "NumPyro",
      "Jolly-Seber"
    ]
  },
  {
    "objectID": "numpyro-js.html#schwarz-arnason-esque-prior",
    "href": "numpyro-js.html#schwarz-arnason-esque-prior",
    "title": "Jolly-Seber",
    "section": "Schwarz-Arnason-esque prior",
    "text": "Schwarz-Arnason-esque prior\nThis version of the model is very similar to the Schwarz-Arnason parameterization described in (Royle and Dorazio 2008). We develop this model by placing a prior distribution on the entry probabilities, \\(\\pi,\\) and the inclusion probability, \\(\\psi.\\) We can use these values to compute the removal probabilities, \\[\n\\begin{align}\n\\gamma_0 &= \\psi \\pi_0 \\\\\n\\gamma_t &= \\frac{\\psi \\pi_t}{\\prod_{j=1}^{t-1}(1 - \\gamma_j)}, \\quad t = 1, \\ldots, T\n\\end{align}\n\\]\nOne benefit of this approach is that it obviates the need to compute the derived quanitites. Moreover, it allows us to directly compare to the data generating processs.\n\ndef js_prior2(capture_history):\n\n    super_size, occasion_count = capture_history.shape\n\n    phi = numpyro.sample('phi', dist.Uniform(0, 1))\n    p = numpyro.sample('p', dist.Uniform(0, 1))\n\n    # parameterize the  entry probabilities in terms of pi and psi\n    psi = numpyro.sample('psi', dist.Uniform(0, 1))\n    pi = numpyro.sample('pi', dist.Dirichlet(jnp.ones(occasion_count)))\n\n    # compute the removal probabilities as a function of psi and pi\n    gamma = jnp.zeros(occasion_count)\n\n    # the `vector.at[0].set(1)` notation is jax for `vector[0] = 1`\n    gamma = gamma.at[0].set(psi * pi[0])\n    for t in range(1, occasion_count):\n        denominator = jnp.prod(1 - gamma[:t])\n        gamma = gamma.at[t].set(psi * pi[t] / denominator)\n    gamma = numpyro.deterministic('gamma', gamma)\n\n    def transition_and_capture(carry, y_current):\n\n        z_previous, t = carry\n\n        trans_probs = jnp.array([\n            [1 - gamma[t], gamma[t], 0.0],  # From not yet entered\n            [0.0, phi, 1 - phi],            # From alive\n            [0.0, 0.0, 1.0]                 # From dead\n        ])\n\n        with numpyro.plate(\"animals\", super_size, dim=-1):\n\n            mu_z_current = trans_probs[z_previous]\n            z_current = numpyro.sample(\n                \"state\",\n                dist.Categorical(dist.util.clamp_probs(mu_z_current)),\n                infer={\"enumerate\": \"parallel\"}\n            )\n\n            mu_y_current = jnp.where(z_current == 1, p, 0.0)\n            numpyro.sample(\n                \"obs\",\n                dist.Bernoulli(dist.util.clamp_probs(mu_y_current)),\n                obs=y_current\n            )\n\n        return (z_current, t + 1), None\n\n    # start everyone in the not yet entered state\n    state_init = jnp.zeros(super_size, dtype=jnp.int32)\n    scan(\n        transition_and_capture,\n        (state_init, 0),\n         jnp.swapaxes(capture_history, 0, 1)\n    )\n\n\n# specify which sampler you want to use\nnuts_kernel = NUTS(js_prior2)\n\n# configure the MCMC run\nmcmc = MCMC(nuts_kernel, num_warmup=WARMUP_COUNT, num_samples=SAMPLE_COUNT,\n            num_chains=CHAIN_COUNT)\n\n# run the MCMC then inspect the output\nmcmc.run(rng_key, capture_histories)\nmcmc.print_summary()\n\n/var/folders/y8/cz021w550rbb072f7qhxyylh0000gq/T/ipykernel_16237/3518034286.py:5: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  mcmc = MCMC(nuts_kernel, num_warmup=WARMUP_COUNT, num_samples=SAMPLE_COUNT,\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1500 [00:00&lt;20:35,  1.21it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   1%|          | 11/1500 [00:00&lt;01:40, 14.76it/s, 127 steps of size 3.32e-02. acc. prob=0.64]warmup:   1%|▏         | 19/1500 [00:01&lt;01:00, 24.60it/s, 63 steps of size 3.83e-02. acc. prob=0.70] warmup:   2%|▏         | 27/1500 [00:01&lt;00:42, 34.78it/s, 7 steps of size 2.22e-02. acc. prob=0.72] warmup:   2%|▏         | 36/1500 [00:01&lt;00:31, 45.85it/s, 15 steps of size 7.82e-02. acc. prob=0.75]warmup:   3%|▎         | 47/1500 [00:01&lt;00:24, 58.45it/s, 31 steps of size 9.26e-02. acc. prob=0.76]warmup:   4%|▍         | 59/1500 [00:01&lt;00:20, 70.74it/s, 31 steps of size 5.29e-02. acc. prob=0.76]warmup:   5%|▍         | 68/1500 [00:01&lt;00:19, 75.03it/s, 15 steps of size 7.42e-02. acc. prob=0.77]warmup:   5%|▌         | 77/1500 [00:01&lt;00:18, 76.99it/s, 15 steps of size 1.19e-01. acc. prob=0.77]warmup:   6%|▌         | 88/1500 [00:01&lt;00:16, 84.93it/s, 15 steps of size 8.59e-02. acc. prob=0.77]warmup:   7%|▋         | 98/1500 [00:01&lt;00:15, 88.95it/s, 31 steps of size 7.90e-02. acc. prob=0.77]warmup:   7%|▋         | 108/1500 [00:02&lt;00:16, 86.32it/s, 7 steps of size 9.10e-01. acc. prob=0.77]warmup:   8%|▊         | 121/1500 [00:02&lt;00:14, 97.42it/s, 15 steps of size 5.51e-01. acc. prob=0.78]warmup:   9%|▉         | 139/1500 [00:02&lt;00:11, 117.12it/s, 31 steps of size 3.25e-01. acc. prob=0.78]warmup:  10%|█         | 155/1500 [00:02&lt;00:10, 122.56it/s, 63 steps of size 2.38e-01. acc. prob=0.77]warmup:  11%|█▏        | 171/1500 [00:02&lt;00:10, 132.80it/s, 7 steps of size 5.82e-01. acc. prob=0.78] warmup:  13%|█▎        | 188/1500 [00:02&lt;00:09, 142.86it/s, 15 steps of size 4.38e-01. acc. prob=0.78]warmup:  14%|█▍        | 207/1500 [00:02&lt;00:08, 155.91it/s, 7 steps of size 7.36e-01. acc. prob=0.78] warmup:  15%|█▌        | 227/1500 [00:02&lt;00:07, 168.65it/s, 7 steps of size 3.30e-01. acc. prob=0.78]warmup:  17%|█▋        | 251/1500 [00:02&lt;00:06, 188.95it/s, 7 steps of size 3.18e+00. acc. prob=0.78]warmup:  18%|█▊        | 271/1500 [00:03&lt;00:06, 191.72it/s, 7 steps of size 1.07e+00. acc. prob=0.78]warmup:  19%|█▉        | 291/1500 [00:03&lt;00:06, 181.07it/s, 31 steps of size 2.31e-01. acc. prob=0.78]warmup:  21%|██        | 310/1500 [00:03&lt;00:06, 181.88it/s, 7 steps of size 5.25e-01. acc. prob=0.78] warmup:  22%|██▏       | 333/1500 [00:03&lt;00:06, 194.45it/s, 7 steps of size 5.31e-01. acc. prob=0.78]warmup:  24%|██▎       | 355/1500 [00:03&lt;00:05, 196.70it/s, 15 steps of size 3.91e-01. acc. prob=0.78]warmup:  25%|██▌       | 381/1500 [00:03&lt;00:05, 212.40it/s, 7 steps of size 4.68e-01. acc. prob=0.79] warmup:  27%|██▋       | 405/1500 [00:03&lt;00:04, 219.46it/s, 7 steps of size 3.96e-01. acc. prob=0.79]warmup:  29%|██▊       | 429/1500 [00:03&lt;00:04, 223.25it/s, 15 steps of size 4.80e-01. acc. prob=0.79]warmup:  30%|███       | 456/1500 [00:03&lt;00:04, 236.03it/s, 7 steps of size 8.93e-01. acc. prob=0.79] warmup:  32%|███▏      | 480/1500 [00:04&lt;00:05, 200.56it/s, 3 steps of size 1.34e-01. acc. prob=0.78]sample:  33%|███▎      | 502/1500 [00:04&lt;00:05, 185.93it/s, 15 steps of size 4.49e-01. acc. prob=0.92]sample:  35%|███▍      | 522/1500 [00:04&lt;00:05, 180.99it/s, 7 steps of size 4.49e-01. acc. prob=0.92] sample:  36%|███▌      | 541/1500 [00:04&lt;00:05, 178.51it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  37%|███▋      | 560/1500 [00:04&lt;00:05, 180.69it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  39%|███▊      | 579/1500 [00:04&lt;00:05, 173.98it/s, 15 steps of size 4.49e-01. acc. prob=0.93]sample:  40%|████      | 600/1500 [00:04&lt;00:04, 183.22it/s, 7 steps of size 4.49e-01. acc. prob=0.92] sample:  41%|████▏     | 621/1500 [00:04&lt;00:04, 188.26it/s, 15 steps of size 4.49e-01. acc. prob=0.93]sample:  43%|████▎     | 641/1500 [00:04&lt;00:04, 186.61it/s, 7 steps of size 4.49e-01. acc. prob=0.93] sample:  44%|████▍     | 660/1500 [00:05&lt;00:04, 184.74it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  45%|████▌     | 680/1500 [00:05&lt;00:04, 187.50it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  47%|████▋     | 699/1500 [00:05&lt;00:04, 178.34it/s, 15 steps of size 4.49e-01. acc. prob=0.93]sample:  48%|████▊     | 717/1500 [00:05&lt;00:04, 166.72it/s, 15 steps of size 4.49e-01. acc. prob=0.93]sample:  49%|████▉     | 735/1500 [00:05&lt;00:04, 169.78it/s, 7 steps of size 4.49e-01. acc. prob=0.93] sample:  51%|█████     | 759/1500 [00:05&lt;00:03, 185.99it/s, 15 steps of size 4.49e-01. acc. prob=0.93]sample:  52%|█████▏    | 778/1500 [00:05&lt;00:03, 183.09it/s, 7 steps of size 4.49e-01. acc. prob=0.93] sample:  53%|█████▎    | 798/1500 [00:05&lt;00:03, 187.41it/s, 23 steps of size 4.49e-01. acc. prob=0.93]sample:  55%|█████▍    | 818/1500 [00:05&lt;00:03, 190.13it/s, 7 steps of size 4.49e-01. acc. prob=0.93] sample:  56%|█████▌    | 839/1500 [00:05&lt;00:03, 194.41it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  57%|█████▋    | 859/1500 [00:06&lt;00:03, 192.02it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  59%|█████▊    | 879/1500 [00:06&lt;00:03, 187.84it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  60%|█████▉    | 898/1500 [00:06&lt;00:03, 188.16it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  61%|██████▏   | 921/1500 [00:06&lt;00:02, 198.23it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  63%|██████▎   | 941/1500 [00:06&lt;00:02, 194.67it/s, 15 steps of size 4.49e-01. acc. prob=0.93]sample:  64%|██████▍   | 961/1500 [00:06&lt;00:02, 185.44it/s, 15 steps of size 4.49e-01. acc. prob=0.93]sample:  65%|██████▌   | 980/1500 [00:06&lt;00:02, 178.38it/s, 7 steps of size 4.49e-01. acc. prob=0.93] sample:  67%|██████▋   | 998/1500 [00:06&lt;00:02, 176.08it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  68%|██████▊   | 1017/1500 [00:06&lt;00:02, 179.40it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  69%|██████▉   | 1039/1500 [00:07&lt;00:02, 190.85it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  71%|███████   | 1059/1500 [00:07&lt;00:02, 191.49it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  72%|███████▏  | 1079/1500 [00:07&lt;00:02, 184.03it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  73%|███████▎  | 1098/1500 [00:07&lt;00:02, 178.60it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  74%|███████▍  | 1116/1500 [00:07&lt;00:02, 176.44it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  76%|███████▌  | 1136/1500 [00:07&lt;00:02, 181.56it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  77%|███████▋  | 1155/1500 [00:07&lt;00:01, 178.95it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  78%|███████▊  | 1176/1500 [00:07&lt;00:01, 186.58it/s, 15 steps of size 4.49e-01. acc. prob=0.93]sample:  80%|███████▉  | 1195/1500 [00:07&lt;00:01, 186.98it/s, 7 steps of size 4.49e-01. acc. prob=0.93] sample:  81%|████████  | 1217/1500 [00:08&lt;00:01, 196.18it/s, 7 steps of size 4.49e-01. acc. prob=0.92]sample:  82%|████████▏ | 1237/1500 [00:08&lt;00:01, 194.26it/s, 7 steps of size 4.49e-01. acc. prob=0.92]sample:  84%|████████▍ | 1257/1500 [00:08&lt;00:01, 187.78it/s, 15 steps of size 4.49e-01. acc. prob=0.93]sample:  85%|████████▌ | 1276/1500 [00:08&lt;00:01, 187.67it/s, 7 steps of size 4.49e-01. acc. prob=0.93] sample:  86%|████████▋ | 1297/1500 [00:08&lt;00:01, 192.88it/s, 15 steps of size 4.49e-01. acc. prob=0.92]sample:  88%|████████▊ | 1317/1500 [00:08&lt;00:01, 180.43it/s, 7 steps of size 4.49e-01. acc. prob=0.92] sample:  89%|████████▉ | 1336/1500 [00:08&lt;00:00, 179.50it/s, 7 steps of size 4.49e-01. acc. prob=0.92]sample:  90%|█████████ | 1355/1500 [00:08&lt;00:00, 179.82it/s, 15 steps of size 4.49e-01. acc. prob=0.92]sample:  92%|█████████▏| 1374/1500 [00:08&lt;00:00, 173.84it/s, 7 steps of size 4.49e-01. acc. prob=0.92] sample:  93%|█████████▎| 1393/1500 [00:09&lt;00:00, 178.14it/s, 15 steps of size 4.49e-01. acc. prob=0.92]sample:  94%|█████████▍| 1412/1500 [00:09&lt;00:00, 181.19it/s, 15 steps of size 4.49e-01. acc. prob=0.92]sample:  95%|█████████▌| 1431/1500 [00:09&lt;00:00, 175.93it/s, 15 steps of size 4.49e-01. acc. prob=0.92]sample:  97%|█████████▋| 1449/1500 [00:09&lt;00:00, 176.53it/s, 7 steps of size 4.49e-01. acc. prob=0.93] sample:  98%|█████████▊| 1467/1500 [00:09&lt;00:00, 177.15it/s, 7 steps of size 4.49e-01. acc. prob=0.93]sample:  99%|█████████▉| 1485/1500 [00:09&lt;00:00, 171.66it/s, 15 steps of size 4.49e-01. acc. prob=0.93]sample: 100%|██████████| 1500/1500 [00:09&lt;00:00, 155.94it/s, 7 steps of size 4.49e-01. acc. prob=0.93] \n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   1%|          | 11/1500 [00:00&lt;00:20, 74.24it/s, 127 steps of size 2.92e-02. acc. prob=0.63]warmup:   1%|▏         | 19/1500 [00:00&lt;00:22, 65.96it/s, 3 steps of size 1.45e-02. acc. prob=0.69]  warmup:   2%|▏         | 26/1500 [00:00&lt;00:27, 53.56it/s, 15 steps of size 8.78e-02. acc. prob=0.74]warmup:   2%|▏         | 33/1500 [00:00&lt;00:25, 57.37it/s, 31 steps of size 8.12e-02. acc. prob=0.75]warmup:   3%|▎         | 43/1500 [00:00&lt;00:21, 69.11it/s, 31 steps of size 4.69e-02. acc. prob=0.75]warmup:   4%|▎         | 53/1500 [00:00&lt;00:19, 72.69it/s, 63 steps of size 3.54e-02. acc. prob=0.75]warmup:   4%|▍         | 62/1500 [00:00&lt;00:18, 76.46it/s, 15 steps of size 1.02e-01. acc. prob=0.77]warmup:   5%|▍         | 70/1500 [00:00&lt;00:18, 77.31it/s, 31 steps of size 6.13e-02. acc. prob=0.77]warmup:   5%|▌         | 82/1500 [00:01&lt;00:16, 87.08it/s, 31 steps of size 4.61e-02. acc. prob=0.77]warmup:   6%|▌         | 91/1500 [00:01&lt;00:16, 83.58it/s, 15 steps of size 8.46e-02. acc. prob=0.77]warmup:   7%|▋         | 101/1500 [00:01&lt;00:17, 82.10it/s, 63 steps of size 9.17e-01. acc. prob=0.77]warmup:   7%|▋         | 112/1500 [00:01&lt;00:15, 89.33it/s, 23 steps of size 2.13e-01. acc. prob=0.77]warmup:   8%|▊         | 122/1500 [00:01&lt;00:15, 90.63it/s, 7 steps of size 6.02e-01. acc. prob=0.78] warmup:   9%|▉         | 132/1500 [00:01&lt;00:15, 85.63it/s, 31 steps of size 4.51e-01. acc. prob=0.78]warmup:  10%|▉         | 148/1500 [00:01&lt;00:12, 105.60it/s, 7 steps of size 6.24e-01. acc. prob=0.78]warmup:  11%|█         | 162/1500 [00:01&lt;00:11, 113.32it/s, 15 steps of size 5.81e-01. acc. prob=0.78]warmup:  12%|█▏        | 179/1500 [00:01&lt;00:10, 128.27it/s, 15 steps of size 2.44e-01. acc. prob=0.78]warmup:  13%|█▎        | 193/1500 [00:02&lt;00:10, 128.09it/s, 7 steps of size 3.66e-01. acc. prob=0.78] warmup:  14%|█▍        | 208/1500 [00:02&lt;00:09, 131.61it/s, 15 steps of size 4.58e-01. acc. prob=0.78]warmup:  15%|█▌        | 227/1500 [00:02&lt;00:08, 146.67it/s, 15 steps of size 6.59e-01. acc. prob=0.78]warmup:  16%|█▋        | 247/1500 [00:02&lt;00:07, 161.90it/s, 3 steps of size 4.93e-01. acc. prob=0.78] warmup:  18%|█▊        | 266/1500 [00:02&lt;00:07, 166.99it/s, 15 steps of size 3.21e-01. acc. prob=0.78]warmup:  19%|█▉        | 286/1500 [00:02&lt;00:06, 173.67it/s, 15 steps of size 2.08e-01. acc. prob=0.78]warmup:  21%|██        | 308/1500 [00:02&lt;00:06, 186.03it/s, 7 steps of size 1.10e+00. acc. prob=0.78] warmup:  22%|██▏       | 327/1500 [00:02&lt;00:06, 181.78it/s, 7 steps of size 8.53e-01. acc. prob=0.78]warmup:  23%|██▎       | 350/1500 [00:02&lt;00:05, 193.74it/s, 7 steps of size 8.36e-01. acc. prob=0.79]warmup:  25%|██▍       | 374/1500 [00:03&lt;00:05, 205.15it/s, 7 steps of size 7.28e-01. acc. prob=0.79]warmup:  27%|██▋       | 399/1500 [00:03&lt;00:05, 217.57it/s, 7 steps of size 5.73e-01. acc. prob=0.79]warmup:  28%|██▊       | 423/1500 [00:03&lt;00:04, 222.46it/s, 7 steps of size 4.12e-01. acc. prob=0.79]warmup:  30%|██▉       | 446/1500 [00:03&lt;00:04, 221.95it/s, 7 steps of size 5.23e-01. acc. prob=0.79]warmup:  31%|███▏      | 469/1500 [00:03&lt;00:04, 214.04it/s, 15 steps of size 4.25e-01. acc. prob=0.79]warmup:  33%|███▎      | 491/1500 [00:03&lt;00:05, 196.03it/s, 3 steps of size 1.66e-01. acc. prob=0.79] sample:  34%|███▍      | 511/1500 [00:03&lt;00:05, 188.94it/s, 15 steps of size 4.74e-01. acc. prob=0.92]sample:  35%|███▌      | 532/1500 [00:03&lt;00:05, 192.39it/s, 15 steps of size 4.74e-01. acc. prob=0.92]sample:  37%|███▋      | 553/1500 [00:03&lt;00:04, 196.28it/s, 7 steps of size 4.74e-01. acc. prob=0.93] sample:  38%|███▊      | 576/1500 [00:04&lt;00:04, 203.84it/s, 7 steps of size 4.74e-01. acc. prob=0.93]sample:  40%|████      | 600/1500 [00:04&lt;00:04, 211.88it/s, 7 steps of size 4.74e-01. acc. prob=0.93]sample:  42%|████▏     | 624/1500 [00:04&lt;00:04, 218.12it/s, 15 steps of size 4.74e-01. acc. prob=0.93]sample:  43%|████▎     | 647/1500 [00:04&lt;00:03, 219.51it/s, 7 steps of size 4.74e-01. acc. prob=0.93] sample:  45%|████▍     | 670/1500 [00:04&lt;00:03, 219.79it/s, 7 steps of size 4.74e-01. acc. prob=0.93]sample:  46%|████▌     | 693/1500 [00:04&lt;00:03, 221.80it/s, 7 steps of size 4.74e-01. acc. prob=0.93]sample:  48%|████▊     | 716/1500 [00:04&lt;00:03, 220.12it/s, 7 steps of size 4.74e-01. acc. prob=0.93]sample:  49%|████▉     | 739/1500 [00:04&lt;00:03, 213.60it/s, 7 steps of size 4.74e-01. acc. prob=0.93]sample:  51%|█████     | 761/1500 [00:04&lt;00:03, 209.45it/s, 15 steps of size 4.74e-01. acc. prob=0.93]sample:  52%|█████▏    | 782/1500 [00:04&lt;00:03, 201.14it/s, 7 steps of size 4.74e-01. acc. prob=0.93] sample:  54%|█████▎    | 804/1500 [00:05&lt;00:03, 204.12it/s, 15 steps of size 4.74e-01. acc. prob=0.93]sample:  55%|█████▌    | 828/1500 [00:05&lt;00:03, 211.99it/s, 7 steps of size 4.74e-01. acc. prob=0.93] sample:  57%|█████▋    | 852/1500 [00:05&lt;00:02, 217.82it/s, 7 steps of size 4.74e-01. acc. prob=0.93]sample:  58%|█████▊    | 876/1500 [00:05&lt;00:02, 221.70it/s, 7 steps of size 4.74e-01. acc. prob=0.93]sample:  60%|█████▉    | 899/1500 [00:05&lt;00:02, 223.42it/s, 15 steps of size 4.74e-01. acc. prob=0.93]sample:  61%|██████▏   | 922/1500 [00:05&lt;00:02, 218.34it/s, 7 steps of size 4.74e-01. acc. prob=0.92] sample:  63%|██████▎   | 944/1500 [00:05&lt;00:02, 214.38it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  64%|██████▍   | 966/1500 [00:05&lt;00:02, 215.85it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  66%|██████▌   | 988/1500 [00:05&lt;00:02, 214.17it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  67%|██████▋   | 1011/1500 [00:06&lt;00:02, 217.78it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  69%|██████▉   | 1033/1500 [00:06&lt;00:02, 216.94it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  70%|███████   | 1055/1500 [00:06&lt;00:02, 206.96it/s, 15 steps of size 4.74e-01. acc. prob=0.92]sample:  72%|███████▏  | 1077/1500 [00:06&lt;00:02, 210.63it/s, 7 steps of size 4.74e-01. acc. prob=0.92] sample:  73%|███████▎  | 1099/1500 [00:06&lt;00:01, 211.74it/s, 3 steps of size 4.74e-01. acc. prob=0.92]sample:  75%|███████▍  | 1121/1500 [00:06&lt;00:01, 208.65it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  76%|███████▌  | 1143/1500 [00:06&lt;00:01, 209.33it/s, 15 steps of size 4.74e-01. acc. prob=0.92]sample:  78%|███████▊  | 1164/1500 [00:06&lt;00:01, 206.85it/s, 7 steps of size 4.74e-01. acc. prob=0.92] sample:  79%|███████▉  | 1188/1500 [00:06&lt;00:01, 211.99it/s, 15 steps of size 4.74e-01. acc. prob=0.92]sample:  81%|████████  | 1210/1500 [00:06&lt;00:01, 208.97it/s, 7 steps of size 4.74e-01. acc. prob=0.92] sample:  82%|████████▏ | 1234/1500 [00:07&lt;00:01, 216.80it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  84%|████████▎ | 1256/1500 [00:07&lt;00:01, 217.53it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  85%|████████▌ | 1278/1500 [00:07&lt;00:01, 212.70it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  87%|████████▋ | 1302/1500 [00:07&lt;00:00, 218.30it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  88%|████████▊ | 1325/1500 [00:07&lt;00:00, 216.19it/s, 15 steps of size 4.74e-01. acc. prob=0.92]sample:  90%|████████▉ | 1347/1500 [00:07&lt;00:00, 214.33it/s, 7 steps of size 4.74e-01. acc. prob=0.92] sample:  91%|█████████▏| 1369/1500 [00:07&lt;00:00, 210.48it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  93%|█████████▎| 1391/1500 [00:07&lt;00:00, 210.81it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  94%|█████████▍| 1413/1500 [00:07&lt;00:00, 202.81it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  96%|█████████▌| 1437/1500 [00:08&lt;00:00, 212.35it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  97%|█████████▋| 1461/1500 [00:08&lt;00:00, 219.00it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample:  99%|█████████▉| 1484/1500 [00:08&lt;00:00, 220.82it/s, 7 steps of size 4.74e-01. acc. prob=0.92]sample: 100%|██████████| 1500/1500 [00:08&lt;00:00, 180.14it/s, 7 steps of size 4.74e-01. acc. prob=0.92]\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   1%|          | 15/1500 [00:00&lt;00:09, 149.19it/s, 7 steps of size 1.11e-02. acc. prob=0.65]warmup:   2%|▏         | 30/1500 [00:00&lt;00:23, 61.90it/s, 5 steps of size 5.31e-02. acc. prob=0.74] warmup:   3%|▎         | 42/1500 [00:00&lt;00:19, 75.48it/s, 31 steps of size 9.62e-02. acc. prob=0.76]warmup:   3%|▎         | 52/1500 [00:00&lt;00:18, 78.43it/s, 11 steps of size 3.08e-02. acc. prob=0.75]warmup:   4%|▍         | 62/1500 [00:00&lt;00:17, 80.89it/s, 7 steps of size 7.52e-02. acc. prob=0.76] warmup:   5%|▌         | 77/1500 [00:00&lt;00:14, 98.88it/s, 7 steps of size 7.29e-02. acc. prob=0.77]warmup:   6%|▌         | 89/1500 [00:01&lt;00:14, 97.18it/s, 23 steps of size 6.60e-02. acc. prob=0.77]warmup:   7%|▋         | 100/1500 [00:01&lt;00:14, 97.94it/s, 31 steps of size 1.37e-01. acc. prob=0.78]warmup:   7%|▋         | 112/1500 [00:01&lt;00:13, 103.18it/s, 7 steps of size 4.96e-01. acc. prob=0.78]warmup:   8%|▊         | 125/1500 [00:01&lt;00:13, 101.89it/s, 63 steps of size 1.69e-01. acc. prob=0.78]warmup:   9%|▉         | 138/1500 [00:01&lt;00:13, 103.43it/s, 63 steps of size 1.90e-01. acc. prob=0.78]warmup:  11%|█         | 159/1500 [00:01&lt;00:10, 126.94it/s, 31 steps of size 2.08e-01. acc. prob=0.78]warmup:  12%|█▏        | 173/1500 [00:01&lt;00:10, 130.14it/s, 7 steps of size 1.95e-01. acc. prob=0.78] warmup:  12%|█▏        | 187/1500 [00:01&lt;00:09, 132.64it/s, 7 steps of size 1.06e+00. acc. prob=0.78]warmup:  13%|█▎        | 202/1500 [00:01&lt;00:09, 135.69it/s, 15 steps of size 3.76e-01. acc. prob=0.78]warmup:  15%|█▍        | 223/1500 [00:01&lt;00:08, 155.99it/s, 7 steps of size 7.28e-01. acc. prob=0.78] warmup:  16%|█▋        | 246/1500 [00:02&lt;00:07, 176.50it/s, 7 steps of size 9.87e-01. acc. prob=0.79]warmup:  18%|█▊        | 267/1500 [00:02&lt;00:06, 183.38it/s, 31 steps of size 2.41e-01. acc. prob=0.78]warmup:  19%|█▉        | 286/1500 [00:02&lt;00:06, 175.59it/s, 7 steps of size 5.30e-01. acc. prob=0.78] warmup:  21%|██        | 310/1500 [00:02&lt;00:06, 192.40it/s, 7 steps of size 7.05e-01. acc. prob=0.78]warmup:  22%|██▏       | 333/1500 [00:02&lt;00:05, 201.08it/s, 7 steps of size 6.48e-01. acc. prob=0.78]warmup:  24%|██▎       | 355/1500 [00:02&lt;00:05, 205.49it/s, 7 steps of size 8.48e-01. acc. prob=0.79]warmup:  25%|██▌       | 382/1500 [00:02&lt;00:05, 223.00it/s, 7 steps of size 4.41e-01. acc. prob=0.79]warmup:  27%|██▋       | 406/1500 [00:02&lt;00:04, 227.05it/s, 7 steps of size 3.97e-01. acc. prob=0.79]warmup:  29%|██▉       | 432/1500 [00:02&lt;00:04, 234.69it/s, 7 steps of size 3.75e-01. acc. prob=0.79]warmup:  30%|███       | 456/1500 [00:03&lt;00:04, 229.06it/s, 31 steps of size 4.29e-01. acc. prob=0.79]warmup:  32%|███▏      | 479/1500 [00:03&lt;00:05, 201.76it/s, 3 steps of size 1.21e-01. acc. prob=0.79] warmup:  33%|███▎      | 500/1500 [00:03&lt;00:05, 191.42it/s, 7 steps of size 4.77e-01. acc. prob=0.79]sample:  35%|███▍      | 521/1500 [00:03&lt;00:05, 195.38it/s, 15 steps of size 4.77e-01. acc. prob=0.93]sample:  36%|███▋      | 544/1500 [00:03&lt;00:04, 202.56it/s, 7 steps of size 4.77e-01. acc. prob=0.93] sample:  38%|███▊      | 569/1500 [00:03&lt;00:04, 211.58it/s, 15 steps of size 4.77e-01. acc. prob=0.92]sample:  39%|███▉      | 592/1500 [00:03&lt;00:04, 214.78it/s, 7 steps of size 4.77e-01. acc. prob=0.93] sample:  41%|████      | 618/1500 [00:03&lt;00:03, 226.22it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  43%|████▎     | 641/1500 [00:03&lt;00:03, 216.31it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  44%|████▍     | 667/1500 [00:04&lt;00:03, 227.23it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  46%|████▌     | 691/1500 [00:04&lt;00:03, 228.46it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  48%|████▊     | 714/1500 [00:04&lt;00:03, 210.32it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  49%|████▉     | 737/1500 [00:04&lt;00:03, 213.22it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  51%|█████     | 759/1500 [00:04&lt;00:03, 213.30it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  52%|█████▏    | 781/1500 [00:04&lt;00:03, 212.97it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  54%|█████▎    | 805/1500 [00:04&lt;00:03, 219.43it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  55%|█████▌    | 829/1500 [00:04&lt;00:02, 224.36it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  57%|█████▋    | 852/1500 [00:04&lt;00:03, 215.30it/s, 15 steps of size 4.77e-01. acc. prob=0.91]sample:  58%|█████▊    | 877/1500 [00:05&lt;00:02, 223.02it/s, 7 steps of size 4.77e-01. acc. prob=0.91] sample:  60%|██████    | 900/1500 [00:05&lt;00:02, 216.94it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  61%|██████▏   | 922/1500 [00:05&lt;00:02, 211.38it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  63%|██████▎   | 945/1500 [00:05&lt;00:02, 215.63it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  64%|██████▍   | 967/1500 [00:05&lt;00:02, 209.11it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  66%|██████▌   | 988/1500 [00:05&lt;00:02, 197.03it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  67%|██████▋   | 1010/1500 [00:05&lt;00:02, 202.01it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  69%|██████▉   | 1035/1500 [00:05&lt;00:02, 213.34it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  71%|███████   | 1058/1500 [00:05&lt;00:02, 215.59it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  72%|███████▏  | 1080/1500 [00:06&lt;00:02, 198.76it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  73%|███████▎  | 1102/1500 [00:06&lt;00:01, 204.50it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  75%|███████▌  | 1125/1500 [00:06&lt;00:01, 209.27it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  77%|███████▋  | 1149/1500 [00:06&lt;00:01, 217.09it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  78%|███████▊  | 1171/1500 [00:06&lt;00:01, 213.31it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  80%|███████▉  | 1193/1500 [00:06&lt;00:01, 210.78it/s, 7 steps of size 4.77e-01. acc. prob=0.91]sample:  81%|████████  | 1215/1500 [00:06&lt;00:01, 213.41it/s, 15 steps of size 4.77e-01. acc. prob=0.91]sample:  82%|████████▏ | 1237/1500 [00:06&lt;00:01, 213.85it/s, 15 steps of size 4.77e-01. acc. prob=0.91]sample:  84%|████████▍ | 1260/1500 [00:06&lt;00:01, 216.05it/s, 7 steps of size 4.77e-01. acc. prob=0.92] sample:  86%|████████▌ | 1283/1500 [00:06&lt;00:00, 220.06it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  87%|████████▋ | 1307/1500 [00:07&lt;00:00, 223.71it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  89%|████████▉ | 1332/1500 [00:07&lt;00:00, 229.03it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  90%|█████████ | 1355/1500 [00:07&lt;00:00, 224.95it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  92%|█████████▏| 1378/1500 [00:07&lt;00:00, 221.03it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  93%|█████████▎| 1401/1500 [00:07&lt;00:00, 220.84it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  95%|█████████▌| 1425/1500 [00:07&lt;00:00, 224.03it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  97%|█████████▋| 1449/1500 [00:07&lt;00:00, 227.71it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample:  98%|█████████▊| 1472/1500 [00:07&lt;00:00, 227.12it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample: 100%|█████████▉| 1497/1500 [00:07&lt;00:00, 233.22it/s, 7 steps of size 4.77e-01. acc. prob=0.92]sample: 100%|██████████| 1500/1500 [00:07&lt;00:00, 189.74it/s, 7 steps of size 4.77e-01. acc. prob=0.92]\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   1%|          | 11/1500 [00:00&lt;00:17, 87.57it/s, 127 steps of size 2.81e-02. acc. prob=0.63]warmup:   1%|▏         | 20/1500 [00:00&lt;00:29, 50.37it/s, 3 steps of size 1.59e-02. acc. prob=0.69]  warmup:   2%|▏         | 26/1500 [00:00&lt;00:31, 47.08it/s, 3 steps of size 1.76e-02. acc. prob=0.71]warmup:   2%|▏         | 32/1500 [00:00&lt;00:30, 47.59it/s, 63 steps of size 3.05e-02. acc. prob=0.73]warmup:   3%|▎         | 39/1500 [00:00&lt;00:27, 53.64it/s, 31 steps of size 9.05e-02. acc. prob=0.75]warmup:   3%|▎         | 50/1500 [00:00&lt;00:21, 68.44it/s, 15 steps of size 5.68e-02. acc. prob=0.76]warmup:   4%|▍         | 60/1500 [00:00&lt;00:19, 75.62it/s, 15 steps of size 1.33e-01. acc. prob=0.77]warmup:   5%|▍         | 69/1500 [00:01&lt;00:18, 78.95it/s, 7 steps of size 1.36e-01. acc. prob=0.77] warmup:   5%|▌         | 78/1500 [00:01&lt;00:17, 81.78it/s, 63 steps of size 4.41e-02. acc. prob=0.77]warmup:   6%|▌         | 88/1500 [00:01&lt;00:16, 85.88it/s, 31 steps of size 8.00e-02. acc. prob=0.77]warmup:   7%|▋         | 99/1500 [00:01&lt;00:15, 91.51it/s, 15 steps of size 1.24e-01. acc. prob=0.78]warmup:   7%|▋         | 109/1500 [00:01&lt;00:16, 85.70it/s, 3 steps of size 8.96e-01. acc. prob=0.78]warmup:   8%|▊         | 122/1500 [00:01&lt;00:14, 94.42it/s, 31 steps of size 2.79e-01. acc. prob=0.77]warmup:   9%|▉         | 139/1500 [00:01&lt;00:12, 111.85it/s, 31 steps of size 3.08e-01. acc. prob=0.78]warmup:  10%|█         | 152/1500 [00:01&lt;00:11, 116.69it/s, 1 steps of size 8.43e-01. acc. prob=0.78] warmup:  11%|█▏        | 172/1500 [00:01&lt;00:09, 136.41it/s, 31 steps of size 2.09e-01. acc. prob=0.78]warmup:  12%|█▏        | 186/1500 [00:02&lt;00:09, 135.26it/s, 7 steps of size 2.39e-01. acc. prob=0.78] warmup:  13%|█▎        | 202/1500 [00:02&lt;00:09, 141.41it/s, 15 steps of size 2.70e-01. acc. prob=0.78]warmup:  15%|█▍        | 220/1500 [00:02&lt;00:08, 149.86it/s, 15 steps of size 4.50e-01. acc. prob=0.78]warmup:  16%|█▌        | 239/1500 [00:02&lt;00:07, 160.57it/s, 7 steps of size 4.56e-01. acc. prob=0.78] warmup:  17%|█▋        | 259/1500 [00:02&lt;00:07, 171.97it/s, 15 steps of size 8.63e-01. acc. prob=0.78]warmup:  19%|█▊        | 279/1500 [00:02&lt;00:07, 173.92it/s, 31 steps of size 2.00e-01. acc. prob=0.78]warmup:  20%|██        | 300/1500 [00:02&lt;00:06, 181.39it/s, 15 steps of size 5.44e-01. acc. prob=0.78]warmup:  21%|██▏       | 319/1500 [00:02&lt;00:06, 175.22it/s, 15 steps of size 4.61e-01. acc. prob=0.78]warmup:  23%|██▎       | 338/1500 [00:02&lt;00:06, 177.37it/s, 7 steps of size 4.04e-01. acc. prob=0.78] warmup:  24%|██▍       | 362/1500 [00:02&lt;00:05, 194.38it/s, 15 steps of size 3.62e-01. acc. prob=0.78]warmup:  26%|██▌       | 386/1500 [00:03&lt;00:05, 207.24it/s, 7 steps of size 4.48e-01. acc. prob=0.79] warmup:  27%|██▋       | 410/1500 [00:03&lt;00:05, 215.82it/s, 7 steps of size 6.50e-01. acc. prob=0.79]warmup:  29%|██▉       | 432/1500 [00:03&lt;00:04, 216.90it/s, 15 steps of size 5.39e-01. acc. prob=0.79]warmup:  30%|███       | 455/1500 [00:03&lt;00:04, 216.06it/s, 31 steps of size 2.96e-01. acc. prob=0.79]warmup:  32%|███▏      | 477/1500 [00:03&lt;00:05, 196.43it/s, 15 steps of size 3.89e-01. acc. prob=0.79]warmup:  33%|███▎      | 498/1500 [00:03&lt;00:05, 183.11it/s, 31 steps of size 2.38e-01. acc. prob=0.79]sample:  34%|███▍      | 517/1500 [00:03&lt;00:05, 184.47it/s, 7 steps of size 4.29e-01. acc. prob=0.96] sample:  36%|███▌      | 536/1500 [00:03&lt;00:05, 181.04it/s, 7 steps of size 4.29e-01. acc. prob=0.94]sample:  37%|███▋      | 560/1500 [00:03&lt;00:04, 195.40it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  39%|███▊      | 580/1500 [00:04&lt;00:04, 196.36it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  40%|████      | 602/1500 [00:04&lt;00:04, 201.93it/s, 15 steps of size 4.29e-01. acc. prob=0.93]sample:  42%|████▏     | 623/1500 [00:04&lt;00:04, 196.69it/s, 15 steps of size 4.29e-01. acc. prob=0.93]sample:  43%|████▎     | 643/1500 [00:04&lt;00:04, 194.11it/s, 7 steps of size 4.29e-01. acc. prob=0.93] sample:  44%|████▍     | 663/1500 [00:04&lt;00:04, 191.82it/s, 7 steps of size 4.29e-01. acc. prob=0.92]sample:  46%|████▌     | 683/1500 [00:04&lt;00:04, 186.98it/s, 15 steps of size 4.29e-01. acc. prob=0.92]sample:  47%|████▋     | 703/1500 [00:04&lt;00:04, 189.12it/s, 7 steps of size 4.29e-01. acc. prob=0.93] sample:  48%|████▊     | 724/1500 [00:04&lt;00:04, 193.10it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  50%|████▉     | 744/1500 [00:04&lt;00:03, 191.29it/s, 15 steps of size 4.29e-01. acc. prob=0.92]sample:  51%|█████     | 764/1500 [00:05&lt;00:03, 189.67it/s, 7 steps of size 4.29e-01. acc. prob=0.93] sample:  52%|█████▏    | 783/1500 [00:05&lt;00:03, 186.65it/s, 7 steps of size 4.29e-01. acc. prob=0.92]sample:  54%|█████▎    | 803/1500 [00:05&lt;00:03, 190.18it/s, 7 steps of size 4.29e-01. acc. prob=0.92]sample:  55%|█████▍    | 823/1500 [00:05&lt;00:03, 182.23it/s, 3 steps of size 4.29e-01. acc. prob=0.92]sample:  56%|█████▌    | 843/1500 [00:05&lt;00:03, 186.87it/s, 7 steps of size 4.29e-01. acc. prob=0.92]sample:  57%|█████▋    | 862/1500 [00:05&lt;00:03, 187.32it/s, 7 steps of size 4.29e-01. acc. prob=0.92]sample:  59%|█████▉    | 882/1500 [00:05&lt;00:03, 188.34it/s, 15 steps of size 4.29e-01. acc. prob=0.92]sample:  60%|██████    | 904/1500 [00:05&lt;00:03, 195.51it/s, 7 steps of size 4.29e-01. acc. prob=0.92] sample:  62%|██████▏   | 924/1500 [00:05&lt;00:03, 190.98it/s, 7 steps of size 4.29e-01. acc. prob=0.92]sample:  63%|██████▎   | 944/1500 [00:06&lt;00:02, 188.56it/s, 15 steps of size 4.29e-01. acc. prob=0.92]sample:  64%|██████▍   | 964/1500 [00:06&lt;00:02, 191.65it/s, 7 steps of size 4.29e-01. acc. prob=0.93] sample:  66%|██████▌   | 988/1500 [00:06&lt;00:02, 201.32it/s, 15 steps of size 4.29e-01. acc. prob=0.93]sample:  67%|██████▋   | 1009/1500 [00:06&lt;00:02, 187.97it/s, 23 steps of size 4.29e-01. acc. prob=0.93]sample:  69%|██████▊   | 1029/1500 [00:06&lt;00:02, 191.06it/s, 7 steps of size 4.29e-01. acc. prob=0.92] sample:  70%|███████   | 1050/1500 [00:06&lt;00:02, 196.40it/s, 7 steps of size 4.29e-01. acc. prob=0.92]sample:  71%|███████▏  | 1070/1500 [00:06&lt;00:02, 194.75it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  73%|███████▎  | 1090/1500 [00:06&lt;00:02, 193.50it/s, 15 steps of size 4.29e-01. acc. prob=0.93]sample:  74%|███████▍  | 1110/1500 [00:06&lt;00:02, 178.94it/s, 7 steps of size 4.29e-01. acc. prob=0.93] sample:  75%|███████▌  | 1129/1500 [00:06&lt;00:02, 179.50it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  77%|███████▋  | 1148/1500 [00:07&lt;00:02, 173.19it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  78%|███████▊  | 1167/1500 [00:07&lt;00:01, 177.34it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  79%|███████▉  | 1187/1500 [00:07&lt;00:01, 181.21it/s, 15 steps of size 4.29e-01. acc. prob=0.92]sample:  81%|████████  | 1209/1500 [00:07&lt;00:01, 190.09it/s, 15 steps of size 4.29e-01. acc. prob=0.92]sample:  82%|████████▏ | 1232/1500 [00:07&lt;00:01, 199.37it/s, 7 steps of size 4.29e-01. acc. prob=0.92] sample:  84%|████████▎ | 1254/1500 [00:07&lt;00:01, 203.26it/s, 7 steps of size 4.29e-01. acc. prob=0.92]sample:  85%|████████▌ | 1275/1500 [00:07&lt;00:01, 202.76it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  86%|████████▋ | 1296/1500 [00:07&lt;00:01, 201.93it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  88%|████████▊ | 1317/1500 [00:07&lt;00:00, 197.90it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  89%|████████▉ | 1337/1500 [00:08&lt;00:00, 193.20it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  91%|█████████ | 1358/1500 [00:08&lt;00:00, 197.59it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  92%|█████████▏| 1378/1500 [00:08&lt;00:00, 190.06it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  93%|█████████▎| 1402/1500 [00:08&lt;00:00, 203.46it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  95%|█████████▍| 1424/1500 [00:08&lt;00:00, 205.21it/s, 15 steps of size 4.29e-01. acc. prob=0.93]sample:  96%|█████████▋| 1445/1500 [00:08&lt;00:00, 203.69it/s, 7 steps of size 4.29e-01. acc. prob=0.93] sample:  98%|█████████▊| 1466/1500 [00:08&lt;00:00, 203.39it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample:  99%|█████████▉| 1487/1500 [00:08&lt;00:00, 198.29it/s, 7 steps of size 4.29e-01. acc. prob=0.93]sample: 100%|██████████| 1500/1500 [00:08&lt;00:00, 169.34it/s, 7 steps of size 4.29e-01. acc. prob=0.93]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n         p      0.50      0.04      0.50      0.44      0.56   2474.72      1.00\n       phi      0.68      0.03      0.68      0.64      0.73   3390.42      1.00\n     pi[0]      0.33      0.04      0.33      0.26      0.39   3629.21      1.00\n     pi[1]      0.16      0.04      0.16      0.10      0.23   3565.42      1.00\n     pi[2]      0.13      0.03      0.13      0.07      0.18   3334.39      1.00\n     pi[3]      0.11      0.03      0.11      0.06      0.16   3405.33      1.00\n     pi[4]      0.10      0.03      0.10      0.06      0.15   3405.02      1.00\n     pi[5]      0.10      0.03      0.10      0.05      0.15   4048.82      1.00\n     pi[6]      0.07      0.03      0.07      0.03      0.11   4412.08      1.00\n       psi      0.40      0.02      0.40      0.36      0.44   3840.84      1.00\n\nNumber of divergences: 0\n\n\n\n\n\n\n# generate the posterior predictive distribution for N\nsamples = mcmc.get_samples()\nz = sample_z(js_prior2, samples, capture_histories)\never_alive = z.max(axis=1) &gt; 0\nsamples['N'] = ever_alive.sum(axis=1)\n\n# create the plot\nfig, ax = plt.subplots(figsize=(5, 4), sharey=True)\n\nax.hist(samples['N'], bins=20, fc='tab:cyan', ec='w', alpha=0.99)\n\nax.set_title(r'Posterior distribution of $N$')\nax.axvline(SUPERPOPULATION_SIZE, linestyle='--', color='tab:pink', label='Truth')\nax.axvline(np.median(samples['N']), linestyle='--', color='tab:purple', label='Median')\n\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\nFigure 4: Posterior distribution of \\(N\\) with priors on \\(\\pi\\) and \\(\\psi\\)\n\n\n\n\n\nThe resulting estimate of \\(N\\) is much closer to the true superpopulation size than the original version with a uniform prior on \\(\\gamma.\\) One potential downside of this approach is that it requires some familiarity with the relationships between the Schwarz-Arnason parameterizations and the restricted occupancy parameterizations of the Jolly-Seber model.",
    "crumbs": [
      "Code",
      "NumPyro",
      "Jolly-Seber"
    ]
  },
  {
    "objectID": "numpyro-js.html#beta-prior",
    "href": "numpyro-js.html#beta-prior",
    "title": "Jolly-Seber",
    "section": "Beta prior",
    "text": "Beta prior\nAn easier alternative is to approximate the above prior. As such, we try to approximate the situation where half of all animal in \\(M\\) are recruited, and each of these is equally likely to be recruited during \\(t\\). So when \\(T=7\\), we hope that \\(1/14\\) of all individuals are allocated to each time step. To do so, we set a Beta prior, \\(\\gamma_t \\sim \\mathrm{Beta}(a_t, b_t),\\) where \\(a_t = S/(14-(t-1))\\) and \\(b_t = S - a_t.\\) Royle and Dorazio (2008) recommend setting \\(S=1.5\\).\n\ndef js_prior3(capture_history):\n\n    super_size, occasion_count = capture_history.shape\n\n    phi = numpyro.sample('phi', dist.Uniform(0, 1))\n    p = numpyro.sample('p', dist.Uniform(0, 1))\n\n    # approximate the situation where half of all individuals are allocated\n    # to each time step\n    S = 1.5\n    t = np.arange(1, occasion_count + 1)\n    C = occasion_count * 2\n    a_t = S / (C - (t - 1))\n    b_t = S - a_t\n    with numpyro.plate('occasions', occasion_count):\n        gamma = numpyro.sample('gamma', dist.Beta(a_t, b_t))\n\n    def transition_and_capture(carry, y_current):\n\n        z_previous, t = carry\n\n        trans_probs = jnp.array([\n            [1 - gamma[t], gamma[t], 0.0],  # From not yet entered\n            [0.0, phi, 1 - phi],            # From alive\n            [0.0, 0.0, 1.0]                 # From dead\n        ])\n\n        with numpyro.plate(\"animals\", super_size, dim=-1):\n\n            mu_z_current = trans_probs[z_previous]\n            z_current = numpyro.sample(\n                \"state\",\n                dist.Categorical(dist.util.clamp_probs(mu_z_current)),\n                infer={\"enumerate\": \"parallel\"}\n            )\n\n            mu_y_current = jnp.where(z_current == 1, p, 0.0)\n            numpyro.sample(\n                \"obs\",\n                dist.Bernoulli(dist.util.clamp_probs(mu_y_current)),\n                obs=y_current\n            )\n\n        return (z_current, t + 1), None\n\n    # start everyone in the not yet entered state\n    state_init = jnp.zeros(super_size, dtype=jnp.int32)\n    scan(\n        transition_and_capture,\n        (state_init, 0),\n         jnp.swapaxes(capture_history, 0, 1)\n    )\n\n\n# specify which sampler you want to use\nnuts_kernel = NUTS(js_prior3)\n\n# configure the MCMC run\nmcmc = MCMC(nuts_kernel, num_warmup=WARMUP_COUNT, num_samples=SAMPLE_COUNT,\n            num_chains=CHAIN_COUNT)\n\n# run the MCMC then inspect the output\nmcmc.run(rng_key, capture_histories)\nmcmc.print_summary()\n\n/var/folders/y8/cz021w550rbb072f7qhxyylh0000gq/T/ipykernel_16237/3629401454.py:5: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  mcmc = MCMC(nuts_kernel, num_warmup=WARMUP_COUNT, num_samples=SAMPLE_COUNT,\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1500 [00:00&lt;16:45,  1.49it/s, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   1%|          | 13/1500 [00:00&lt;01:07, 22.17it/s, 7 steps of size 9.86e-03. acc. prob=0.63]warmup:   1%|▏         | 20/1500 [00:00&lt;00:58, 25.41it/s, 63 steps of size 3.40e-02. acc. prob=0.70]warmup:   2%|▏         | 29/1500 [00:01&lt;00:39, 36.99it/s, 15 steps of size 1.04e-01. acc. prob=0.74]warmup:   2%|▏         | 36/1500 [00:01&lt;00:34, 42.80it/s, 63 steps of size 6.29e-02. acc. prob=0.75]warmup:   3%|▎         | 47/1500 [00:01&lt;00:25, 57.55it/s, 31 steps of size 4.41e-02. acc. prob=0.75]warmup:   4%|▎         | 56/1500 [00:01&lt;00:22, 63.67it/s, 31 steps of size 7.46e-02. acc. prob=0.76]warmup:   5%|▍         | 70/1500 [00:01&lt;00:18, 78.47it/s, 47 steps of size 5.66e-02. acc. prob=0.76]warmup:   6%|▌         | 86/1500 [00:01&lt;00:14, 98.25it/s, 7 steps of size 4.04e-02. acc. prob=0.77] warmup:   7%|▋         | 98/1500 [00:01&lt;00:13, 100.16it/s, 15 steps of size 1.09e-01. acc. prob=0.77]warmup:   7%|▋         | 112/1500 [00:01&lt;00:12, 109.86it/s, 7 steps of size 4.14e-01. acc. prob=0.77]warmup:   8%|▊         | 126/1500 [00:01&lt;00:11, 116.35it/s, 15 steps of size 7.32e-01. acc. prob=0.78]warmup:  10%|▉         | 145/1500 [00:02&lt;00:10, 134.67it/s, 15 steps of size 5.57e-01. acc. prob=0.78]warmup:  11%|█         | 162/1500 [00:02&lt;00:09, 142.87it/s, 31 steps of size 3.35e-01. acc. prob=0.78]warmup:  12%|█▏        | 177/1500 [00:02&lt;00:09, 141.95it/s, 15 steps of size 4.42e-01. acc. prob=0.78]warmup:  13%|█▎        | 192/1500 [00:02&lt;00:09, 144.05it/s, 15 steps of size 4.35e-01. acc. prob=0.78]warmup:  14%|█▍        | 213/1500 [00:02&lt;00:07, 162.68it/s, 7 steps of size 5.66e-01. acc. prob=0.78] warmup:  15%|█▌        | 230/1500 [00:02&lt;00:08, 157.57it/s, 7 steps of size 9.50e-01. acc. prob=0.78]warmup:  17%|█▋        | 248/1500 [00:02&lt;00:07, 162.69it/s, 7 steps of size 2.54e-01. acc. prob=0.78]warmup:  18%|█▊        | 265/1500 [00:03&lt;00:11, 105.36it/s, 63 steps of size 7.86e-02. acc. prob=0.78]warmup:  19%|█▊        | 279/1500 [00:03&lt;00:15, 80.35it/s, 31 steps of size 1.84e-01. acc. prob=0.78] warmup:  19%|█▉        | 290/1500 [00:03&lt;00:17, 67.39it/s, 31 steps of size 1.88e-01. acc. prob=0.78]warmup:  20%|█▉        | 299/1500 [00:03&lt;00:20, 57.43it/s, 14 steps of size 1.35e-01. acc. prob=0.78]warmup:  20%|██        | 307/1500 [00:03&lt;00:21, 56.51it/s, 127 steps of size 5.00e-02. acc. prob=0.78]warmup:  21%|██        | 314/1500 [00:04&lt;00:22, 53.25it/s, 15 steps of size 2.04e-01. acc. prob=0.78] warmup:  21%|██▏       | 320/1500 [00:04&lt;00:23, 49.92it/s, 31 steps of size 8.14e-02. acc. prob=0.78]warmup:  22%|██▏       | 326/1500 [00:04&lt;00:23, 50.30it/s, 31 steps of size 1.84e-01. acc. prob=0.78]warmup:  22%|██▏       | 332/1500 [00:04&lt;00:23, 50.44it/s, 31 steps of size 1.30e-01. acc. prob=0.78]warmup:  23%|██▎       | 338/1500 [00:04&lt;00:22, 50.64it/s, 29 steps of size 6.21e-02. acc. prob=0.78]warmup:  23%|██▎       | 344/1500 [00:04&lt;00:22, 51.21it/s, 31 steps of size 9.47e-02. acc. prob=0.78]warmup:  23%|██▎       | 352/1500 [00:04&lt;00:20, 55.83it/s, 31 steps of size 1.03e-01. acc. prob=0.78]warmup:  24%|██▍       | 358/1500 [00:05&lt;00:25, 44.60it/s, 31 steps of size 1.56e-01. acc. prob=0.78]warmup:  24%|██▍       | 363/1500 [00:05&lt;00:25, 44.48it/s, 31 steps of size 1.32e-01. acc. prob=0.78]warmup:  25%|██▍       | 372/1500 [00:05&lt;00:21, 51.37it/s, 63 steps of size 1.03e-01. acc. prob=0.78]warmup:  25%|██▌       | 379/1500 [00:05&lt;00:20, 53.78it/s, 31 steps of size 1.31e-01. acc. prob=0.78]warmup:  26%|██▌       | 385/1500 [00:05&lt;00:20, 55.02it/s, 31 steps of size 9.69e-02. acc. prob=0.78]warmup:  26%|██▌       | 393/1500 [00:05&lt;00:19, 57.91it/s, 63 steps of size 1.04e-01. acc. prob=0.78]warmup:  27%|██▋       | 399/1500 [00:05&lt;00:20, 54.30it/s, 31 steps of size 1.50e-01. acc. prob=0.78]warmup:  27%|██▋       | 405/1500 [00:05&lt;00:20, 53.13it/s, 31 steps of size 1.96e-01. acc. prob=0.78]warmup:  27%|██▋       | 411/1500 [00:06&lt;00:21, 51.80it/s, 11 steps of size 1.53e-01. acc. prob=0.78]warmup:  28%|██▊       | 417/1500 [00:06&lt;00:21, 50.82it/s, 31 steps of size 1.59e-01. acc. prob=0.78]warmup:  28%|██▊       | 423/1500 [00:06&lt;00:21, 51.26it/s, 31 steps of size 1.52e-01. acc. prob=0.78]warmup:  29%|██▊       | 430/1500 [00:06&lt;00:20, 51.72it/s, 63 steps of size 9.50e-02. acc. prob=0.78]warmup:  29%|██▉       | 436/1500 [00:06&lt;00:22, 47.34it/s, 63 steps of size 1.21e-01. acc. prob=0.78]warmup:  29%|██▉       | 441/1500 [00:06&lt;00:22, 46.35it/s, 63 steps of size 9.49e-02. acc. prob=0.78]warmup:  30%|██▉       | 447/1500 [00:06&lt;00:21, 48.26it/s, 31 steps of size 1.20e-01. acc. prob=0.78]warmup:  30%|███       | 456/1500 [00:06&lt;00:18, 57.55it/s, 63 steps of size 1.13e-01. acc. prob=0.78]warmup:  31%|███       | 462/1500 [00:06&lt;00:18, 56.98it/s, 63 steps of size 1.71e-01. acc. prob=0.78]warmup:  32%|███▏      | 477/1500 [00:07&lt;00:12, 80.45it/s, 23 steps of size 3.79e-01. acc. prob=0.78]warmup:  33%|███▎      | 489/1500 [00:07&lt;00:11, 90.36it/s, 9 steps of size 1.05e-01. acc. prob=0.78] warmup:  33%|███▎      | 499/1500 [00:07&lt;00:10, 91.39it/s, 8 steps of size 8.38e-02. acc. prob=0.78]sample:  34%|███▍      | 509/1500 [00:07&lt;00:11, 86.76it/s, 15 steps of size 2.64e-01. acc. prob=0.98]sample:  35%|███▍      | 521/1500 [00:07&lt;00:10, 93.97it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  35%|███▌      | 532/1500 [00:07&lt;00:10, 96.68it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  36%|███▋      | 544/1500 [00:07&lt;00:09, 102.19it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  37%|███▋      | 555/1500 [00:07&lt;00:10, 85.94it/s, 31 steps of size 2.64e-01. acc. prob=0.96] sample:  38%|███▊      | 565/1500 [00:08&lt;00:11, 83.23it/s, 31 steps of size 2.64e-01. acc. prob=0.96]sample:  38%|███▊      | 574/1500 [00:08&lt;00:10, 84.58it/s, 7 steps of size 2.64e-01. acc. prob=0.96] sample:  39%|███▉      | 584/1500 [00:08&lt;00:10, 87.24it/s, 31 steps of size 2.64e-01. acc. prob=0.96]sample:  40%|███▉      | 597/1500 [00:08&lt;00:09, 97.67it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  41%|████      | 609/1500 [00:08&lt;00:08, 103.80it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  41%|████▏     | 621/1500 [00:08&lt;00:08, 107.13it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  42%|████▏     | 634/1500 [00:08&lt;00:07, 112.37it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  43%|████▎     | 648/1500 [00:08&lt;00:07, 118.67it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  44%|████▍     | 660/1500 [00:08&lt;00:07, 117.46it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  45%|████▍     | 672/1500 [00:09&lt;00:07, 112.25it/s, 31 steps of size 2.64e-01. acc. prob=0.96]sample:  46%|████▌     | 685/1500 [00:09&lt;00:07, 115.87it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  46%|████▋     | 697/1500 [00:09&lt;00:07, 114.36it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  47%|████▋     | 709/1500 [00:09&lt;00:07, 108.87it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  48%|████▊     | 720/1500 [00:09&lt;00:07, 109.16it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  49%|████▊     | 731/1500 [00:09&lt;00:07, 108.01it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  50%|████▉     | 744/1500 [00:09&lt;00:06, 112.16it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  50%|█████     | 757/1500 [00:09&lt;00:06, 116.73it/s, 7 steps of size 2.64e-01. acc. prob=0.96] sample:  51%|█████▏    | 769/1500 [00:09&lt;00:06, 115.76it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  52%|█████▏    | 782/1500 [00:09&lt;00:06, 115.13it/s, 31 steps of size 2.64e-01. acc. prob=0.96]sample:  53%|█████▎    | 794/1500 [00:10&lt;00:06, 113.35it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  54%|█████▎    | 806/1500 [00:10&lt;00:06, 115.12it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  55%|█████▍    | 819/1500 [00:10&lt;00:05, 117.88it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  55%|█████▌    | 831/1500 [00:10&lt;00:05, 111.85it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  56%|█████▌    | 843/1500 [00:10&lt;00:06, 107.25it/s, 8 steps of size 2.64e-01. acc. prob=0.96] sample:  57%|█████▋    | 856/1500 [00:10&lt;00:05, 111.36it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  58%|█████▊    | 868/1500 [00:10&lt;00:05, 113.56it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  59%|█████▉    | 883/1500 [00:10&lt;00:05, 121.27it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  60%|█████▉    | 896/1500 [00:10&lt;00:05, 117.91it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  61%|██████    | 908/1500 [00:11&lt;00:05, 117.13it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  61%|██████▏   | 921/1500 [00:11&lt;00:04, 119.58it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  62%|██████▏   | 933/1500 [00:11&lt;00:04, 118.29it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  63%|██████▎   | 945/1500 [00:11&lt;00:04, 111.48it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  64%|██████▍   | 958/1500 [00:11&lt;00:04, 115.48it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  65%|██████▍   | 971/1500 [00:11&lt;00:04, 116.80it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  66%|██████▌   | 984/1500 [00:11&lt;00:04, 119.28it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  66%|██████▋   | 996/1500 [00:11&lt;00:04, 118.44it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  67%|██████▋   | 1009/1500 [00:11&lt;00:04, 120.62it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  68%|██████▊   | 1022/1500 [00:12&lt;00:03, 122.18it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  69%|██████▉   | 1035/1500 [00:12&lt;00:03, 123.42it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  70%|██████▉   | 1048/1500 [00:12&lt;00:03, 124.08it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  71%|███████   | 1061/1500 [00:12&lt;00:03, 121.38it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  72%|███████▏  | 1074/1500 [00:12&lt;00:03, 119.43it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  72%|███████▏  | 1087/1500 [00:12&lt;00:03, 119.65it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  73%|███████▎  | 1100/1500 [00:12&lt;00:03, 121.30it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  74%|███████▍  | 1113/1500 [00:12&lt;00:03, 116.65it/s, 7 steps of size 2.64e-01. acc. prob=0.96] sample:  75%|███████▌  | 1127/1500 [00:12&lt;00:03, 120.87it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  76%|███████▌  | 1140/1500 [00:13&lt;00:03, 116.26it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  77%|███████▋  | 1152/1500 [00:13&lt;00:03, 113.77it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  78%|███████▊  | 1165/1500 [00:13&lt;00:02, 117.15it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  78%|███████▊  | 1177/1500 [00:13&lt;00:02, 116.55it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  79%|███████▉  | 1189/1500 [00:13&lt;00:02, 114.83it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  80%|████████  | 1203/1500 [00:13&lt;00:02, 119.28it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  81%|████████  | 1215/1500 [00:13&lt;00:02, 115.24it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  82%|████████▏ | 1227/1500 [00:13&lt;00:02, 115.32it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  83%|████████▎ | 1240/1500 [00:13&lt;00:02, 117.91it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  83%|████████▎ | 1252/1500 [00:13&lt;00:02, 116.81it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  84%|████████▍ | 1264/1500 [00:14&lt;00:02, 113.64it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  85%|████████▌ | 1278/1500 [00:14&lt;00:01, 118.48it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  86%|████████▌ | 1291/1500 [00:14&lt;00:01, 120.44it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  87%|████████▋ | 1304/1500 [00:14&lt;00:01, 120.49it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  88%|████████▊ | 1317/1500 [00:14&lt;00:01, 118.69it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  89%|████████▊ | 1330/1500 [00:14&lt;00:01, 120.58it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  90%|████████▉ | 1343/1500 [00:14&lt;00:01, 120.30it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  90%|█████████ | 1356/1500 [00:14&lt;00:01, 117.18it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  91%|█████████▏| 1369/1500 [00:14&lt;00:01, 118.17it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  92%|█████████▏| 1382/1500 [00:15&lt;00:00, 118.91it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  93%|█████████▎| 1394/1500 [00:15&lt;00:00, 117.85it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  94%|█████████▎| 1406/1500 [00:15&lt;00:00, 112.93it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  95%|█████████▍| 1418/1500 [00:15&lt;00:00, 111.19it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  95%|█████████▌| 1430/1500 [00:15&lt;00:00, 112.69it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  96%|█████████▌| 1442/1500 [00:15&lt;00:00, 105.58it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  97%|█████████▋| 1454/1500 [00:15&lt;00:00, 107.21it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  98%|█████████▊| 1466/1500 [00:15&lt;00:00, 109.53it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample:  99%|█████████▊| 1479/1500 [00:15&lt;00:00, 114.20it/s, 7 steps of size 2.64e-01. acc. prob=0.96] sample:  99%|█████████▉| 1491/1500 [00:16&lt;00:00, 113.35it/s, 15 steps of size 2.64e-01. acc. prob=0.96]sample: 100%|██████████| 1500/1500 [00:16&lt;00:00, 92.91it/s, 15 steps of size 2.64e-01. acc. prob=0.96] \n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   0%|          | 7/1500 [00:00&lt;00:23, 62.60it/s, 31 steps of size 6.82e-02. acc. prob=0.57]warmup:   1%|          | 14/1500 [00:00&lt;00:33, 43.82it/s, 15 steps of size 1.79e-01. acc. prob=0.71]warmup:   1%|▏         | 19/1500 [00:00&lt;00:44, 33.37it/s, 7 steps of size 1.40e-01. acc. prob=0.73] warmup:   2%|▏         | 28/1500 [00:00&lt;00:30, 48.16it/s, 15 steps of size 2.00e-01. acc. prob=0.75]warmup:   2%|▏         | 37/1500 [00:00&lt;00:24, 59.03it/s, 7 steps of size 2.48e-02. acc. prob=0.74] warmup:   3%|▎         | 47/1500 [00:00&lt;00:20, 69.72it/s, 15 steps of size 6.99e-02. acc. prob=0.76]warmup:   4%|▍         | 59/1500 [00:00&lt;00:18, 79.96it/s, 31 steps of size 1.22e-01. acc. prob=0.77]warmup:   5%|▍         | 70/1500 [00:01&lt;00:16, 86.65it/s, 15 steps of size 1.34e-01. acc. prob=0.77]warmup:   6%|▌         | 85/1500 [00:01&lt;00:13, 102.33it/s, 15 steps of size 7.91e-02. acc. prob=0.77]warmup:   7%|▋         | 98/1500 [00:01&lt;00:13, 106.16it/s, 31 steps of size 8.77e-02. acc. prob=0.77]warmup:   7%|▋         | 109/1500 [00:01&lt;00:17, 80.02it/s, 15 steps of size 3.89e-02. acc. prob=0.77]warmup:   8%|▊         | 119/1500 [00:01&lt;00:23, 57.80it/s, 31 steps of size 2.39e-01. acc. prob=0.77]warmup:   8%|▊         | 127/1500 [00:01&lt;00:25, 53.15it/s, 31 steps of size 2.24e-01. acc. prob=0.78]warmup:   9%|▉         | 134/1500 [00:02&lt;00:25, 53.36it/s, 7 steps of size 4.05e-02. acc. prob=0.77] warmup:   9%|▉         | 141/1500 [00:02&lt;00:28, 48.00it/s, 15 steps of size 2.19e-01. acc. prob=0.78]warmup:  10%|▉         | 148/1500 [00:02&lt;00:26, 50.69it/s, 31 steps of size 2.18e-01. acc. prob=0.78]warmup:  11%|█         | 160/1500 [00:02&lt;00:20, 64.86it/s, 7 steps of size 4.85e-01. acc. prob=0.78] warmup:  11%|█▏        | 172/1500 [00:02&lt;00:17, 77.00it/s, 7 steps of size 4.31e-01. acc. prob=0.78]warmup:  12%|█▏        | 184/1500 [00:02&lt;00:15, 87.54it/s, 7 steps of size 8.99e-01. acc. prob=0.78]warmup:  14%|█▎        | 205/1500 [00:02&lt;00:11, 117.47it/s, 15 steps of size 4.86e-01. acc. prob=0.78]warmup:  15%|█▍        | 221/1500 [00:02&lt;00:10, 127.73it/s, 7 steps of size 4.76e-01. acc. prob=0.78] warmup:  16%|█▌        | 235/1500 [00:03&lt;00:09, 127.20it/s, 15 steps of size 6.90e-01. acc. prob=0.78]warmup:  17%|█▋        | 256/1500 [00:03&lt;00:08, 147.35it/s, 15 steps of size 3.74e-01. acc. prob=0.78]warmup:  18%|█▊        | 272/1500 [00:03&lt;00:12, 96.45it/s, 15 steps of size 2.83e-01. acc. prob=0.78] warmup:  19%|█▉        | 285/1500 [00:03&lt;00:12, 99.68it/s, 2 steps of size 1.44e-01. acc. prob=0.78] warmup:  20%|█▉        | 297/1500 [00:03&lt;00:12, 96.16it/s, 15 steps of size 1.73e-01. acc. prob=0.78]warmup:  21%|██        | 308/1500 [00:03&lt;00:12, 97.12it/s, 15 steps of size 1.74e-01. acc. prob=0.78]warmup:  21%|██▏       | 319/1500 [00:03&lt;00:13, 85.29it/s, 31 steps of size 1.49e-01. acc. prob=0.78]warmup:  22%|██▏       | 329/1500 [00:04&lt;00:13, 88.41it/s, 31 steps of size 1.68e-01. acc. prob=0.78]warmup:  23%|██▎       | 339/1500 [00:04&lt;00:12, 91.09it/s, 15 steps of size 1.86e-01. acc. prob=0.78]warmup:  23%|██▎       | 351/1500 [00:04&lt;00:12, 95.22it/s, 31 steps of size 2.06e-01. acc. prob=0.78]warmup:  24%|██▍       | 365/1500 [00:04&lt;00:10, 106.94it/s, 7 steps of size 3.64e-01. acc. prob=0.78]warmup:  25%|██▌       | 378/1500 [00:04&lt;00:10, 108.15it/s, 31 steps of size 1.73e-01. acc. prob=0.78]warmup:  26%|██▌       | 390/1500 [00:04&lt;00:10, 110.70it/s, 15 steps of size 1.76e-01. acc. prob=0.78]warmup:  27%|██▋       | 405/1500 [00:04&lt;00:09, 119.29it/s, 15 steps of size 2.19e-01. acc. prob=0.78]warmup:  28%|██▊       | 418/1500 [00:04&lt;00:09, 116.35it/s, 15 steps of size 3.77e-01. acc. prob=0.79]warmup:  29%|██▊       | 431/1500 [00:04&lt;00:09, 118.02it/s, 15 steps of size 2.90e-01. acc. prob=0.79]warmup:  30%|██▉       | 444/1500 [00:05&lt;00:08, 117.90it/s, 31 steps of size 2.47e-01. acc. prob=0.79]warmup:  31%|███       | 460/1500 [00:05&lt;00:08, 128.08it/s, 31 steps of size 1.97e-01. acc. prob=0.78]warmup:  32%|███▏      | 473/1500 [00:05&lt;00:08, 114.91it/s, 31 steps of size 2.16e-01. acc. prob=0.78]warmup:  32%|███▏      | 485/1500 [00:05&lt;00:09, 104.15it/s, 15 steps of size 6.33e-01. acc. prob=0.79]warmup:  33%|███▎      | 498/1500 [00:05&lt;00:09, 109.69it/s, 15 steps of size 2.64e-01. acc. prob=0.79]sample:  34%|███▍      | 512/1500 [00:05&lt;00:08, 116.51it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  35%|███▌      | 527/1500 [00:05&lt;00:07, 123.04it/s, 15 steps of size 3.09e-01. acc. prob=0.93]sample:  36%|███▌      | 541/1500 [00:05&lt;00:07, 124.91it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  37%|███▋      | 554/1500 [00:05&lt;00:07, 126.21it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  38%|███▊      | 567/1500 [00:06&lt;00:07, 121.02it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  39%|███▊      | 581/1500 [00:06&lt;00:07, 124.89it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  40%|███▉      | 594/1500 [00:06&lt;00:07, 124.87it/s, 7 steps of size 3.09e-01. acc. prob=0.94]sample:  41%|████      | 610/1500 [00:06&lt;00:06, 132.38it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  42%|████▏     | 626/1500 [00:06&lt;00:06, 139.18it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  43%|████▎     | 641/1500 [00:06&lt;00:06, 141.13it/s, 7 steps of size 3.09e-01. acc. prob=0.94]sample:  44%|████▎     | 656/1500 [00:06&lt;00:06, 139.10it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  45%|████▍     | 670/1500 [00:06&lt;00:06, 138.08it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  46%|████▌     | 684/1500 [00:06&lt;00:06, 135.21it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  47%|████▋     | 699/1500 [00:07&lt;00:05, 136.90it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  48%|████▊     | 714/1500 [00:07&lt;00:05, 139.67it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  49%|████▊     | 728/1500 [00:07&lt;00:05, 138.61it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  50%|████▉     | 746/1500 [00:07&lt;00:05, 147.85it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  51%|█████     | 761/1500 [00:07&lt;00:05, 140.08it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  52%|█████▏    | 776/1500 [00:07&lt;00:05, 136.65it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  53%|█████▎    | 791/1500 [00:07&lt;00:05, 137.69it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  54%|█████▍    | 807/1500 [00:07&lt;00:04, 142.95it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  55%|█████▍    | 822/1500 [00:07&lt;00:04, 143.81it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  56%|█████▌    | 837/1500 [00:08&lt;00:04, 140.95it/s, 7 steps of size 3.09e-01. acc. prob=0.94]sample:  57%|█████▋    | 852/1500 [00:08&lt;00:04, 139.52it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  58%|█████▊    | 868/1500 [00:08&lt;00:04, 143.60it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  59%|█████▉    | 885/1500 [00:08&lt;00:04, 148.45it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  60%|██████    | 900/1500 [00:08&lt;00:04, 144.18it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  61%|██████    | 915/1500 [00:08&lt;00:04, 139.36it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  62%|██████▏   | 929/1500 [00:08&lt;00:04, 136.71it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  63%|██████▎   | 947/1500 [00:08&lt;00:03, 145.60it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  64%|██████▍   | 962/1500 [00:08&lt;00:03, 138.69it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  65%|██████▌   | 976/1500 [00:09&lt;00:03, 136.18it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  66%|██████▌   | 991/1500 [00:09&lt;00:03, 138.87it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  67%|██████▋   | 1005/1500 [00:09&lt;00:03, 135.21it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  68%|██████▊   | 1020/1500 [00:09&lt;00:03, 136.26it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  69%|██████▉   | 1037/1500 [00:09&lt;00:03, 143.10it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  70%|███████   | 1052/1500 [00:09&lt;00:03, 143.79it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  71%|███████   | 1068/1500 [00:09&lt;00:02, 144.95it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  72%|███████▏  | 1083/1500 [00:09&lt;00:03, 134.68it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  73%|███████▎  | 1098/1500 [00:09&lt;00:02, 136.34it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  74%|███████▍  | 1112/1500 [00:09&lt;00:02, 136.04it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  75%|███████▌  | 1126/1500 [00:10&lt;00:02, 136.11it/s, 7 steps of size 3.09e-01. acc. prob=0.94]sample:  76%|███████▌  | 1142/1500 [00:10&lt;00:02, 142.00it/s, 7 steps of size 3.09e-01. acc. prob=0.94]sample:  77%|███████▋  | 1157/1500 [00:10&lt;00:02, 143.22it/s, 23 steps of size 3.09e-01. acc. prob=0.94]sample:  78%|███████▊  | 1172/1500 [00:10&lt;00:02, 138.87it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  79%|███████▉  | 1186/1500 [00:10&lt;00:02, 133.08it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  80%|████████  | 1202/1500 [00:10&lt;00:02, 139.50it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  81%|████████  | 1217/1500 [00:10&lt;00:02, 136.39it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  82%|████████▏ | 1233/1500 [00:10&lt;00:01, 140.40it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  83%|████████▎ | 1248/1500 [00:10&lt;00:01, 137.04it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  84%|████████▍ | 1264/1500 [00:11&lt;00:01, 142.67it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  85%|████████▌ | 1279/1500 [00:11&lt;00:01, 143.29it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  86%|████████▋ | 1294/1500 [00:11&lt;00:01, 142.17it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  87%|████████▋ | 1309/1500 [00:11&lt;00:01, 138.19it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  88%|████████▊ | 1326/1500 [00:11&lt;00:01, 144.34it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  89%|████████▉ | 1341/1500 [00:11&lt;00:01, 142.93it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  90%|█████████ | 1356/1500 [00:11&lt;00:01, 139.16it/s, 7 steps of size 3.09e-01. acc. prob=0.94]sample:  91%|█████████▏| 1372/1500 [00:11&lt;00:00, 143.89it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  92%|█████████▏| 1387/1500 [00:11&lt;00:00, 143.27it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  94%|█████████▎| 1405/1500 [00:12&lt;00:00, 151.14it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  95%|█████████▍| 1421/1500 [00:12&lt;00:00, 141.31it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  96%|█████████▌| 1436/1500 [00:12&lt;00:00, 135.58it/s, 7 steps of size 3.09e-01. acc. prob=0.94] sample:  97%|█████████▋| 1450/1500 [00:12&lt;00:00, 130.35it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  98%|█████████▊| 1465/1500 [00:12&lt;00:00, 132.63it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample:  99%|█████████▊| 1479/1500 [00:12&lt;00:00, 133.10it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample: 100%|█████████▉| 1495/1500 [00:12&lt;00:00, 140.55it/s, 15 steps of size 3.09e-01. acc. prob=0.94]sample: 100%|██████████| 1500/1500 [00:12&lt;00:00, 117.43it/s, 15 steps of size 3.09e-01. acc. prob=0.94]\n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   0%|          | 7/1500 [00:00&lt;00:31, 47.22it/s, 95 steps of size 6.47e-02. acc. prob=0.57]warmup:   1%|          | 13/1500 [00:00&lt;00:32, 45.58it/s, 63 steps of size 4.48e-02. acc. prob=0.67]warmup:   1%|▏         | 22/1500 [00:00&lt;00:24, 60.42it/s, 15 steps of size 7.77e-02. acc. prob=0.72]warmup:   2%|▏         | 29/1500 [00:00&lt;00:25, 57.82it/s, 47 steps of size 6.34e-02. acc. prob=0.74]warmup:   2%|▏         | 37/1500 [00:00&lt;00:23, 62.75it/s, 47 steps of size 8.02e-02. acc. prob=0.75]warmup:   3%|▎         | 52/1500 [00:00&lt;00:18, 78.43it/s, 95 steps of size 9.74e-02. acc. prob=0.76]warmup:   4%|▍         | 60/1500 [00:01&lt;00:30, 46.88it/s, 15 steps of size 1.47e-01. acc. prob=0.77]warmup:   5%|▍         | 71/1500 [00:01&lt;00:26, 53.63it/s, 111 steps of size 8.92e-02. acc. prob=0.77]warmup:   5%|▌         | 80/1500 [00:01&lt;00:26, 54.26it/s, 127 steps of size 1.16e-01. acc. prob=0.77]warmup:   6%|▌         | 87/1500 [00:01&lt;00:29, 47.78it/s, 255 steps of size 5.11e-02. acc. prob=0.77]warmup:   6%|▋         | 96/1500 [00:01&lt;00:26, 52.92it/s, 55 steps of size 1.84e-01. acc. prob=0.78] warmup:   7%|▋         | 104/1500 [00:01&lt;00:24, 58.10it/s, 15 steps of size 4.28e-01. acc. prob=0.77]warmup:   8%|▊         | 113/1500 [00:01&lt;00:21, 64.83it/s, 63 steps of size 1.11e-01. acc. prob=0.77]warmup:   8%|▊         | 121/1500 [00:02&lt;00:21, 64.69it/s, 15 steps of size 3.48e-01. acc. prob=0.78]warmup:   9%|▊         | 128/1500 [00:02&lt;00:21, 64.88it/s, 63 steps of size 1.12e-01. acc. prob=0.77]warmup:   9%|▉         | 135/1500 [00:02&lt;00:22, 61.32it/s, 31 steps of size 1.66e-01. acc. prob=0.78]warmup:  10%|▉         | 143/1500 [00:02&lt;00:20, 64.91it/s, 31 steps of size 1.44e-01. acc. prob=0.78]warmup:  10%|█         | 150/1500 [00:02&lt;00:23, 57.64it/s, 15 steps of size 2.53e-01. acc. prob=0.78]warmup:  10%|█         | 157/1500 [00:03&lt;00:39, 33.94it/s, 31 steps of size 6.55e-02. acc. prob=0.77]warmup:  11%|█         | 162/1500 [00:03&lt;00:47, 28.20it/s, 127 steps of size 5.33e-02. acc. prob=0.77]warmup:  11%|█         | 166/1500 [00:03&lt;00:52, 25.33it/s, 255 steps of size 2.94e-02. acc. prob=0.77]warmup:  11%|█▏        | 170/1500 [00:03&lt;00:49, 27.01it/s, 63 steps of size 7.03e-02. acc. prob=0.77] warmup:  12%|█▏        | 174/1500 [00:04&lt;01:31, 14.51it/s, 127 steps of size 4.41e-02. acc. prob=0.77]warmup:  12%|█▏        | 178/1500 [00:04&lt;01:18, 16.94it/s, 127 steps of size 5.21e-02. acc. prob=0.77]warmup:  12%|█▏        | 181/1500 [00:04&lt;01:23, 15.75it/s, 319 steps of size 2.49e-02. acc. prob=0.77]warmup:  12%|█▏        | 184/1500 [00:04&lt;01:18, 16.78it/s, 23 steps of size 2.36e-02. acc. prob=0.77] warmup:  12%|█▏        | 187/1500 [00:04&lt;01:18, 16.70it/s, 63 steps of size 2.29e-02. acc. prob=0.77]warmup:  13%|█▎        | 190/1500 [00:05&lt;01:27, 14.97it/s, 63 steps of size 7.28e-03. acc. prob=0.77]warmup:  13%|█▎        | 192/1500 [00:05&lt;02:08, 10.20it/s, 255 steps of size 2.11e-02. acc. prob=0.77]warmup:  13%|█▎        | 194/1500 [00:05&lt;02:06, 10.35it/s, 63 steps of size 5.26e-02. acc. prob=0.78] warmup:  13%|█▎        | 198/1500 [00:05&lt;01:30, 14.41it/s, 63 steps of size 6.38e-02. acc. prob=0.78]warmup:  13%|█▎        | 202/1500 [00:06&lt;01:10, 18.50it/s, 35 steps of size 1.49e-02. acc. prob=0.77]warmup:  14%|█▎        | 205/1500 [00:06&lt;01:21, 15.96it/s, 63 steps of size 6.43e-02. acc. prob=0.78]warmup:  14%|█▍        | 208/1500 [00:06&lt;01:11, 18.09it/s, 63 steps of size 6.37e-02. acc. prob=0.78]warmup:  14%|█▍        | 211/1500 [00:06&lt;01:08, 18.75it/s, 63 steps of size 6.81e-02. acc. prob=0.78]warmup:  14%|█▍        | 214/1500 [00:06&lt;01:26, 14.91it/s, 255 steps of size 3.86e-02. acc. prob=0.78]warmup:  15%|█▍        | 218/1500 [00:07&lt;01:12, 17.70it/s, 127 steps of size 3.84e-02. acc. prob=0.78]warmup:  15%|█▍        | 221/1500 [00:07&lt;01:07, 18.86it/s, 31 steps of size 7.33e-02. acc. prob=0.78] warmup:  15%|█▍        | 224/1500 [00:07&lt;01:02, 20.55it/s, 63 steps of size 5.93e-02. acc. prob=0.78]warmup:  15%|█▌        | 229/1500 [00:07&lt;00:47, 26.61it/s, 15 steps of size 3.69e-02. acc. prob=0.78]warmup:  16%|█▌        | 233/1500 [00:07&lt;00:47, 26.66it/s, 127 steps of size 2.64e-02. acc. prob=0.78]warmup:  16%|█▌        | 236/1500 [00:07&lt;00:54, 22.99it/s, 127 steps of size 7.13e-02. acc. prob=0.78]warmup:  16%|█▌        | 241/1500 [00:07&lt;00:45, 27.94it/s, 31 steps of size 8.01e-02. acc. prob=0.78] warmup:  16%|█▋        | 245/1500 [00:08&lt;00:49, 25.20it/s, 63 steps of size 6.73e-02. acc. prob=0.78]warmup:  17%|█▋        | 248/1500 [00:08&lt;00:51, 24.53it/s, 63 steps of size 7.92e-02. acc. prob=0.78]warmup:  17%|█▋        | 251/1500 [00:08&lt;00:50, 24.61it/s, 95 steps of size 6.99e-01. acc. prob=0.78]warmup:  18%|█▊        | 265/1500 [00:08&lt;00:24, 51.36it/s, 15 steps of size 1.98e-01. acc. prob=0.78]warmup:  19%|█▊        | 281/1500 [00:08&lt;00:15, 77.39it/s, 15 steps of size 8.05e-01. acc. prob=0.78]warmup:  20%|██        | 302/1500 [00:08&lt;00:10, 109.47it/s, 15 steps of size 7.96e-01. acc. prob=0.78]warmup:  21%|██▏       | 319/1500 [00:08&lt;00:09, 124.10it/s, 15 steps of size 5.79e-01. acc. prob=0.78]warmup:  23%|██▎       | 338/1500 [00:08&lt;00:08, 139.19it/s, 15 steps of size 4.42e-01. acc. prob=0.78]warmup:  24%|██▎       | 356/1500 [00:08&lt;00:07, 150.15it/s, 15 steps of size 6.44e-01. acc. prob=0.79]warmup:  25%|██▌       | 376/1500 [00:09&lt;00:06, 162.34it/s, 7 steps of size 6.63e-01. acc. prob=0.79] warmup:  27%|██▋       | 398/1500 [00:09&lt;00:06, 177.06it/s, 15 steps of size 6.08e-01. acc. prob=0.79]warmup:  28%|██▊       | 419/1500 [00:09&lt;00:05, 184.49it/s, 7 steps of size 5.56e-01. acc. prob=0.79] warmup:  29%|██▉       | 439/1500 [00:09&lt;00:05, 188.45it/s, 7 steps of size 5.27e-01. acc. prob=0.79]warmup:  31%|███       | 460/1500 [00:09&lt;00:05, 185.21it/s, 31 steps of size 1.32e-01. acc. prob=0.79]warmup:  32%|███▏      | 479/1500 [00:09&lt;00:06, 158.48it/s, 7 steps of size 1.63e-01. acc. prob=0.79] warmup:  33%|███▎      | 496/1500 [00:09&lt;00:06, 157.43it/s, 31 steps of size 2.50e-01. acc. prob=0.79]sample:  34%|███▍      | 516/1500 [00:09&lt;00:05, 166.89it/s, 7 steps of size 3.79e-01. acc. prob=0.89] sample:  36%|███▌      | 536/1500 [00:09&lt;00:05, 174.49it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  37%|███▋      | 554/1500 [00:10&lt;00:05, 170.88it/s, 15 steps of size 3.79e-01. acc. prob=0.92]sample:  38%|███▊      | 572/1500 [00:10&lt;00:05, 169.40it/s, 7 steps of size 3.79e-01. acc. prob=0.92] sample:  39%|███▉      | 591/1500 [00:10&lt;00:05, 172.92it/s, 15 steps of size 3.79e-01. acc. prob=0.92]sample:  41%|████      | 613/1500 [00:10&lt;00:04, 183.56it/s, 15 steps of size 3.79e-01. acc. prob=0.92]sample:  42%|████▏     | 632/1500 [00:10&lt;00:04, 179.82it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  43%|████▎     | 651/1500 [00:10&lt;00:04, 175.34it/s, 7 steps of size 3.79e-01. acc. prob=0.91] sample:  45%|████▍     | 669/1500 [00:10&lt;00:04, 166.39it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  46%|████▌     | 687/1500 [00:10&lt;00:04, 170.02it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  47%|████▋     | 705/1500 [00:10&lt;00:04, 169.31it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  48%|████▊     | 725/1500 [00:11&lt;00:04, 174.11it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  50%|████▉     | 743/1500 [00:11&lt;00:04, 165.13it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  51%|█████     | 762/1500 [00:11&lt;00:04, 169.46it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  52%|█████▏    | 780/1500 [00:11&lt;00:04, 169.21it/s, 7 steps of size 3.79e-01. acc. prob=0.91] sample:  53%|█████▎    | 799/1500 [00:11&lt;00:04, 174.57it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  54%|█████▍    | 817/1500 [00:11&lt;00:03, 174.16it/s, 7 steps of size 3.79e-01. acc. prob=0.91] sample:  56%|█████▌    | 836/1500 [00:11&lt;00:03, 175.93it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  57%|█████▋    | 856/1500 [00:11&lt;00:03, 181.69it/s, 7 steps of size 3.79e-01. acc. prob=0.91] sample:  58%|█████▊    | 875/1500 [00:11&lt;00:03, 174.00it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  60%|█████▉    | 896/1500 [00:12&lt;00:03, 180.66it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  61%|██████    | 915/1500 [00:12&lt;00:03, 171.70it/s, 7 steps of size 3.79e-01. acc. prob=0.91] sample:  62%|██████▏   | 933/1500 [00:12&lt;00:03, 173.28it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  63%|██████▎   | 952/1500 [00:12&lt;00:03, 177.73it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  65%|██████▍   | 970/1500 [00:12&lt;00:03, 176.27it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  66%|██████▌   | 988/1500 [00:12&lt;00:03, 169.52it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  67%|██████▋   | 1008/1500 [00:12&lt;00:02, 177.75it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  69%|██████▊   | 1029/1500 [00:12&lt;00:02, 184.83it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  70%|██████▉   | 1048/1500 [00:12&lt;00:02, 184.51it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  71%|███████   | 1067/1500 [00:12&lt;00:02, 182.82it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  72%|███████▏  | 1086/1500 [00:13&lt;00:02, 176.81it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  74%|███████▎  | 1104/1500 [00:13&lt;00:02, 171.14it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  75%|███████▍  | 1122/1500 [00:13&lt;00:02, 167.94it/s, 7 steps of size 3.79e-01. acc. prob=0.91] sample:  76%|███████▌  | 1139/1500 [00:13&lt;00:02, 167.65it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  77%|███████▋  | 1159/1500 [00:13&lt;00:01, 173.26it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  78%|███████▊  | 1177/1500 [00:13&lt;00:01, 168.10it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  80%|███████▉  | 1195/1500 [00:13&lt;00:01, 170.90it/s, 7 steps of size 3.79e-01. acc. prob=0.91] sample:  81%|████████  | 1213/1500 [00:13&lt;00:01, 166.86it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  82%|████████▏ | 1230/1500 [00:13&lt;00:01, 163.33it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  83%|████████▎ | 1247/1500 [00:14&lt;00:01, 161.32it/s, 7 steps of size 3.79e-01. acc. prob=0.91] sample:  84%|████████▍ | 1264/1500 [00:14&lt;00:01, 156.90it/s, 23 steps of size 3.79e-01. acc. prob=0.91]sample:  85%|████████▌ | 1280/1500 [00:14&lt;00:01, 153.88it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  86%|████████▋ | 1297/1500 [00:14&lt;00:01, 157.98it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  88%|████████▊ | 1313/1500 [00:14&lt;00:01, 157.21it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  89%|████████▊ | 1329/1500 [00:14&lt;00:01, 153.84it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  90%|████████▉ | 1346/1500 [00:14&lt;00:00, 157.56it/s, 7 steps of size 3.79e-01. acc. prob=0.91] sample:  91%|█████████ | 1362/1500 [00:14&lt;00:00, 152.72it/s, 7 steps of size 3.79e-01. acc. prob=0.91]sample:  92%|█████████▏| 1382/1500 [00:14&lt;00:00, 165.87it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  93%|█████████▎| 1399/1500 [00:15&lt;00:00, 154.84it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  94%|█████████▍| 1415/1500 [00:15&lt;00:00, 145.80it/s, 7 steps of size 3.79e-01. acc. prob=0.91] sample:  96%|█████████▌| 1437/1500 [00:15&lt;00:00, 162.50it/s, 15 steps of size 3.79e-01. acc. prob=0.91]sample:  97%|█████████▋| 1454/1500 [00:15&lt;00:00, 161.25it/s, 7 steps of size 3.79e-01. acc. prob=0.91] sample:  98%|█████████▊| 1471/1500 [00:15&lt;00:00, 155.86it/s, 7 steps of size 3.79e-01. acc. prob=0.92]sample:  99%|█████████▉| 1488/1500 [00:15&lt;00:00, 157.08it/s, 15 steps of size 3.79e-01. acc. prob=0.92]sample: 100%|██████████| 1500/1500 [00:15&lt;00:00, 95.48it/s, 15 steps of size 3.79e-01. acc. prob=0.92] \n  0%|          | 0/1500 [00:00&lt;?, ?it/s]warmup:   1%|          | 10/1500 [00:00&lt;00:14, 99.53it/s, 31 steps of size 1.09e-01. acc. prob=0.66]warmup:   1%|▏         | 20/1500 [00:00&lt;00:19, 77.29it/s, 7 steps of size 1.60e-02. acc. prob=0.69] warmup:   2%|▏         | 29/1500 [00:00&lt;00:18, 78.33it/s, 7 steps of size 1.02e-01. acc. prob=0.74]warmup:   3%|▎         | 41/1500 [00:00&lt;00:15, 92.28it/s, 7 steps of size 3.18e-02. acc. prob=0.74]warmup:   3%|▎         | 51/1500 [00:00&lt;00:16, 89.37it/s, 31 steps of size 1.29e-01. acc. prob=0.76]warmup:   4%|▍         | 63/1500 [00:00&lt;00:14, 96.42it/s, 15 steps of size 1.63e-01. acc. prob=0.77]warmup:   5%|▍         | 73/1500 [00:00&lt;00:14, 97.00it/s, 15 steps of size 7.66e-02. acc. prob=0.77]warmup:   6%|▌         | 83/1500 [00:00&lt;00:15, 90.70it/s, 23 steps of size 9.68e-02. acc. prob=0.77]warmup:   6%|▋         | 96/1500 [00:01&lt;00:13, 100.45it/s, 31 steps of size 7.97e-02. acc. prob=0.77]warmup:   7%|▋         | 107/1500 [00:01&lt;00:18, 76.44it/s, 63 steps of size 1.84e-01. acc. prob=0.77]warmup:   8%|▊         | 117/1500 [00:01&lt;00:16, 81.60it/s, 7 steps of size 6.26e-01. acc. prob=0.78] warmup:   9%|▊         | 129/1500 [00:01&lt;00:15, 90.35it/s, 15 steps of size 6.45e-01. acc. prob=0.78]warmup:   9%|▉         | 139/1500 [00:01&lt;00:15, 89.94it/s, 15 steps of size 6.32e-01. acc. prob=0.78]warmup:  11%|█         | 158/1500 [00:01&lt;00:12, 106.29it/s, 63 steps of size 1.02e-01. acc. prob=0.77]warmup:  11%|█▏        | 171/1500 [00:01&lt;00:11, 111.62it/s, 7 steps of size 1.55e-01. acc. prob=0.78] warmup:  12%|█▏        | 187/1500 [00:01&lt;00:10, 124.53it/s, 3 steps of size 3.11e-01. acc. prob=0.78]warmup:  13%|█▎        | 200/1500 [00:02&lt;00:10, 122.00it/s, 15 steps of size 2.48e-01. acc. prob=0.78]warmup:  14%|█▍        | 213/1500 [00:02&lt;00:10, 122.73it/s, 7 steps of size 1.10e-01. acc. prob=0.78] warmup:  15%|█▌        | 226/1500 [00:02&lt;00:10, 116.77it/s, 7 steps of size 3.86e-01. acc. prob=0.78]warmup:  16%|█▋        | 247/1500 [00:02&lt;00:08, 141.27it/s, 7 steps of size 7.61e-01. acc. prob=0.78]warmup:  17%|█▋        | 262/1500 [00:02&lt;00:08, 141.72it/s, 7 steps of size 5.45e-01. acc. prob=0.78]warmup:  18%|█▊        | 277/1500 [00:02&lt;00:09, 134.83it/s, 7 steps of size 1.68e-01. acc. prob=0.78]warmup:  19%|█▉        | 291/1500 [00:02&lt;00:09, 132.85it/s, 7 steps of size 5.37e-01. acc. prob=0.78]warmup:  20%|██        | 307/1500 [00:02&lt;00:08, 139.98it/s, 7 steps of size 4.89e-01. acc. prob=0.78]warmup:  22%|██▏       | 324/1500 [00:02&lt;00:08, 146.01it/s, 15 steps of size 3.66e-01. acc. prob=0.78]warmup:  23%|██▎       | 339/1500 [00:02&lt;00:07, 147.00it/s, 31 steps of size 2.54e-01. acc. prob=0.78]warmup:  24%|██▍       | 363/1500 [00:03&lt;00:06, 170.67it/s, 15 steps of size 4.60e-01. acc. prob=0.78]warmup:  25%|██▌       | 382/1500 [00:03&lt;00:06, 176.04it/s, 15 steps of size 3.94e-01. acc. prob=0.79]warmup:  27%|██▋       | 402/1500 [00:03&lt;00:06, 182.98it/s, 7 steps of size 4.60e-01. acc. prob=0.79] warmup:  28%|██▊       | 424/1500 [00:03&lt;00:05, 190.78it/s, 15 steps of size 6.55e-01. acc. prob=0.79]warmup:  30%|██▉       | 447/1500 [00:03&lt;00:05, 199.35it/s, 15 steps of size 3.05e-01. acc. prob=0.79]warmup:  31%|███       | 467/1500 [00:03&lt;00:05, 187.98it/s, 7 steps of size 3.26e-01. acc. prob=0.79] warmup:  32%|███▏      | 486/1500 [00:03&lt;00:05, 174.12it/s, 7 steps of size 5.21e-01. acc. prob=0.79]sample:  34%|███▎      | 504/1500 [00:03&lt;00:05, 167.50it/s, 7 steps of size 3.81e-01. acc. prob=0.97]sample:  35%|███▍      | 524/1500 [00:03&lt;00:05, 173.19it/s, 15 steps of size 3.81e-01. acc. prob=0.94]sample:  36%|███▌      | 542/1500 [00:04&lt;00:05, 173.97it/s, 7 steps of size 3.81e-01. acc. prob=0.93] sample:  37%|███▋      | 560/1500 [00:04&lt;00:05, 171.28it/s, 7 steps of size 3.81e-01. acc. prob=0.93]sample:  39%|███▉      | 583/1500 [00:04&lt;00:04, 186.34it/s, 7 steps of size 3.81e-01. acc. prob=0.92]sample:  40%|████      | 602/1500 [00:04&lt;00:04, 182.31it/s, 7 steps of size 3.81e-01. acc. prob=0.92]sample:  41%|████▏     | 621/1500 [00:04&lt;00:04, 176.89it/s, 7 steps of size 3.81e-01. acc. prob=0.92]sample:  43%|████▎     | 640/1500 [00:04&lt;00:04, 177.84it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample:  44%|████▍     | 660/1500 [00:04&lt;00:04, 182.31it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample:  45%|████▌     | 680/1500 [00:04&lt;00:04, 186.61it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample:  47%|████▋     | 699/1500 [00:04&lt;00:04, 179.31it/s, 7 steps of size 3.81e-01. acc. prob=0.92] sample:  48%|████▊     | 718/1500 [00:05&lt;00:04, 176.89it/s, 7 steps of size 3.81e-01. acc. prob=0.92]sample:  49%|████▉     | 736/1500 [00:05&lt;00:04, 177.42it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample:  50%|█████     | 754/1500 [00:05&lt;00:04, 172.73it/s, 7 steps of size 3.81e-01. acc. prob=0.92] sample:  51%|█████▏    | 772/1500 [00:05&lt;00:04, 171.50it/s, 7 steps of size 3.81e-01. acc. prob=0.91]sample:  53%|█████▎    | 791/1500 [00:05&lt;00:04, 175.48it/s, 7 steps of size 3.81e-01. acc. prob=0.91]sample:  54%|█████▍    | 809/1500 [00:05&lt;00:04, 170.14it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  55%|█████▌    | 827/1500 [00:05&lt;00:04, 165.94it/s, 7 steps of size 3.81e-01. acc. prob=0.91] sample:  56%|█████▋    | 844/1500 [00:05&lt;00:04, 147.70it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  57%|█████▋    | 861/1500 [00:05&lt;00:04, 152.38it/s, 7 steps of size 3.81e-01. acc. prob=0.91] sample:  58%|█████▊    | 877/1500 [00:06&lt;00:04, 149.70it/s, 7 steps of size 3.81e-01. acc. prob=0.91]sample:  60%|█████▉    | 896/1500 [00:06&lt;00:03, 160.19it/s, 3 steps of size 3.81e-01. acc. prob=0.91]sample:  61%|██████    | 915/1500 [00:06&lt;00:03, 167.81it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  62%|██████▏   | 933/1500 [00:06&lt;00:03, 170.08it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  63%|██████▎   | 951/1500 [00:06&lt;00:03, 170.20it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  65%|██████▍   | 969/1500 [00:06&lt;00:03, 170.68it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  66%|██████▌   | 987/1500 [00:06&lt;00:03, 168.96it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  67%|██████▋   | 1008/1500 [00:06&lt;00:02, 178.60it/s, 7 steps of size 3.81e-01. acc. prob=0.91]sample:  68%|██████▊   | 1026/1500 [00:06&lt;00:02, 177.54it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  70%|██████▉   | 1046/1500 [00:07&lt;00:02, 181.38it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  71%|███████   | 1065/1500 [00:07&lt;00:02, 164.10it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  72%|███████▏  | 1082/1500 [00:07&lt;00:02, 163.82it/s, 7 steps of size 3.81e-01. acc. prob=0.92] sample:  73%|███████▎  | 1099/1500 [00:07&lt;00:02, 152.30it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  74%|███████▍  | 1115/1500 [00:07&lt;00:02, 147.23it/s, 7 steps of size 3.81e-01. acc. prob=0.92] sample:  76%|███████▌  | 1133/1500 [00:07&lt;00:02, 155.74it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  77%|███████▋  | 1150/1500 [00:07&lt;00:02, 158.43it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample:  78%|███████▊  | 1167/1500 [00:07&lt;00:02, 160.48it/s, 7 steps of size 3.81e-01. acc. prob=0.92] sample:  79%|███████▉  | 1184/1500 [00:07&lt;00:01, 162.74it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample:  80%|████████  | 1205/1500 [00:08&lt;00:01, 174.72it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  82%|████████▏ | 1223/1500 [00:08&lt;00:01, 167.58it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  83%|████████▎ | 1241/1500 [00:08&lt;00:01, 167.62it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample:  84%|████████▍ | 1258/1500 [00:08&lt;00:01, 166.71it/s, 7 steps of size 3.81e-01. acc. prob=0.91] sample:  85%|████████▌ | 1276/1500 [00:08&lt;00:01, 169.23it/s, 7 steps of size 3.81e-01. acc. prob=0.91]sample:  86%|████████▌ | 1293/1500 [00:08&lt;00:01, 147.21it/s, 7 steps of size 3.81e-01. acc. prob=0.91]sample:  87%|████████▋ | 1309/1500 [00:08&lt;00:01, 146.50it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  88%|████████▊ | 1325/1500 [00:08&lt;00:01, 146.35it/s, 7 steps of size 3.81e-01. acc. prob=0.91] sample:  89%|████████▉ | 1342/1500 [00:08&lt;00:01, 152.64it/s, 7 steps of size 3.81e-01. acc. prob=0.92]sample:  91%|█████████ | 1358/1500 [00:09&lt;00:00, 147.93it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample:  92%|█████████▏| 1377/1500 [00:09&lt;00:00, 158.81it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  93%|█████████▎| 1394/1500 [00:09&lt;00:00, 161.82it/s, 15 steps of size 3.81e-01. acc. prob=0.91]sample:  94%|█████████▍| 1411/1500 [00:09&lt;00:00, 159.67it/s, 7 steps of size 3.81e-01. acc. prob=0.92] sample:  95%|█████████▌| 1431/1500 [00:09&lt;00:00, 168.45it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample:  97%|█████████▋| 1450/1500 [00:09&lt;00:00, 173.96it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample:  98%|█████████▊| 1468/1500 [00:09&lt;00:00, 167.71it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample:  99%|█████████▉| 1485/1500 [00:09&lt;00:00, 157.01it/s, 15 steps of size 3.81e-01. acc. prob=0.92]sample: 100%|██████████| 1500/1500 [00:09&lt;00:00, 151.82it/s, 15 steps of size 3.81e-01. acc. prob=0.92]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n  gamma[0]      0.14      0.02      0.13      0.10      0.17   2074.24      1.00\n  gamma[1]      0.08      0.02      0.08      0.04      0.11   2096.10      1.00\n  gamma[2]      0.06      0.02      0.06      0.03      0.10   1327.02      1.00\n  gamma[3]      0.06      0.02      0.06      0.03      0.09   1504.76      1.00\n  gamma[4]      0.06      0.02      0.06      0.03      0.09   1246.58      1.01\n  gamma[5]      0.06      0.02      0.06      0.03      0.09   1548.78      1.01\n  gamma[6]      0.04      0.02      0.04      0.01      0.08   1332.07      1.00\n         p      0.49      0.04      0.49      0.43      0.56   1954.63      1.00\n       phi      0.69      0.03      0.69      0.64      0.73   2292.06      1.00\n\nNumber of divergences: 11\n\n\n\n\n\n\n# generate the posterior predictive distribution for N\nsamples = mcmc.get_samples()\nz = sample_z(js_prior3, samples, capture_histories)\never_alive = z.max(axis=1) &gt; 0\nsamples['N'] = ever_alive.sum(axis=1)\n\n# create the plot\nfig, ax = plt.subplots(figsize=(5, 4), sharey=True)\n\nax.hist(samples['N'], bins=20, fc='tab:cyan', ec='w', alpha=0.99)\n\nax.set_title(r'Posterior distribution of $N$')\nax.axvline(SUPERPOPULATION_SIZE, linestyle='--', color='tab:pink', label='Truth')\nax.axvline(np.median(samples['N']), linestyle='--', color='tab:purple', label='Median')\n\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\nFigure 5: Posterior distribution of \\(N\\) with approximate prior on \\(\\gamma\\)\n\n\n\n\n\nOnce again these priors do a better job of estimating \\(N\\) than the Uniform prior.",
    "crumbs": [
      "Code",
      "NumPyro",
      "Jolly-Seber"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Philip T. Patton",
    "section": "",
    "text": "Phil is a quantitative ecologist with the Migratory Bird Center at the Smithsonian’s National Zoo and Conservation Biology Institute. For this postdoctoral appointment, he will help the National Park Service improve inventory and monitoring programs, under the guidance of J. Andrew Royle (USGS) and T. Scott Sillett (Smithsonian).\nFor his dissertation, he researched ways to improve stock assessments of non-migratory dolphins in Hawaiʻi. This included automating photo-identification of these animals, understanding how these automated tools interact with capture-recapture models, and estimating demographic parameters using cutting edge methods in capture recapture. He was a NOAA QUEST Fellow, under the Marine Mammal Research Program at the University of Hawai`i and the Cetacean Research Program at the Pacific Islands Fisheries Science Center.\nHe did his master’s with Krishna Pacifici at North Carolina State University, where he studied ways to improve estimates of species distribution, particularly when species interact and when the data contains sampling errors.\nFeel free to email me at philtpatton@gmail.com if you would like to get in touch."
  },
  {
    "objectID": "defense.html",
    "href": "defense.html",
    "title": "Dissertation Defense",
    "section": "",
    "text": "Please find a recording of my defense here.\n\nMy dissertation defense is scheduled for Friday, August 29th, 2025 at 2:30PM Hawaiian Standard Time. Please join in-person if you can! The talk will be in the Marine Science Building, Room 100, on the University of Hawaiʻi, Mānoa campus. There will be a pau hana afterwards with drinks and snacks.\nThanks to my former labmate, Dr. Brijonnay Madrigal, for putting together the following summary of the parking situation.\nZones 17 and 13 are the two locations you can park in (see map below). Zone 17 is in the structure and is a $5 flat-rate (pay at entrance kiosk) and you will just have to walk across Dole street and through lower campus to get to the Marine Sciences Building (MSB). It is about a 7-min walk from the parking structure. The other option is to park in Zone 13 in the green, visitor stalls which is closer to MSB but more expensive ($3 per 1/2 hour).\n\n\n\nVisitor parking map, with annotations by Bri\n\n\n\nRemote attendance\nTo attend remotely, click this link to follow along with the slides. (Link to slides to follow!) When I advance the slides, it will advance the slides in your browser window.\nTo see and hear me, please follow this Zoom link.\n\nJoin Zoom Meeting  https://hawaii.zoom.us/j/86718282355\nMeeting ID: 867 1828 2355 Passcode: MBIOPP"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Philip T. Patton",
    "section": "",
    "text": "Downloadable version"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Philip T. Patton",
    "section": "Education",
    "text": "Education\n\nPh.D., Marine Biology, Hawaiʻi Institute of Marine Biology, 2025\n\nChair: Lars Bejder\n\nM.S., Fisheries, Wildlife, and Conservation Biology, North Carolina State University, 2016\n\nChair: Krishna Pacifici\n\nB.S., Conservation Biology, SUNY College of Environmental Science and Forestry, 2013"
  },
  {
    "objectID": "cv.html#research-experience",
    "href": "cv.html#research-experience",
    "title": "Philip T. Patton",
    "section": "Research Experience",
    "text": "Research Experience\n\nQuantitative Ecologist, Migratory Bird Center, Smithsonian’s National Zoo and Conservation Biology Institute, 2025 - Present\nNOAA QUEST Fellow, Pacific Islands Fisheries Science Center, NOAA Fisheries, 2021 - 2025\nGraduate Research Assistant, Hawaiʻi Institute of Marine Biology, University of Hawaiʻi at Mānoa, 2021 - Present\nGraduate Research Assistant, Quantitative Ecology & Resource Management, University of Washington, 2016 - 2017\nGraduate Research Assistant, Applied Ecology, North Carolina State University, 2014 - 2016"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Philip T. Patton",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nData Analyst, Health Services, Deschutes County, 2020 - 2021\nData Analyst, Supply Chain AI & Machine Learning, Starbucks Coffee Company, 2019\nQuantitative Analyst, Seattle City Light, City of Seattle, 2017 - 2019"
  },
  {
    "objectID": "cv.html#grants-awards-and-fellowships",
    "href": "cv.html#grants-awards-and-fellowships",
    "title": "Philip T. Patton",
    "section": "Grants, Awards, and Fellowships",
    "text": "Grants, Awards, and Fellowships\n\nPeter Castro Graduate Student Research Fund, Hawai‘i Institute of Marine Biology, 2025, $1100\nColonel Willys E. & Sandina L. Lord Endowed Scholarship, Hawai‘i Institute of Marine Biology, 2024, $500\nBest Student Poster: 2nd Place, International Statistical Ecology Conference, 2024\nPeter Castro HIMB Graduate Student Support Fund - Travel, Hawaiʻi Institute of Marine Biology, 2023, $500\nLinda and Jim Collister Scholarship, Hawaiʻi Institute of Marine Biology, 2023, $1,000\nQuantitative Ecology and Socioeconomic Training Fellowship (QUEST), NOAA Fisheries, 2021 to present, $180,000\nAchievement Scholarship, University of Hawaiʻi at Mānoa, 2023, $500\nColonel Willys E. & Sandina L. Lord Endowed Scholarship, Hawaiʻi Institute of Marine Biology, 2022, $2,000\nStudent Travel Award, University of Washington, 2017, $500\nStudent and Postdoc Travel Award, University of Washington, 2017, $750\nTravel Award, University of Washington, 2017, $500\nGlobal Change Fellowship, USGS, 2015 to 2016, $12,000"
  },
  {
    "objectID": "cv.html#papers",
    "href": "cv.html#papers",
    "title": "Philip T. Patton",
    "section": "Papers",
    "text": "Papers\n\nVanderzee, A., Cheeseman, T., Patton, P.T., & Calambokidis, J. Evaluating a dorsal fin matching algorithm applied to Risso’s dolphins in the Southern California Bight. In revision\nParnell, K., Smith, C., Diaz, A., Fertitta, K., Thompson, P., Patton, P.T., Charrier, I., Robinson, S.J., Pacini, A. and Bejder, L., 2025. Underwater sound production of free-ranging Hawaiian monk seals. Royal Society Open Science, 12(11). PDF\nBrijs, J., Moore, C., Schakmann, M., Souza, T., Grellman, K., Tran, L.L., Patton, P.T., and Johansen, J.L. (2025) Eat more, often: The capacity of piscivores to meet increased energy demands in warming oceans. Science of the Total Environment, 973, 179105. PDF\nPatton, P. T., Pacifici, K., Baird, R. W., Oleson, E. M., Allen, J. B., Ashe, E., Athayde, A., Basran, C. J., Cabrera, E., Calambokidis, J., Cardoso, J., Carroll, E. L., Cesario, A., Cheney, B. J., Cheeseman, T., Corsi, E., Currie, J. J., Durban, J. W., Falcone, E. A., …, Bejder, L. (2024). Optimizing automated photo-identification for population assessments. Conservation Biology, e14436 PDF\nPatton, P. T., Cheeseman, T., Abe, K., Yamaguchi, T., Reade, W., Southerland, K., Howard, A., Oleson, E. M., Allen, J. B., Ashe, E., Athayde, A., Baird, R. W., Basran, C., Cabrera, E., Calambokidis, J., Cardoso, J., Carroll, E. L., Cesario, A., Cheney, B. J. …, Bejder, L. (2023). A deep learning approach to photo‑identification demonstrates high performance on two dozen cetacean species. Methods in Ecology and Evolution, 14, 2611–2625 PDF\nVivier, F., Wells, R.S., Hill, M.C., Yano, K.M., Bradford, A.L., Leunissen, E.M., Pacini, A., Booth, C.G., Rocho-Levine, J., Currie J.J., Patton, P.T., & Bejder, L. (2023) Quantifying the age-structure of free-ranging delphinid populations: testing the accuracy of Unoccupied Aerial System-photogrammetry. Ecology and Evolution, 13, e10082. PDF\nPatton, P. T., Pacifici, K., & Collazo, J. A. (2022) Modeling and estimating co-occurrence between the invasive Shiny Cowbird and its Puerto Rican hosts. Biological Invasions, 24, 2951–2960 PDF"
  },
  {
    "objectID": "cv.html#presentations",
    "href": "cv.html#presentations",
    "title": "Philip T. Patton",
    "section": "Presentations",
    "text": "Presentations\n\nPatton, P.T., Pacifici, K., Allen, J.B., Ashe, E., Athayde, A., Baird, R.W., …& Bejder, L. Evaluating trade‑offs between automation and bias in population assessments relying on photo‑identification. Paper presented at the Biennial Conference on the Biology of Marine Mammals in Perth, Australia. November 2024.\nFertitta, K., Diaz, A., Smith, C., Patton, P.T., Parnell, K., Charrier, I., Pacini, A., & Bejder, L. Descriptions of Hawaiian monk seal underwater vocal behavior at Lehua Rock. Poster presented at the Biennial Conference on the Biology of Marine Mammals in Perth, Australia. November 2024\nParnell, K., Fertitta, K., Diaz, A., Smith, C., Thompson, P., Patton, P.T., Charrier, I., Barbierri, M., Pacini, A., & Bejder, L. Talking story with ’īlio holo i ka uaua (Hawaiian monk seals): First descriptions of underwater sound production in free‑ranging individuals. Poster presented at the Biennial Conference on the Biology of Marine Mammals in Perth, Australia. November 2024\nPatton, P.T., Pacifici, K., Allen, J.B., Ashe, E., Athayde, A., Baird, R.W., …& Bejder, L. Evaluating trade‑offs between automation and bias in population assessments relying on photo‑identification. Poster presented at the International Statistical Ecology Conference. Swansea, Wales. July 2024. Best Student Poster: 2nd Place\nPatton, P.T. Some hierarchical and machine learning models for wildlife science. Invited talk at University of Natural Resources and Life Sciences (BOKU), Vienna, Austria. July 2023.\nPatton, P.T., Oleson, E.M., Baird, R.W., McPherson, L.M., Mahaffy, S.D., &. Bejder, L. The effect of fully automated photo‑identification on mark-recapture estimates. Paper presented at the EURING Analytical Meeting. Montpellier, France. April 2023\nPatton, P. T. & Gardner, B. Misspecifying movement models in spatial capture recapture studies. Paper presented at The Ecological Society of America Conference. Portland, OR, USA. August 2017\nPatton, P. T., Pacifici, K., & Collazo, J. A. Modeling and estimating co‑occurrence between generalist brood parasites and host communities. Paper presented at the EURING Analytical Meeting. Barcelona, Spain. June 2017\nPatton, P. T., Pacifici, K., & Collazo, J. A. Multi‑species occupancy models that incorporate false positive and false negative sampling errors. Paper presented at The Wildlife Society Conference. Raleigh, NC, USA. October 2016\nPatton, P. T., Pacifici, K., & Collazo, J. A. Joint host‑parasite occurrence models can improve predictions and reveal ecological traps. Paper presented at the International Statistical Ecology Conference. Seattle, WA, USA. July 2016"
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Philip T. Patton",
    "section": "Teaching Experience",
    "text": "Teaching Experience\n\nTeaching Assistant, Global Climate Data (MBIO 690), University of Hawaiʻi at Mānoa, Spring 2025\nTeaching Assistant, Mathematical Ecology of Marine Systems (MBIO 610), University of Hawaiʻi at Mānoa, Spring 2025\nTeaching Assistant, Introduction to Scientific Computing (MBIO 690), University of Hawaiʻi at Mānoa, Fall 2024\nTeaching Assistant, Principles of Wildlife Science (FW 453), North Carolina State, Spring 2016\nTeaching Assistant, Introduction to Probability and Statistics (APM 391), SUNY ESF, Fall 2012\nTutor, Calculus I (APM 105), Academic Support Services, SUNY ESF, 2011 to 2013"
  },
  {
    "objectID": "cv.html#professional-development",
    "href": "cv.html#professional-development",
    "title": "Philip T. Patton",
    "section": "Professional Development",
    "text": "Professional Development\n\nAn Introduction to Close-Kin Mark-Recapture, EURING Analytical Meeting\nC++ Virtual Training, NOAA Fisheries\nBayesian Model Selection and Decision Theory for Ecologists, International Statistical Ecology Conference\nFlexible Programming with NIMBLE, International Statistical Ecology Conference\nIntroduction to Structured Decision Making, National Conservation Training Center"
  },
  {
    "objectID": "cv.html#professional-service",
    "href": "cv.html#professional-service",
    "title": "Philip T. Patton",
    "section": "Professional Service",
    "text": "Professional Service\n\nReferee: Wildlife Society Bulletin, Marine Mammal Science\nMember: British Ecological Society, The Wildlife Society (biometrics working group), The Ecological Society of America (statistical ecology section)\nRepresentative to the Faculty, Marine Biology Graduate Program, University of Hawaiʻi at Mānoa\nRepresentative to the Graduate Student Organization, Marine Biology Graduate Program, University of Hawaiʻi at Mānoa"
  },
  {
    "objectID": "pymc-msom.html",
    "href": "pymc-msom.html",
    "title": "Community occupancy",
    "section": "",
    "text": "In this notebook, I explore fitting community occupancy models in PyMC. Community occupancy models are a multi-species extension of standard occupancy models. The benefit of these models is that, by treating each species as a random effect, they can estimate occupancy and detection more precisely than single species models. Further, through data augmentation, they can estimate the richness of the supercommunity, that is, the total number of species that use the study area during the surveys.",
    "crumbs": [
      "Code",
      "PyMC",
      "Community occupancy"
    ]
  },
  {
    "objectID": "pymc-msom.html#known-n",
    "href": "pymc-msom.html#known-n",
    "title": "Community occupancy",
    "section": "Known \\(N\\)",
    "text": "Known \\(N\\)\nFirst, I fit the the known \\(N\\) version of the model. The goal of this version is to estimate occurrence and detection for each species, without estimating species richness.\nThis notebook makes extensive use of the coords feature in PyMC. Coords makes it easier to incorporate the species-level effects via the multivariate normal. I use a \\(\\text{Normal}(0, 2)\\) prior for both \\(\\mu\\) parameters, and a LKJ Cholesky covariance prior for \\(\\mathbf{\\Sigma}.\\)\nThroughout the notebook, I use the nutpie sampler within PyMC. Nutpie is a NUTS sampler written in Rust, and is often faster than PyMC.\n\ncoords = {'process': ['detection', 'occurrence'],\n          'process_bis': ['detection', 'occurrence'],\n          'species': lookup}\n\nwith pm.Model(coords=coords) as known:\n\n    # priors for community-level means for detection and occurrence\n    mu = pm.Normal('mu', 0, 2, dims='process')\n\n    # prior for covariance matrix for occurrence and detection\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"chol\", n=2, eta=2.0, sd_dist=pm.Exponential.dist(1.0, shape=2)\n    )\n    cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=(\"process\", \"process_bis\"))\n\n    # species-level occurrence and detection probabilities on logit-scale\n    ab = pm.MvNormal(\"ab\", mu, chol=chol, dims=(\"species\", \"process\"))\n\n    # probability of detection. newaxis allows for broadcasting\n    a = ab[:, 0][:, np.newaxis]\n    p = pm.Deterministic(\"p\", pm.math.invlogit(a))\n\n    # probability of detection. newaxis allows for broadcasting\n    b = ab[:, 1][:, np.newaxis]\n    psi = pm.Deterministic(\"psi\", pm.math.invlogit(b))\n\n    # likelihood\n    pm.ZeroInflatedBinomial('Y', p=p, psi=psi, n=K, observed=Y)\n\npm.model_to_graphviz(known)\n\n\n\n\n\n\n\nFigure 2: Visual representation of the known \\(N\\) version of the community occupancy model.\n\n\n\n\n\n\nwith known:\n    known_idata = pm.sample(nuts_sampler='nutpie')\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.35\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.36\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.42\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.37\n                    15\n                \n            \n            \n        \n    \n\n\n\n\nmu_hat_royle = [-1.11, -1.7]\naz.plot_trace(known_idata, var_names=['mu'], figsize=(8,2),\n              lines=[(\"mu\", {}, [mu_hat_royle])]);\n\n\n\n\n\n\n\nFigure 3: Trace plots for the community level means of occupancy and abundance in the known \\(N\\) version of the BBS model. The estimates from Royle and Dorazio (2008) are shown by vertical and horizontal lines.\n\n\n\n\n\n\nsamps = az.extract(known_idata, var_names='ab')\nab_mean = samps.mean(axis=2)\n\nfig, ax = plt.subplots(figsize=(5,4))\nax.scatter(invlogit(ab_mean[:, 1]), invlogit(ab_mean[:, 0]), alpha=0.5)\nax.set_xlim((0, 1))\nax.set_xlabel('Occupancy probability')\nax.set_ylim((0, 0.8))\nax.set_ylabel('Detection probability')\nplt.show()\n\n\n\n\n\n\n\nFigure 4: Species-level probabilities of detection and occupancy.\n\n\n\n\n\nThe estimates of the community-level means is quite close to the estimates from Royle and Dorazio (2008). We can visualize the species-level probabilities of detection and occupancy. Compare with Figure 12.3 in Royle and Dorazio (2008).",
    "crumbs": [
      "Code",
      "PyMC",
      "Community occupancy"
    ]
  },
  {
    "objectID": "pymc-msom.html#unknown-n",
    "href": "pymc-msom.html#unknown-n",
    "title": "Community occupancy",
    "section": "Unknown \\(N\\)",
    "text": "Unknown \\(N\\)\nNext, I train the unknown \\(N\\) version of the model. Like many other notebooks in this series, it relies on augmenting the detection histories with all-zero histories. These represent the detection histories for species that may use the study site, but were not detected over the \\(K=11\\) surveys. I also augment the species names in the coords dict, such that we can still use the dims argument in the multivariate normal. Mirroring Royle and Dorazio (2008), I augment the history \\(M - n\\) all-zero histories, where \\(M=250\\) and \\(n\\) is the number of species detected during the survey.\n\nThe CustomDist class\nIn the notebooks for closed models, we have relied on the marginalize function from pymc_extras. In my testing, however, this function fails with this model, perhaps because of the the double layers of zero inflation (site occupation, community inclusion). As such, we will have to marginalize the discrete latent state for inclusion, \\(w,\\) ourselves. To do so, we have to create a custom distribution via PyMC’s CustomDist class.\nCustomDist requires two main inputs: logp and random. The logp is the log probability statement, which is similar to how one might define the likelihood in Stan. In this case, the logp is exactly how we would marginalize the discrete latent \\(w\\) state. In this case, we do so on the log scale with functions like log1m_exp, which adds numerical stability. The downside is that it can be a little harder to understand on this scale. In this case, I essentially provided a placeholder for the random argument, since I only ever use it for observed random variables (i.e., we never have to sample this variable). As such, it just returns zeros that are the size of x.\n\ndef logp(x, psi, n, p, omega):\n\n    rv = pm.ZeroInflatedBinomial.dist(psi=psi, n=n, p=p)\n    lp = pm.logp(rv, x)\n    lp_sum = lp.sum(axis=1)\n    lp_exp = pm.math.exp(lp_sum)\n\n    res = pm.math.switch(\n        x.sum(axis=1) &gt; 0,\n        lp_exp * omega,\n        lp_exp * omega + (1 - omega)\n    )\n\n    return pm.math.log(res)\n\ndef random(psi, n, p, omega, rng=None, size=None):\n    return np.zeros(size)\n\nThen, we can implement CustdomDist as if it’s any other distribution, with the principle difference being that we provide the two keyword arguments, logp and random.\n\nM = 250\nall_zero_history = np.zeros((M - n, J))\nY_augmented = np.row_stack((Y, all_zero_history))\n\naug_names = [f'aug{i}' for i in np.arange(M - n)]\nspp_aug = np.concatenate((lookup, aug_names))\n\ncoords = {'process': ['detection', 'occurrence'],\n          'process_bis': ['detection', 'occurrence'],\n          'species_aug': spp_aug}\n\nwith pm.Model(coords=coords) as unknown:\n\n    # priors for inclusion\n    omega = pm.Beta('omega', 0.001, 1)\n\n    # priors for community-level means for detection and occurrence\n    mu = pm.Normal('mu', 0, 2, dims='process')\n\n    # prior for covariance matrix for occurrence and detection\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"chol\", n=2, eta=2.0, sd_dist=pm.Exponential.dist(1.0, shape=2)\n    )\n    cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=(\"process\", \"process_bis\"))\n\n    # species-level occurrence and detection probabilities on logit-scale\n    ab = pm.MvNormal(\"ab\", mu, chol=chol, dims=(\"species_aug\", \"process\"))\n\n    # probability of detection\n    alpha = ab[:, 0]\n    p = pm.Deterministic(\"p\", pm.math.invlogit(alpha))\n\n    # probability of occurrence\n    beta = ab[:, 1]\n    psi = pm.Deterministic(\"psi\", pm.math.invlogit(beta))\n\n    # likelihood\n    pm.CustomDist(\n        'Y',\n        psi[:, np.newaxis],\n        K,\n        p[:, np.newaxis],\n        omega,\n        logp=logp,\n        random=random,\n        observed=Y_augmented\n    )\n\npm.model_to_graphviz(unknown)\n\n/var/folders/y8/cz021w550rbb072f7qhxyylh0000gq/T/ipykernel_17809/577304303.py:3: DeprecationWarning: `row_stack` alias is deprecated. Use `np.vstack` directly.\n  Y_augmented = np.row_stack((Y, all_zero_history))\n\n\n\n\n\n\n\n\nFigure 5: Visual representation of the unknown \\(N\\) version of the BBS model.\n\n\n\n\n\n\nwith unknown:\n    unknown_idata = pm.sample(nuts_sampler='nutpie')\n    # unknown_idata = pm.sample()\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for 15 seconds\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.25\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.26\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.25\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.29\n                    15\n                \n            \n            \n        \n    \n\n\n\nWe see some warnings about the effective sample size and the \\(\\hat{R}\\) statistic. Some of these warnings may just relate to the individual random effects.\n\naz.summary(unknown_idata, var_names=['omega', 'cov', 'mu'])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nomega\n0.526\n0.065\n0.415\n0.651\n0.004\n0.002\n250.0\n438.0\n1.01\n\n\ncov[detection, detection]\n1.255\n0.312\n0.736\n1.861\n0.017\n0.010\n310.0\n851.0\n1.03\n\n\ncov[detection, occurrence]\n1.382\n0.459\n0.582\n2.247\n0.030\n0.017\n243.0\n637.0\n1.02\n\n\ncov[occurrence, detection]\n1.382\n0.459\n0.582\n2.247\n0.030\n0.017\n243.0\n637.0\n1.02\n\n\ncov[occurrence, occurrence]\n5.066\n1.333\n2.839\n7.584\n0.093\n0.051\n215.0\n628.0\n1.01\n\n\nmu[detection]\n-2.002\n0.187\n-2.370\n-1.679\n0.012\n0.005\n239.0\n804.0\n1.02\n\n\nmu[occurrence]\n-2.044\n0.435\n-2.898\n-1.303\n0.031\n0.019\n203.0\n355.0\n1.01\n\n\n\n\n\n\n\n\nomega_hat_royle = [0.55]\naz.plot_trace(unknown_idata, var_names=['omega'], figsize=(8,2),\n              lines=[(\"omega\", {}, [omega_hat_royle])]);\n\n\n\n\n\n\n\nFigure 6: Trace plots for the inclusion parameter for the unknown \\(N\\) version of the BBS model. The estimate from Royle and Dorazio (2008) are shown by vertical and horizontal lines.\n\n\n\n\n\nI can plot the posterior distribution of species richness \\(N.\\) This is slightly more complicated than before sinc there is an additional level of zero-inflation (included and never detected or not-included) in this model compared to the occupancy model (present and never detection or not present).\n\n# relevant posterior samples\npost = az.extract(unknown_idata)\no_samps = post.omega.to_numpy()\n\n# only care about the undetected species\np_samps = post.p.to_numpy()[n:]\n\n# probability that the species was in the study area but wasn't detected\np_if_present = o_samps * binom.pmf(0, n=K, p=p_samps)\np_total = p_if_present + (1 - o_samps)\n\n# simulate the latent occurrence state for the undetected species\nZ = RNG.binomial(1, p_if_present / p_total)\n\n# simulate species richness\nnumber_undetected = Z.sum(axis=0)\nN_samps = number_undetected + n\n\n# posterior distribution\nN_hat_royle = 138\nfig, ax = plt.subplots(figsize=(6, 4))\nax.hist(N_samps, edgecolor='white', bins=20)\nax.set_xlabel('Species richness $N$')\nax.set_ylabel('Posterior samples')\nax.axvline(N_hat_royle, linestyle='--', color='C1')\nax.axvline(N_samps.mean(), linestyle='--', color='C2')\nplt.show()\n\n\n\n\n\n\n\nFigure 7: Posterior distribution of species richness from the BBS model.\n\n\n\n\n\n\n%load_ext watermark\n\n%watermark -n -u -v -iv -w\n\nLast updated: Wed Jan 14 2026\n\nPython implementation: CPython\nPython version       : 3.13.9\nIPython version      : 9.9.0\n\npandas    : 2.3.3\nmatplotlib: 3.10.8\npytensor  : 2.36.3\narviz     : 0.23.0\npymc      : 5.27.0\nseaborn   : 0.13.2\nnumpy     : 2.3.5\n\nWatermark: 2.5.0",
    "crumbs": [
      "Code",
      "PyMC",
      "Community occupancy"
    ]
  }
]