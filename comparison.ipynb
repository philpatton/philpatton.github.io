{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Model comparison\n",
        "description: Comparing closed capture-recapture models in PyMC\n",
        "author:\n",
        "  name: Philip T. Patton\n",
        "  affiliation:\n",
        "    - Marine Mammal Research Program\n",
        "    - HawaiÊ»i Institute of Marine Biology\n",
        "date: today\n",
        "bibliography: refs.bib\n",
        "execute:\n",
        "  cache: true\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "In this notebook, I demonstrate an approach to model selection in PyMC. To do so, follow the lead of @king2008, although not nearly as elegantly. They demonstrate an approach to model selection for a typical suite of closed capture-recapture models. These include the effects of behavior $b$, time $t,$ and individual heterogeneity $h$ on capture probabilities $p$. The eight models considered here are combinations of the three: $M_{0},$ $M_{t},$ $M_{b},$ $M_{tb},$ $M_{h},$ $M_{th},$ $M_{bh}$. The full model, $M_{tbh}$, is  \n",
        "$$\n",
        "\\begin{equation}\n",
        "\\text{logit} \\; p_{it} = \\mu + \\alpha_t + \\beta x_{it} + \\gamma_i,\n",
        "\\end{equation}\n",
        "$$\n",
        "where $\\mu$ is the average catchability, $\\alpha_t$ is the effect of each occasion on catchability, $\\beta$ is the behavioral effect, $x_{it}$ indicates whether the individual has been previously caught, and $\\gamma_i$ is the individual random effect such that $\\gamma_i \\sim \\text{Normal}(0,\\sigma)$. Formulating the model this way makes the other models nested subsets of the full model.\n",
        "\n",
        "Like @king2008, I use the the Moray Firth bottlenose dolphin data as a motivating example. @wilson1999 detected $n=56$ dolphins over the course of $T=17$ boat surveys between May and September 1992. They generated the capture-recapture histories by way of photo-identification, which is near and dear to my heart (and my dissertation). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# libraries \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "from pymc.distributions.dist_math import binomln, logpow\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "pal = sns.color_palette(\"Set2\")\n",
        "sns.set_palette(pal)\n",
        "\n",
        "# hyperparameters \n",
        "SEED = 808\n",
        "RNG = np.random.default_rng(SEED)\n",
        "\n",
        "def augment_history(history):\n",
        "    '''Augment a capture history with all-zero histories.'''\n",
        "    \n",
        "    animals_captured, T = history.shape\n",
        "\n",
        "    # create M - n all zero histories\n",
        "    zero_history_count = M - animals_captured\n",
        "    zero_history = np.zeros((zero_history_count, T))\n",
        "\n",
        "    # tack those on to the capture history\n",
        "    augmented = np.row_stack((history, zero_history))\n",
        "\n",
        "    return augmented \n",
        "\n",
        "def get_behavior_covariate(history):\n",
        "    \n",
        "    # note the occasion when each individual was first seen\n",
        "    first_seen = (history != 0).argmax(axis=1)\n",
        "    \n",
        "    # create the covariate for the behavior effect\n",
        "    behavior_covariate = np.zeros_like(history)\n",
        "    for i, f in enumerate(first_seen):\n",
        "        behavior_covariate[i, (f + 1):] = 1\n",
        "\n",
        "    return behavior_covariate\n",
        "\n",
        "def get_occasion_covariate(history):\n",
        "\n",
        "    _, T = history.shape\n",
        "    l = []\n",
        "    for t in range(T):\n",
        "        oc = np.zeros_like(history)\n",
        "        oc[:, t] = 1\n",
        "        l.append(oc)\n",
        "\n",
        "    return np.stack(l, axis=2)\n",
        "\n",
        "def sim_N(idata):\n",
        "    \n",
        "    psi_samps = az.extract(idata).psi.values\n",
        "    p_samps = az.extract(idata).p.values\n",
        "    not_p = (1 - p_samps)\n",
        "    \n",
        "    if p_samps.ndim == 1:\n",
        "        p_included = psi_samps * (not_p) ** T \n",
        "        number_undetected = RNG.binomial(M - n, p_included)\n",
        "\n",
        "    elif p_samps.ndim == 3:\n",
        "        p_included = psi_samps * not_p.prod(axis=1)\n",
        "        number_undetected = RNG.binomial(1, p_included).sum(axis=0)\n",
        "\n",
        "    # N = n + number_undetected\n",
        "    N = RNG.binomial(M, psi_samps)\n",
        "    return N\n",
        "\n",
        "# convert the dolphin capture history from '1001001' to array\n",
        "dolphin = np.loadtxt('firth.txt', dtype=str)\n",
        "dolphin = np.array([list(map(int, d)) for d in dolphin])\n",
        "\n",
        "# augment the capture history with all zero histories\n",
        "n, T = dolphin.shape\n",
        "M = 500\n",
        "dolphin_augmented = augment_history(dolphin)\n",
        "\n",
        "# covariates for t and b\n",
        "occasion_covariate = get_occasion_covariate(dolphin_augmented)\n",
        "behavior_covariate = get_behavior_covariate(dolphin_augmented)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The discovery curve, the number of unique dolphins encountered as a function of the total number of dolphins encountered, may be flattening. This suggests that, at this point in the study, @wilson1999 may have encountered many of the unique individuals in the population."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Discovery curve for the Moray Firth bottlenose dolphin surveys [@wilson1999].'\n",
        "#| label: fig-discovery\n",
        "\n",
        "# how many dolphins have been seen?\n",
        "total_seen = dolphin.sum(axis=0).cumsum()\n",
        "\n",
        "# how many new dolphins have been seen?\n",
        "first_seen = (dolphin != 0).argmax(axis=1)\n",
        "newbies = [sum(first_seen == t) for t in range(T)]\n",
        "total_newbies = np.cumsum(newbies)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 3.5))\n",
        "ax.plot(total_seen, total_newbies)\n",
        "ax.fill_between(total_seen, total_newbies, alpha=0.2)\n",
        "ax.set_title('Discovery curve')\n",
        "ax.set_xlabel('Total dolphins')\n",
        "ax.set_ylabel('Unique dolphins')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training each model\n",
        "\n",
        "This notebook looks messier than the others, in that I train several models with little commentary along the way. In practice, it would probably be better to wrap these up into a function or a class. To complete the model, I used the following priors,\n",
        "$$\n",
        "\\begin{align}\n",
        "\\psi &\\sim \\text{Uniform}(0, 1)\\\\\n",
        "\\mu &\\sim \\text{Logistic}(0, 1) \\\\\n",
        "\\alpha_t &\\sim \\text{Normal}(0, \\sigma_{\\alpha}) \\\\\n",
        "\\beta &\\sim \\text{Normal}(0, \\sigma_{\\beta}) \\\\\n",
        "\\gamma_i &\\sim \\text{Normal}(0, \\sigma_{\\gamma}) \\\\\n",
        "\\sigma_{\\alpha} &\\sim \\text{InverseGamma}(4, 3) \\\\\n",
        "\\sigma_{\\beta} &\\sim \\text{InverseGamma}(4, 3) \\\\\n",
        "\\sigma_{\\gamma} &\\sim \\text{InverseGamma}(4, 3), \n",
        "\\end{align}\n",
        "$$\n",
        "which were also used by @king2008. Although note that I used an informative $\\text{Beta}(1, 5)$ prior for $\\psi$ in the full model (see below). I use the same `logp` seen in the occupancy and closed capture-recapture notebooks, which accounts for row-level zero-inflation. Unlike other notebooks, I did not look at the summaries or the trace plots unless the sampler indicated that it had issues during training.\n",
        "\n",
        "Throughout the notebook, I use the [nutpie](https://github.com/pymc-devs/nutpie) sampler within PyMC. Nutpie is a NUTS sampler written in Rust, and is often [faster than PyMC](https://discourse.pymc.io/t/new-sampler-library-nutpie/9765). Also, I have tweaked the sampling keyword arguments for each model, since they are a little finicky. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def logp(value, n, p, psi):\n",
        "    \n",
        "    binom = binomln(n, value) + logpow(p, value) + logpow(1 - p, n - value)\n",
        "    bin_sum = pm.math.sum(binom, axis=1)\n",
        "    bin_exp = pm.math.exp(bin_sum)\n",
        "\n",
        "    res = pm.math.switch(\n",
        "        value.sum(axis=1) > 0,\n",
        "        bin_exp * psi,\n",
        "        bin_exp * psi + (1 - psi)\n",
        "    )\n",
        "    \n",
        "    return pm.math.log(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Visual representation of model $M_{0}$.'\n",
        "#| label: fig-m0\n",
        "\n",
        "with pm.Model() as m0:\n",
        "\n",
        "    # Priors\n",
        "    # inclusion\n",
        "    psi = pm.Uniform('psi', 0, 1)  \n",
        "\n",
        "    # mean catchability \n",
        "    mu = pm.Logistic('mu', 0, 1)\n",
        "\n",
        "    # Linear model\n",
        "    mu_matrix = (np.ones((T, M)) * mu).T\n",
        "    p = pm.Deterministic('p', pm.math.invlogit(mu_matrix))\n",
        "\n",
        "    # Likelihood \n",
        "    pm.CustomDist(\n",
        "        'y',\n",
        "        1,\n",
        "        p,\n",
        "        psi,\n",
        "        logp=logp,\n",
        "        observed=dolphin_augmented\n",
        "    )\n",
        "    \n",
        "pm.model_to_graphviz(m0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with m0:\n",
        "    m0_idata = pm.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Visual representation of model $M_t$.\n",
        "#| label: fig-mt\n",
        "\n",
        "with pm.Model() as mt:\n",
        "\n",
        "    # Priors\n",
        "    # inclusion\n",
        "    psi = pm.Uniform('psi', 0, 1)  \n",
        "\n",
        "    # mean catchability \n",
        "    mu = pm.Logistic('mu', 0, 1)\n",
        "\n",
        "    # time effect\n",
        "    sigma_alpha = pm.InverseGamma('sigma_alpha', 4, 3)\n",
        "    alpha = pm.Normal('alpha', 0, pm.math.sqrt(sigma_alpha), shape=T)\n",
        "\n",
        "    # Linear model\n",
        "    # nu = mu + pm.math.dot(occasion_covariate, alpha)\n",
        "    nu = mu + alpha\n",
        "    p = pm.Deterministic('p', pm.math.invlogit(nu))\n",
        "\n",
        "    # Likelihood \n",
        "    pm.CustomDist(\n",
        "        'y',\n",
        "        1,\n",
        "        p,\n",
        "        psi,\n",
        "        logp=logp,\n",
        "        observed=dolphin_augmented\n",
        "    )\n",
        "    \n",
        "pm.model_to_graphviz(mt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with mt:\n",
        "    mt_idata = pm.sample()\n",
        "    # pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Visual representation of model $M_b$.\n",
        "#| label: fig-mb\n",
        "\n",
        "with pm.Model() as mb:\n",
        "\n",
        "    # Priors\n",
        "    # inclusion\n",
        "    psi = pm.Uniform('psi', 0, 1)  \n",
        "\n",
        "    # mean catchability \n",
        "    mu = pm.Logistic('mu', 0, 1)\n",
        "    \n",
        "    # behavior effect\n",
        "    sigma_beta = pm.InverseGamma('sigma_beta', 4, 3)\n",
        "    beta = pm.Normal('beta', 0, pm.math.sqrt(sigma_beta))\n",
        "\n",
        "    # Linear model\n",
        "    nu = mu + behavior_covariate * beta \n",
        "    p = pm.Deterministic('p', pm.math.invlogit(nu))\n",
        "\n",
        "    # Likelihood \n",
        "    pm.CustomDist(\n",
        "        'y',\n",
        "        1,\n",
        "        p,\n",
        "        psi,\n",
        "        logp=logp,\n",
        "        observed=dolphin_augmented\n",
        "    )\n",
        "    \n",
        "pm.model_to_graphviz(mb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with mb:\n",
        "    mb_idata = pm.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Visual representation of model $M_{tb}$.'\n",
        "#| label: fig-mtb\n",
        "\n",
        "with pm.Model() as mtb:\n",
        "\n",
        "    # Priors\n",
        "    # inclusion\n",
        "    psi = pm.Uniform('psi', 0, 1)  \n",
        "\n",
        "    # mean catchability \n",
        "    mu = pm.Logistic('mu', 0, 1)\n",
        "\n",
        "    # time effect\n",
        "    sigma_alpha = pm.InverseGamma('sigma_alpha', 4, 3)\n",
        "    alpha = pm.Normal('alpha', 0, pm.math.sqrt(sigma_alpha), shape=T)\n",
        "\n",
        "    # behavior effect\n",
        "    sigma_beta = pm.InverseGamma('sigma_beta', 4, 3)\n",
        "    beta = pm.Normal('beta', 0, pm.math.sqrt(sigma_beta))\n",
        "\n",
        "    # Linear model\n",
        "    nu = mu + alpha + behavior_covariate * beta\n",
        "    p = pm.Deterministic('p', pm.math.invlogit(nu))\n",
        "\n",
        "    # Likelihood \n",
        "    pm.CustomDist(\n",
        "        'y',\n",
        "        1,\n",
        "        p,\n",
        "        psi,\n",
        "        logp=logp,\n",
        "        observed=dolphin_augmented\n",
        "    )\n",
        "    \n",
        "pm.model_to_graphviz(mtb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with mtb:\n",
        "    mtb_idata = pm.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Visual representation of model $M_h$.\n",
        "#| label: fig-mh\n",
        "\n",
        "with pm.Model() as mh:\n",
        "\n",
        "    # Priors\n",
        "    # inclusion\n",
        "    psi = pm.Uniform('psi', 0, 1)  \n",
        "\n",
        "    # mean catchability \n",
        "    mu = pm.Logistic('mu', 0, 1)\n",
        "\n",
        "    # individual effect\n",
        "    sigma_gamma = pm.InverseGamma('sigma_gamma', 4, 3)\n",
        "    gamma = pm.Normal('gamma', 0, pm.math.sqrt(sigma_gamma), shape=M)\n",
        "\n",
        "    # Linear model\n",
        "    individual_effect = (np.ones((T, M)) * gamma).T\n",
        "    nu = mu + individual_effect\n",
        "    p = pm.Deterministic('p', pm.math.invlogit(nu))\n",
        "\n",
        "    # Likelihood \n",
        "    pm.CustomDist(\n",
        "        'y',\n",
        "        1,\n",
        "        p,\n",
        "        psi,\n",
        "        logp=logp,\n",
        "        observed=dolphin_augmented\n",
        "    )\n",
        "    \n",
        "pm.model_to_graphviz(mh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with mh:\n",
        "    mh_idata = pm.sample(3000, target_accept=0.99, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(mh_idata, var_names=['psi', 'mu', 'sigma_gamma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.plot_trace(mh_idata, figsize=(8, 6), var_names=['psi', 'mu', 'sigma_gamma']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Visual representation of model $M_{th}$.'\n",
        "#| label: fig-mth\n",
        "\n",
        "with pm.Model() as mth:\n",
        "\n",
        "    # Priors\n",
        "    # inclusion\n",
        "    psi = pm.Beta('psi', 1, 1)  \n",
        "\n",
        "    # mean catchability \n",
        "    mu = pm.Logistic('mu', 0, 1)\n",
        "\n",
        "    # time effect\n",
        "    sigma_alpha = pm.InverseGamma('sigma_alpha', 4, 3)\n",
        "    alpha = pm.Normal('alpha', 0, pm.math.sqrt(sigma_alpha), shape=T)\n",
        "\n",
        "    # individual effect\n",
        "    sigma_gamma = pm.InverseGamma('sigma_gamma', 4, 3)\n",
        "    gamma = pm.Normal('gamma', 0, pm.math.sqrt(sigma_gamma), shape=M)\n",
        "\n",
        "    # Linear model\n",
        "    individual_effect = (np.ones((T, M)) * gamma).T\n",
        "    nu = mu + alpha + individual_effect\n",
        "    p = pm.Deterministic('p', pm.math.invlogit(nu))\n",
        "\n",
        "    # Likelihood \n",
        "    pm.CustomDist(\n",
        "        'y',\n",
        "        1,\n",
        "        p,\n",
        "        psi,\n",
        "        logp=logp,\n",
        "        observed=dolphin_augmented\n",
        "    )\n",
        "    \n",
        "pm.model_to_graphviz(mth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with mth:\n",
        "    mth_idata = pm.sample(draws=3000, target_accept=0.95, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(mth_idata, var_names=['psi', 'mu', 'sigma_alpha', 'sigma_gamma', 'alpha'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Trace plots for model $M_{th}$.'\n",
        "#| label: fig-mth_trace\n",
        "\n",
        "az.plot_trace(mth_idata, figsize=(8, 10),\n",
        "              var_names=['psi', 'mu', 'sigma_alpha', 'sigma_gamma', 'alpha']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Visual representation of model $M_{bh}$.'\n",
        "#| label: fig-mbh\n",
        "\n",
        "with pm.Model() as mbh:\n",
        "\n",
        "    # Priors\n",
        "    # inclusion\n",
        "    psi = pm.Beta('psi', 1, 1)  \n",
        "\n",
        "    # mean catchability \n",
        "    mu = pm.Logistic('mu', 0, 1)\n",
        "\n",
        "    # behavior effect\n",
        "    sigma_beta = pm.InverseGamma('sigma_beta', 4, 3)\n",
        "    beta = pm.Normal('beta', 0, pm.math.sqrt(sigma_beta))\n",
        "    \n",
        "    # individual effect\n",
        "    sigma_gamma = pm.InverseGamma('sigma_gamma', 4, 3)\n",
        "    gamma = pm.Normal('gamma', 0, pm.math.sqrt(sigma_gamma), shape=M)\n",
        "\n",
        "    # Linear model\n",
        "    individual_effect = (np.ones((T, M)) * gamma).T\n",
        "    nu = mu + behavior_covariate * beta + individual_effect\n",
        "    p = pm.Deterministic('p', pm.math.invlogit(nu))\n",
        "\n",
        "    # Likelihood \n",
        "    pm.CustomDist(\n",
        "        'y',\n",
        "        1,\n",
        "        p,\n",
        "        psi,\n",
        "        logp=logp,\n",
        "        observed=dolphin_augmented\n",
        "    )\n",
        "    \n",
        "pm.model_to_graphviz(mbh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with mbh:\n",
        "    mbh_idata = pm.sample(draws=3000, target_accept=0.95, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(mbh_idata, var_names=['psi', 'mu', 'beta', 'sigma_gamma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.plot_trace(mbh_idata, figsize=(8, 10),\n",
        "              var_names=['psi', 'mu', 'beta', 'sigma_beta', 'sigma_gamma']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Visual representation of model $M_{tbh}$.'\n",
        "#| label: fig-mtbh\n",
        "\n",
        "with pm.Model() as mtbh:\n",
        "\n",
        "    # Priors\n",
        "    # inclusion\n",
        "    psi = pm.Beta('psi', 1, 5)  \n",
        "\n",
        "    # mean catchability \n",
        "    mu = pm.Logistic('mu', 0, 1)\n",
        "\n",
        "    # time effect\n",
        "    sigma_alpha = pm.InverseGamma('sigma_alpha', 4, 3)\n",
        "    alpha = pm.Normal('alpha', 0, pm.math.sqrt(sigma_alpha), shape=T)\n",
        "\n",
        "    # behavior effect\n",
        "    sigma_beta = pm.InverseGamma('sigma_beta', 4, 3)\n",
        "    beta = pm.Normal('beta', 0, pm.math.sqrt(sigma_beta))\n",
        "\n",
        "    # individual effect\n",
        "    sigma_gamma = pm.InverseGamma('sigma_gamma', 4, 3)\n",
        "    gamma = pm.Normal('gamma', 0, pm.math.sqrt(sigma_gamma), shape=M)\n",
        "\n",
        "    # Linear model\n",
        "    individual_effect = (np.ones((T, M)) * gamma).T\n",
        "    nu = mu + alpha + behavior_covariate * beta + individual_effect\n",
        "    p = pm.Deterministic('p', pm.math.invlogit(nu))\n",
        "\n",
        "    # Likelihood \n",
        "    pm.CustomDist(\n",
        "        'y',\n",
        "        1,\n",
        "        p,\n",
        "        psi,\n",
        "        logp=logp,\n",
        "        observed=dolphin_augmented\n",
        "    )\n",
        "    \n",
        "pm.model_to_graphviz(mtbh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with mtbh:\n",
        "    mtbh_idata = pm.sample(draws=2000, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(mtbh_idata, \n",
        "           var_names=['psi', 'mu', 'alpha', 'beta', 'sigma_alpha', 'sigma_beta', 'sigma_gamma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Trace plots for the model with $M_{tbh}$.'\n",
        "#| label: fig-mtbh_trace\n",
        "\n",
        "az.plot_trace(mtbh_idata, figsize=(8,14),\n",
        "           var_names=['psi', 'mu', 'alpha', 'beta', 'sigma_alpha', 'sigma_beta', 'sigma_gamma']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The trace plots and summary statistics show convergence issues for many of the individual heterogeneity models. The variance parameter, $\\sigma_{\\gamma},$ seems to sample poorly. Further, models with both behavioral and individual effects lead to extremely large estimates of $\\psi$. This appears to happen regardless of the size of the data augmentation $M.$ \n",
        "\n",
        "Note that I upped the `target_accept` value for some models. This slows the sampler, but lowers the risk of divergence. \n",
        "\n",
        "# Model comparison\n",
        "\n",
        "Next, I select a model for inference using an approximation of leave-one-out (loo) cross-validation [@vehtari2017]. This approximation can be calculated using PyMC. To do so, I calculate the log-likelihood for each model, which is added to the `InferenceData` object. This makes it possible to compare the models using loo and `az.compare`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with m0:\n",
        "    pm.compute_log_likelihood(m0_idata)\n",
        "\n",
        "with mt:\n",
        "    pm.compute_log_likelihood(mt_idata)\n",
        "\n",
        "with mb:\n",
        "    pm.compute_log_likelihood(mb_idata)\n",
        "\n",
        "with mtb:\n",
        "    pm.compute_log_likelihood(mtb_idata)\n",
        "\n",
        "with mh:\n",
        "    pm.compute_log_likelihood(mh_idata)\n",
        "\n",
        "with mth:\n",
        "    pm.compute_log_likelihood(mth_idata)\n",
        "\n",
        "with mbh:\n",
        "    pm.compute_log_likelihood(mbh_idata)\n",
        "\n",
        "with mtbh:\n",
        "    pm.compute_log_likelihood(mtbh_idata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_dict = {\"m0\": m0_idata, \"mt\": mt_idata, \"mb\": mb_idata, \n",
        "              \"mtb\": mtb_idata, \"mh\": mh_idata, \"mth\": mth_idata, \n",
        "              \"mbh\": mbh_idata, \"mtbh\": mtbh_idata}\n",
        "\n",
        "comparison = az.compare(model_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The comparison tools notes issues with several of the models, suggesting a lack of robustness. Inspection of the comparison table shows that the struggling models all include the individual effect $h.$ A more thorough analysis would consider reparameterizing the model, e.g., through the non-centered parameterization. In lieu of that, I simply discard the models that fail this test and re-do the comparison with the passing models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "comparison.round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "good_dict = {\"m0\": m0_idata, \"mt\": mt_idata, \"mb\": mb_idata, \"mtb\": mtb_idata}\n",
        "good_comparison = az.compare(good_dict)\n",
        "good_comparison.round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Differences in the ELPD criteria, calculated using loo, for each model [@vehtari2017].'\n",
        "#| label: fig-comp\n",
        "\n",
        "az.plot_compare(good_comparison, figsize=(5, 4));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The comparison shows that all of the model weight belongs to two models: $M_t$ and $M_{tb}.$\n",
        "\n",
        "# Model averaged predictions\n",
        "\n",
        "Finally, we can use the model weights to simulate a weighted posterior of $N.$ To do so, I take a weighted sample of each of the posteriors of $N,$ with the weight dictated by the comparison tool. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Posteriors of $N$ from the four models under consideration (top panel), with the model averaged posterior (bottom panel).'\n",
        "#| label: fig-N\n",
        "\n",
        "posteriors = [sim_N(good_dict[model]) for model in good_dict]\n",
        "weights = [good_comparison.loc[model].weight for model in good_dict]\n",
        "sample_count = len(posteriors[0])\n",
        "\n",
        "l = []\n",
        "for w, p in zip(weights, posteriors):\n",
        "    weighted_sample = RNG.choice(p, size=int(w * sample_count))\n",
        "    l.append(weighted_sample)\n",
        "\n",
        "weighted_posterior = np.concatenate(l)\n",
        "\n",
        "fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(7, 6), sharex=True, sharey=True, tight_layout=True)\n",
        "\n",
        "pal = sns.color_palette(\"Set2\")\n",
        "\n",
        "# labs = [k for k in good_dict.keys()]\n",
        "labs = [r'$M_{0}$', r'$M_{t}$', r'$M_{b}$', r'$M_{tb}$']\n",
        "for i, p in enumerate(posteriors):\n",
        "    ax0.hist(p, color=pal[i], edgecolor='white', bins=60, alpha=0.6, label=labs[i])\n",
        "\n",
        "ax0.set_title(r'Posteriors of $N$')\n",
        "# ax1.set_title(r'Weighted posterior')\n",
        "\n",
        "ax0.set_xlim((53, 150))\n",
        "ax0.legend()\n",
        "\n",
        "ax0.set_ylabel('Number of samples')\n",
        "ax1.set_ylabel('Number of samples')\n",
        "\n",
        "ax1.hist(weighted_posterior, edgecolor='white', bins=60, alpha=0.9, color=pal[6], label='Weighted')\n",
        "ax1.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also look at the posterior densities of $p$ from Model $M_t,$ the second most weighted model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Posteriors of $p$ from model $M_t$\n",
        "#| label: fig-p\n",
        "\n",
        "p_samps = az.extract(mt_idata).p\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "\n",
        "a = 0.4\n",
        "# ax[0].set_title(\"Poisson\")\n",
        "pal = sns.color_palette('viridis', T)\n",
        "for t in range(T):\n",
        "    label_idx = t % 2\n",
        "    if label_idx == 0:\n",
        "        az.plot_dist(p_samps[t], ax=ax, color=pal[t], label=f'$t_{{{t}}}$',\n",
        "                     plot_kwargs={'linewidth':3, 'alpha': a})\n",
        "    else:\n",
        "        az.plot_dist(p_samps[t], ax=ax, color=pal[t],\n",
        "                     plot_kwargs={'linewidth':3, 'alpha': a})\n",
        "\n",
        "ax.set_title(r'Posterior densities of $p$ from $M_t$')\n",
        "ax.set_xlabel(r'$p$')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates a simple way to compare models using leave one out cross-validation (loo) and a classic example from capture-recapture. This is just one way, however, to perform model comparison using PyMC. Perhaps a more effective solution for this problem would be placing a shrinkage prior on the $\\sigma$ parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%load_ext watermark\n",
        "\n",
        "%watermark -n -u -v -iv -w  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/philtpatton/miniforge3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}