{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Occupancy models\n",
        "description: 'Static, single-species occupancy models in PyMC'\n",
        "author:\n",
        "  name: Philip T. Patton\n",
        "  affiliation:\n",
        "    - Marine Mammal Research Program\n",
        "    - Hawai ªi Institute of Marine Biology\n",
        "date: today\n",
        "bibliography: refs.bib\n",
        "jupyter: pymc_env\n",
        "---\n",
        "\n",
        "In this notebook, I demonstrate how to fit static site-occupancy models in PyMC [@royle2008, Chapter 3]. The standard site-occupancy model models binary detection/non-detection data $y_{j,k}$ for repeated surveys $k=1,2,\\dots,K$ at sites $j=1,2,\\dots,J.$ The species is present at the sites when $z_j=1,$ and absent otherwise. We assume that our probability of detecting the species given that the site is occupied is  $P(y_{j,k}|z_j=1)=p,$ and zero when the site is unoccupied. The probability of occurrence, which is typically the parameter of interest, is $P(z_{j}=1)=\\psi.$ As such, we can think of this as a zero-inflated binomial model, where\n",
        "$$\n",
        "\\begin{align}\n",
        "&y_j \\sim\n",
        "\\begin{cases}\n",
        "    0,   & \\text{if } z_j = 0 \\\\\n",
        "    \\text{Binomial}(K, p),   & \\text{if } z_j = 1\n",
        "\\end{cases} \\\\\n",
        "&z_j \\sim \\text{Bernoulli}(\\psi)\n",
        "\\end{align},\n",
        "$$\n",
        "which assumes a constant occurrence probability across sites and a constant detection probability. I start with this simple model, then add site- and visit-level covariates later.\n",
        "\n",
        "# Simulated examples\n",
        "\n",
        "To start, I demonstrate how to simulate this zero-inflated model using NumPy. Throughout this section, I use hyperparameter values similar to those of @kery2011."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# relevant libraries\n",
        "import numpy as np\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# only necessary on MacOS Sequoia\n",
        "# https://discourse.pymc.io/t/pytensor-fails-to-compile-model-after-upgrading-to-mac-os-15-4/16796/5\n",
        "import pytensor\n",
        "pytensor.config.cxx = '/usr/bin/clang++'\n",
        "\n",
        "# plotting styles\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "\n",
        "def scale(x):\n",
        "    '''Scale x: 0 is the mean and 1 is one standard deviation from the mean.'''\n",
        "    return (x - np.nanmean(x)) / np.nanstd(x)\n",
        "\n",
        "def invlogit(x):\n",
        "    '''Compute inverse logit of x.'''\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sim_y(p, z, site_count, visit_count):\n",
        "    '''Simulate detections given detection probability p and occurrence state z.'''\n",
        "    ones = np.ones((site_count, visit_count))\n",
        "    p_array = p * ones\n",
        "\n",
        "    flips = rng.binomial(1, p_array)\n",
        "    y = (flips.T * z_true).T\n",
        "\n",
        "    return y\n",
        "\n",
        "## simulation\n",
        "\n",
        "SEED = 808\n",
        "rng = np.random.default_rng(seed=SEED)\n",
        "\n",
        "# sampling characteristics\n",
        "site_count = 200\n",
        "visit_count = 3\n",
        "\n",
        "## ecological model\n",
        "\n",
        "# true parameter values\n",
        "psi_true = 0.8\n",
        "\n",
        "# simulate occurrence state\n",
        "z_true = rng.binomial(1, psi_true, size=site_count)\n",
        "\n",
        "## detection model\n",
        "\n",
        "# true parameter values\n",
        "p_true = 0.5\n",
        "\n",
        "# simulate detection\n",
        "y = sim_y(p_true, z_true, site_count, visit_count)\n",
        "\n",
        "# number of detections at each site\n",
        "y_summarized = y.sum(axis=1)\n",
        "\n",
        "# detection data at the first five sites\n",
        "y[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating parameters with PyMC\n",
        "\n",
        "Next, I use PyMC to train the occupancy model with the simulated data. First, similar to JAGS and Stan, the model must be specified using the PyMC syntax. This is done using a [context manager](https://peps.python.org/pep-0343/) in Python, essentially, a `with` statement. This creates a `Model` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with pm.Model() as constant:\n",
        "\n",
        "    # priors for the detetion and occurrence probabilities\\\n",
        "    psi = pm.Uniform('psi', 0, 1)\n",
        "    p = pm.Uniform('p', 0, 1)\n",
        "\n",
        "    # likelihood for the summarized data\n",
        "    pm.ZeroInflatedBinomial('y', p=p, psi=psi, n=visit_count,\n",
        "                            observed=y_summarized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In JAGS, the prior for $p$ would be specified as `p ~ dunif(0, 1).` The PyMC equivalent is `p = pm.Uniform('p', 0, 1)`. This could, alternatively, be specified as `p = pm.Uniform('detection probability', 0, 1)`. For the likelihood, I use PyMC's built-in `ZeroInflatedBinomial` distribution. We tell PyMC that this is an observed random variable by supplying data to the `observed` argument. PyMC also has handy tools for visualizing the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Visual representation of model $p(\\cdot)\\psi(\\cdot).$ `MarginalMixture` refers to the zero-inflated binomial distribution.\n",
        "#| label: fig-pdot\n",
        "\n",
        "pm.model_to_graphviz(constant)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now I can sample from the posterior. Again, I use the context manager, this time referring to the model by name. It's typical to name the output with `idata` because, by default, PyMC returns an object of class `InferenceData` from the Arviz package. Arviz is similar to the coda package for R."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with constant:\n",
        "    constant_idata = pm.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyMC will try to use the No-U-Turn Sampler (NUTS) whenever possible. As you can see, it samples the posterior quickly. I can plot the output using the `az.plot_trace()`, supplying the true values for $p$ and $\\psi$ for comparison. I can also look at a tabular summary using `az.summary()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Traceplots for the $p(\\cdot)\\psi(\\cdot)$ model. The true parameter values are shown by vertical and horizontal lines.\n",
        "#| label: fig-pdot_trace\n",
        "az.plot_trace(\n",
        "    constant_idata,\n",
        "    compact=True,\n",
        "    figsize=(8,4),\n",
        "    lines=[(\"psi\", {}, [psi_true]), (\"p\", {}, [p_true])]\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(constant_idata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding site covariates\n",
        "\n",
        "Next, I add in some realism by simulating a site-level covariate $x$ that affects the occurrence probability. I model this effect with a logit-linear model, i.e., $\\psi_j=\\text{logit}^{-1}(\\beta_0 + \\beta_1 x_j).$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## ecological model\n",
        "\n",
        "# true parameter values\n",
        "beta0_true = -1\n",
        "beta1_true = 3\n",
        "\n",
        "# covariates\n",
        "x = scale(rng.uniform(size=site_count))\n",
        "\n",
        "# linear model\n",
        "mu_true = beta0_true + beta1_true * x\n",
        "psi_true = invlogit(mu_true)\n",
        "\n",
        "# simulate occurrence state\n",
        "z_true = rng.binomial(1, psi_true)\n",
        "\n",
        "## detection model\n",
        "\n",
        "# true parameter values\n",
        "p_true = 0.75\n",
        "\n",
        "# simulate detection\n",
        "y = sim_y(p_true, z_true, site_count, visit_count)\n",
        "\n",
        "# vector with the number of detections at each site\n",
        "y_summarized = y.sum(axis=1)\n",
        "\n",
        "# detection data at the first five sites\n",
        "y[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, I specify the model with PyMC. Like JAGS, the random variables can be manipulated, as in a linear model with $x_j.$ These behave like NumPy arrays, meaning that vectorized operations and broadcasting are available. To monitor the output of these manipulations, use the `pm.Deterministic` class. In this case, I am monitoring the site level occurrence probability $\\psi_j.$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Visual representation of model $p(\\cdot)\\psi(x).$ `MarginalMixture` refers to the zero-inflated binomial distribution.\n",
        "#| label: fig-psix\n",
        "with pm.Model() as psix:\n",
        "\n",
        "    # occurrence process\n",
        "    # priors\n",
        "    beta0 = pm.Normal(\"beta0\", mu=0, sigma=2)\n",
        "    beta1 = pm.Normal(\"beta1\", mu=0, sigma=2)\n",
        "\n",
        "    # linear model\n",
        "    mu = beta0 + beta1 * x\n",
        "    psi = pm.Deterministic(\"psi\", pm.math.invlogit(mu))\n",
        "\n",
        "    # detection process\n",
        "    # prior\n",
        "    p = pm.Uniform('p', 0, 1)\n",
        "\n",
        "    # likelihood for the summarized data\n",
        "    pm.ZeroInflatedBinomial('y', p=p, psi=psi, n=visit_count,\n",
        "                            observed=y_summarized)\n",
        "\n",
        "pm.model_to_graphviz(psix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with psix:\n",
        "    psix_idata = pm.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Traceplots for the $p(\\cdot)\\psi(x)$ model. The true parameter values are shown by vertical and horizontal lines\n",
        "#| label: psix_trace\n",
        "az.plot_trace(\n",
        "    psix_idata,\n",
        "    figsize=(8,6),\n",
        "    var_names=['beta0', 'beta1', 'p'],\n",
        "    lines=[(\"beta0\", {}, [beta0_true]), (\"beta1\", {}, [beta1_true]),\n",
        "           ('p', {}, [p_true])]\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(psix_idata, var_names=['beta0', 'beta1', 'p'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding visit covariates\n",
        "\n",
        "Finally, I add in visit-level covariate $w_{j,k}$ that affects detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## ecological model\n",
        "\n",
        "# true parameter values\n",
        "beta0_true = -1\n",
        "beta1_true = 3\n",
        "\n",
        "# covariates\n",
        "x = scale(rng.uniform(size=site_count))\n",
        "\n",
        "# linear model\n",
        "mu_true = beta0_true + beta1_true * x\n",
        "psi_true = invlogit(mu_true)\n",
        "\n",
        "# simulate occurrence state\n",
        "z_true = rng.binomial(1, psi_true)\n",
        "\n",
        "# true parameter values\n",
        "alpha0_true = 1\n",
        "alpha1_true = -3\n",
        "\n",
        "# covariates\n",
        "w = rng.uniform(size=site_count * visit_count).reshape(site_count, visit_count)\n",
        "w = scale(w)\n",
        "\n",
        "# linear model\n",
        "nu_true = alpha0_true + alpha1_true * w\n",
        "p_true = invlogit(nu_true)\n",
        "\n",
        "# simulate detection\n",
        "y = sim_y(p_true, z_true, site_count, visit_count)\n",
        "\n",
        "print(y.shape)\n",
        "y[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have visit covariates, we can no longer rely on the `pm.ZeroInflatedBinomial` class for our likelihood because it assumes that all zeros might be come from either process (i.e., not there or not detected). In an occupancy model, however, we only assume that zeros can arrive from either process at sites where the species was never detected. And we have no way of compelling `pm.ZeroInflatedBinomial` to only consider zero inflation at certain sites.\n",
        "\n",
        "As such, we will now code the model in terms of the discrete, latent $z_i$ state, where $z_i=1$ if site $i$ is occupied. This is the canonical way to code occupancy models in WinBUGS, JAGS, and NIMBLE [@royle2008,@kery2011]. The NUTS sampler, however, does not jive with discrete latent states. As such, PyMC will assign a binary Gibbs sampler to `z` by default, which works, albeit less efficiently than the NUTS sampler. That said, PyMC will assign the other continuous parameters to the NUTS sampler, which is good!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Visual representation of the $p(w)\\psi(w)$ model.\n",
        "#| label: fig-pw\n",
        "with pm.Model() as binary_gibbs:\n",
        "\n",
        "    # occurrence process\n",
        "    # priors\n",
        "    beta0 = pm.Normal(\"beta0\", mu=0, sigma=2)\n",
        "    beta1 = pm.Normal(\"beta1\", mu=0, sigma=2)\n",
        "\n",
        "    # linear model\n",
        "    mu = beta0 + beta1 * x\n",
        "    psi = pm.Deterministic(\"psi\", pm.math.invlogit(mu))\n",
        "\n",
        "    # detection process\n",
        "    # priors\n",
        "    alpha0 = pm.Normal('alpha0', mu=0, sigma=2)\n",
        "    alpha1 = pm.Normal('alpha1', mu=0, sigma=2)\n",
        "\n",
        "    # linear model\n",
        "    nu = alpha0 + alpha1 * w\n",
        "    p = pm.Deterministic('p', pm.math.invlogit(nu))\n",
        "\n",
        "    # occupied / unoccupied state at each site\n",
        "    z = pm.Bernoulli(\"z\", psi)\n",
        "\n",
        "    # [:, None] allows us to multiply a vector across every column of a matrix\n",
        "    mu_y = z[:, None] * p\n",
        "\n",
        "    # the likelihood is now bernoulli conditional on the z state\n",
        "    pm.Bernoulli(\"y\", mu_y, observed=y)\n",
        "\n",
        "pm.model_to_graphviz(binary_gibbs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `mu_y = z[:, None] * p` notation may look strange to an R user. This is a trick that's related to NumPy's [broadcasting rules](https://numpy.org/doc/stable/user/basics.broadcasting.html). Broadcasting allows us to multiply arrays with different dimensions. In this case, we have a vector `z` that we would like to multiply against a matrix `p`, such that the first value in `z` is multiplied against every value in the first row of `p`, and so on. R does this naturally since it has slightly different broadcasting rules than NumPy. To do this in NumPy, we need to make `z` a column vector by adding a dummy dimension, hence the `[:, None]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with binary_gibbs:\n",
        "    binary_gibbs_idata = pm.sample()\n",
        "az.summary(binary_gibbs_idata, var_names=['beta0', 'beta1', 'alpha0', 'alpha1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automated Marginalization\n",
        "\n",
        "Thankfully, PyMC has a handy experimental feature called `marginalize` that automatically marginalizes out discrete latent states. This is an experimental feature, and as such should be treated with some caution. Nevertheless, I have had success with it in the most common closed Bayesian population models (e.g., occupancy, capture-recapture, and distance sampling). The PyMC team houses these experimental features in the `pymc-extras` package, which can be installed following the instructions [here](https://www.pymc.io/projects/extras/en/stable/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pymc_extras as pmx\n",
        "marginal = pmx.marginalize(binary_gibbs, [\"z\"])\n",
        "\n",
        "with marginal:\n",
        "    marginal_idata = pm.sample()\n",
        "\n",
        "az.summary(marginal_idata, var_names=['beta0', 'beta1', 'alpha0', 'alpha1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Tracepots for the $p(w)\\psi(x)$ model. The true parameter values are shown by vertical and horizontal lines\n",
        "#| label: fig-pw_trace\n",
        "az.plot_trace(\n",
        "    binary_gibbs_idata,\n",
        "    figsize=(8,6),\n",
        "    var_names=['beta0', 'beta1', 'alpha0', 'alpha1'],\n",
        "    lines=[(\"beta0\", {}, [beta0_true]), (\"beta1\", {}, [beta1_true]),\n",
        "           ('alpha0', {}, [alpha0_true]), ('alpha1', {}, [alpha1_true])]\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the marginal model produces the same estimates more quickly.\n",
        "\n",
        "# Real data example\n",
        "\n",
        "Finally, I demonstrate the model using a real data example. These data come from @henden2013, and were used as a demonstration in @hooten2019, Chapter 23. They represent detection/non-detection data of Willow Warblers from Finnmark, Norway. The $J=27$ sites were sampled $K=3$ times. Replicating the analysis in Box 23.7 in @hooten2019, I use two covariates for site: site area and willow tree height. Further, I use two covariates for visit: an indicator for the visit and willow tree height."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# read in the data\n",
        "data = pd.read_csv('PlosOne-DataFinnmark.csv')\n",
        "\n",
        "# subset the data to select willow warbler\n",
        "is_warbler = data.Species == \"Willow Warbler\"\n",
        "Y = data.loc[is_warbler, ['Y05.1', 'Y05.2', 'Y05.3']].to_numpy()\n",
        "n, J = Y.shape\n",
        "\n",
        "# generate site covariate matrix\n",
        "site_intercept = np.ones(n)\n",
        "pland = scale(data.loc[is_warbler, 'Pland']).to_numpy()\n",
        "wheight = scale(data.loc[is_warbler, 'wheight']).to_numpy()\n",
        "\n",
        "X = np.c_[site_intercept, pland, wheight]\n",
        "\n",
        "# generate visit covariate array\n",
        "visit_int = np.ones_like(Y)\n",
        "visit_wheight = np.repeat(wheight, repeats=J).reshape(n, J)\n",
        "\n",
        "# indicates which visit this is [0, 1, 2, 0, ...]\n",
        "_, visit_indicator = np.indices(Y.shape)\n",
        "visit_indicator = scale(visit_indicator)\n",
        "\n",
        "W = np.stack((visit_int, visit_indicator, visit_wheight), axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example uses an extremely handy feature of PyMC: coordinates. This allows us to specify a prior for each $\\alpha$ and $\\beta$ value in one line of code, using the `dims` argument in our prior distribution. The length of vector is implied by length of the list in `coords`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Visual representation of the willow warbler occupancy model.\n",
        "#| label: fig-warbler\n",
        "coords = {\"beta_coefs\": [\"Intercept\", \"Pland\", 'Wheight'],\n",
        "          \"alpha_coefs\": [\"Intercept\", \"Visit\", 'Wheight']}\n",
        "\n",
        "with pm.Model(coords=coords) as warbler:\n",
        "\n",
        "    # occurrence process priors\n",
        "    Beta = pm.Normal(\"Beta\", mu=0, sigma=2, dims=\"beta_coefs\")\n",
        "\n",
        "    # linear model\n",
        "    mu = pm.math.dot(X, Beta)\n",
        "    psi = pm.Deterministic(\"psi\", pm.math.invlogit(mu))\n",
        "\n",
        "    # detection process priors\n",
        "    Alpha = pm.Normal('Alpha', mu=0, sigma=2, dims='alpha_coefs')\n",
        "\n",
        "    # linear model\n",
        "    nu = pm.math.dot(W, Alpha)\n",
        "    p = pm.Deterministic('p', pm.math.invlogit(nu))\n",
        "\n",
        "    # occupied / unoccupied state at each site\n",
        "    z = pm.Bernoulli(\"z\", psi)\n",
        "\n",
        "    # [:, None] allows us to multiply a vector across every column of a matrix\n",
        "    mu_y = z[:, None] * p\n",
        "\n",
        "    # the likelihood is now bernoulli conditional on the z state\n",
        "    pm.Bernoulli(\"y\", mu_y, observed=Y)\n",
        "\n",
        "pm.model_to_graphviz(warbler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the dimensionality of the prior distributions is clear, with (3) different priors specified for each random variable in the vectors $\\alpha$ and $\\beta$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "warbler_marginal = pmx.marginalize(warbler, ['z'])\n",
        "with warbler_marginal:\n",
        "    warbler_idata = pm.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I upped the number of draw iterations to 4,000 per chain, 16,000 total, since this dataset includes real-world messiness. Nevertheless, sampling the posterior took only 6 seconds!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(warbler_idata, var_names=['Alpha', 'Beta'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I compare the parameter estimates to the ones estimated by @hooten2019, Chapter 23."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Traceplots from the willow warbler occupancy model. Estimates from @hooten2019 are shown by vertical and horizontal lines.\n",
        "#| label: fig-warbler_trace\n",
        "\n",
        "alpha_hat_hooten =  [0.04, 0.47, 0.68]\n",
        "beta_hat_hooten = [0.56, 1.92, 0.93]\n",
        "\n",
        "az.plot_trace(\n",
        "    warbler_idata,\n",
        "    figsize=(8,4),\n",
        "    var_names=['Alpha', 'Beta'],\n",
        "    lines=[(\"Alpha\", {}, [alpha_hat_hooten]),\n",
        "           (\"Beta\", {}, [beta_hat_hooten])]\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a high level of agreement between the two methods. While their algorithm was designed for teaching and interpretability, it is noteworthy that the PyMC model is 10x faster.\n",
        "\n",
        "Arviz also produces forest plots for looking at effect sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Forest plots from willow warbler occupancy model. ESS refers to the effective sample size.\n",
        "#| label: fig-warbler_forest\n",
        "az.plot_forest(warbler_idata, var_names=['Alpha', \"Beta\"],\n",
        "               figsize=(6,4),\n",
        "               hdi_prob=0.95, ess=True, combined=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model comparison\n",
        "\n",
        "PyMC also has handy tools for model comparison. I demonstrate these by fitting a model to the warbler data with a constant probability of detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Visual representaion of the warbler occupancy model with constant $p.$\n",
        "#| label: fig-warbler_pdot\n",
        "Y_sum = Y.sum(axis=1)\n",
        "\n",
        "with pm.Model(coords=coords) as warbler_constantp:\n",
        "\n",
        "    # occurrence process priors\n",
        "    Beta = pm.Normal(\"Beta\", mu=0, sigma=2, dims=\"beta_coefs\")\n",
        "\n",
        "    # linear model\n",
        "    mu = pm.math.dot(X, Beta)\n",
        "    psi = pm.Deterministic(\"psi\", pm.math.invlogit(mu))\n",
        "\n",
        "    # detection process priors\n",
        "    p = pm.Uniform('p', 0, 1)\n",
        "\n",
        "    # likelihood\n",
        "    pm.ZeroInflatedBinomial('y', p=p, psi=psi, n=J, observed=Y_sum)\n",
        "\n",
        "pm.model_to_graphviz(warbler_constantp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with warbler_constantp:\n",
        "    warbler_constantp_idata = pm.sample(4000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, I caclculate the leave-one-out (loo) cross-validation score for each model [@vehtari2017]. This involves first computing the log likelihood for each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with warbler:\n",
        "    pm.compute_log_likelihood(warbler_idata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "warbler_loo = az.loo(warbler_idata)\n",
        "\n",
        "warbler_loo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with warbler_constantp:\n",
        "    pm.compute_log_likelihood(warbler_constantp_idata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "warbler_constantp_loo = az.loo(warbler_constantp_idata)\n",
        "\n",
        "warbler_constantp_loo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Arviz has handy tools for comparing the results. First, I generate a tabular summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_comp_loo = az.compare({r\"$p(visit,wheight)$\": warbler_idata,\n",
        "                          r\"$p(\\cdot)$\": warbler_constantp_idata})\n",
        "df_comp_loo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This indicates that the $p(\\cdot)$ model is favored over the $p(visit,wheight)$ model.\n",
        "\n",
        "Arviz also generates plots for these comparisons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Comparison between the $p(visit,wheight)$ and the $p(\\cdot)$ models in terms of loo.'\n",
        "#| label: fig-warbler_comp\n",
        "az.plot_compare(df_comp_loo, insample_dev=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%load_ext watermark\n",
        "\n",
        "%watermark -n -u -v -iv -w"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "pymc_env",
      "language": "python",
      "display_name": "Python (PyMC)",
      "path": "/Users/philtpatton/Library/Jupyter/kernels/pymc_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}