{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Spatial capture-recapture\n",
        "description: Closed spatial capture-recapture models in PyMC\n",
        "author:\n",
        "  name: Philip T. Patton\n",
        "  affiliation:\n",
        "    - Marine Mammal Research Program\n",
        "    - HawaiÊ»i Institute of Marine Biology\n",
        "date: today\n",
        "bibliography: refs.bib\n",
        "jupyter: pymc_env\n",
        "execute:\n",
        "  cache: true\n",
        "---\n",
        "\n",
        "In this notebook, I show how to train spatial capture-recapture (SCR) models in PyMC. SCR expands upon traditional capture-recapture by incorporating the location of the traps in the analysis. This matters because, typically, animals that live near a particular trap are more likely to be caught in it. In doing so, SCR links individual-level processes to the population-level, expanding the scientific scope of simple designs.\n",
        "\n",
        "In this notebook, I train the simplest possible SCR model, SCR0 [@royle2013,Chapter 5], where the goal is estimating the true population size $N$. Similar to the other closed population notebooks, I do so using parameter-expanded data-augmentation (PX-DA). I also borrow the concept of the *detection function* from the distance sampling notebook.\n",
        "\n",
        "As a motivating example, I use the [ovenbird mist netting dataset](https://www.otago.ac.nz/density/examples/index.html) provided by Murray Efford via the secr package in R. The design of the study is outlined in @efford2004 and @borchers2008. In this dataset, ovenbirds were trapped in 44 mist nets over 8 to 10 consecutive days during the summers of 2005 to 2009."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Locations of the mist nets in the ovenbird dataset [@efford2004]'\n",
        "#| label: fig-nets\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import pymc_extras as pmx\n",
        "import pytensor.tensor as pt\n",
        "import seaborn as sns\n",
        "\n",
        "# only necessary on MacOS Sequoia\n",
        "# https://discourse.pymc.io/t/pytensor-fails-to-compile-model-after-upgrading-to-mac-os-15-4/16796/5\n",
        "import pytensor\n",
        "pytensor.config.cxx = '/usr/bin/clang++'\n",
        "\n",
        "# hyper parameters\n",
        "SEED = 42\n",
        "RNG = np.random.default_rng(SEED)\n",
        "BUFFER = 100\n",
        "M = 200\n",
        "\n",
        "# plotting defaults\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "plt.rcParams['axes.spines.left'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.bottom'] = False\n",
        "sns.set_palette(\"tab10\")\n",
        "\n",
        "def invlogit(x):\n",
        "    '''Inverse logit function'''\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def euclid_dist(X, S, library='np'):\n",
        "    '''Pairwise euclidian distance between points in (M, 2) and (N, 2) arrays'''\n",
        "    diff = X[np.newaxis, :, :] - S[:, np.newaxis, :]\n",
        "\n",
        "    if library == 'np':\n",
        "        return np.sqrt(np.sum(diff ** 2, axis=-1))\n",
        "\n",
        "    elif library == 'pm':\n",
        "        return pm.math.sqrt(pm.math.sum(diff ** 2, axis=-1))\n",
        "\n",
        "def half_normal(d, s, library='np'):\n",
        "    '''Half normal detection function.'''\n",
        "    if library == 'np':\n",
        "        return np.exp( - (d ** 2) / (2 * s ** 2))\n",
        "\n",
        "    elif library == 'pm':\n",
        "        return pm.math.exp( - (d ** 2) / (2 * s ** 2))\n",
        "\n",
        "def exponential(d, s, library='np'):\n",
        "    '''Negative exponential detection function.'''\n",
        "    if library == 'np':\n",
        "        return np.exp(- d / s)\n",
        "\n",
        "    elif library == 'pm':\n",
        "        return pm.math.exp(- d / s)\n",
        "\n",
        "# coordinates for each trap\n",
        "ovenbird_trap = pd.read_csv('ovenbirdtrap.txt', delimiter=' ')\n",
        "trap_count, _ = ovenbird_trap.shape\n",
        "\n",
        "# information about each trap\n",
        "trap_x = ovenbird_trap.x\n",
        "trap_y = ovenbird_trap.y\n",
        "X = ovenbird_trap[['x', 'y']].to_numpy()\n",
        "\n",
        "# define the state space around the traps\n",
        "x_max = trap_x.max() + BUFFER\n",
        "y_max = trap_y.max() + BUFFER\n",
        "x_min = trap_x.min() - BUFFER\n",
        "y_min = trap_y.min() - BUFFER\n",
        "\n",
        "# scale for plotting\n",
        "scale = (y_max - y_min) / (x_max - x_min)\n",
        "\n",
        "# plot the trap locations\n",
        "plot_width = 2\n",
        "plot_height = plot_width * scale\n",
        "fig, ax = plt.subplots(figsize=(plot_width, plot_height))\n",
        "\n",
        "# plot the traps\n",
        "ax.scatter(trap_x, trap_y, marker='x', s=40, linewidth=1.5, color='C1')\n",
        "ax.set_ylim((y_min, y_max))\n",
        "ax.set_xlim((x_min, x_max))\n",
        "\n",
        "ax.annotate(\n",
        "    '44 nets\\n30m apart', ha='center',\n",
        "    xy=(55, -150), xycoords='data', color='black',\n",
        "    xytext=(40, 30), textcoords='offset points',\n",
        "    arrowprops=dict(arrowstyle=\"->\", color='black', linewidth=1,\n",
        "                    connectionstyle=\"angle3,angleA=90,angleB=0\"))\n",
        "\n",
        "# aesthetics\n",
        "ax.set_aspect('equal')\n",
        "ax.set_title('Mist net locations')\n",
        "ax.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One difference between spatial and traditional (non-spatial) capture is the addition of the trap identifier in the capture history. Whereas a traditional capture history is `[individual, occasion]`, a spatial capture history might be `[individual, occasion, trap]`.\n",
        "\n",
        "In the ovenbird example, I ignore the `year` dimension, pooling parameters across years, which allows for better estimation of the detection parameters. My hack for doing so is treating every band/year combination as a unique individual in a combined year capture history. This is easy to implement, creates an awkward interpretation of $N$ (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ovenbird capture history\n",
        "oven_ch = pd.read_csv('ovenbirdcapt.txt', delimiter=' ')\n",
        "\n",
        "# create a unique bird/year identifier for each individual\n",
        "oven_ch['ID'] = oven_ch.groupby(['Year','Band']).ngroup()\n",
        "occasion_count = oven_ch.Day.max()\n",
        "\n",
        "# merge the datasets, making sure that traps with no detections are included\n",
        "ovenbird = (\n",
        "    ovenbird_trap.merge(oven_ch[['ID', 'Net', 'Day']], how='left')\n",
        "      [['ID', 'Day', 'Net', 'x', 'y']]\n",
        "      .sort_values('ID')\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "ovenbird.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulation\n",
        "\n",
        "Before estimating the parameters, I perform a small simulation. The simulation starts with a core idea of SCR: the *activity center*. The activity center $\\mathbf{s}_i$ is the most likely place that you'd find an individual $i$ over the course of the trapping study. In this case, I assume that activity centers are uniformly distributed across the sample space.\n",
        "\n",
        "I compute the probability of detection for individual $i$ at trap $j$ as $p_{i,j}=g_0 \\exp(-d_{i,j}^2/2\\sigma^2),$ where $g_0$ is the probability of detecting an individual when it's activity center is at the trap, $d_{i,j}$ is the euclidean distance between the trap and the activity center, and $\\sigma$ is the detection range parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# true population size\n",
        "N = 150\n",
        "\n",
        "# simulate activity centers\n",
        "S_true = RNG.uniform((x_min, y_min), (x_max, y_max), (N, 2))\n",
        "\n",
        "# true distance between the trap and the activity centers\n",
        "d_true = euclid_dist(X, S_true)\n",
        "\n",
        "# detection parameters\n",
        "g0_true = 0.025\n",
        "sigma_true = 73\n",
        "\n",
        "# simulate the number of captures at each trap for each individual\n",
        "capture_probability = g0_true * half_normal(d_true, sigma_true)\n",
        "sim_Y = RNG.binomial(occasion_count, capture_probability)\n",
        "\n",
        "# filter out undetected individuals\n",
        "was_detected = sim_Y.sum(axis=1) > 0\n",
        "sim_Y_det = sim_Y[was_detected]\n",
        "n_detected = int(was_detected.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following @royle2013, Chapter 5, I first fit the version of the model where we assume that we know the true population size. In this case, I'm only estimating the detection parameters and the activity center locations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Visual representation of the model where $N$ is known.\n",
        "#| label: fig-known\n",
        "\n",
        "# upper bound for the uniform prior on sigma\n",
        "U_SIGMA = 150\n",
        "\n",
        "with pm.Model() as known:\n",
        "\n",
        "    # priors for the activity centers\n",
        "    S = pm.Uniform('S', (x_min, y_min), (x_max, y_max), shape=(n_detected, 2))\n",
        "\n",
        "    # priors for the detection parameters\n",
        "    g0 = pm.Uniform('g0', 0, 1)\n",
        "    sigma = pm.Uniform('sigma', 0, U_SIGMA)\n",
        "\n",
        "    # probability of capture for each individual at each trap\n",
        "    distance = euclid_dist(X, S, 'pm')\n",
        "    p = pm.Deterministic('p', g0 * half_normal(distance, sigma))\n",
        "\n",
        "    # likelihood\n",
        "    pm.Binomial(\n",
        "        'y',\n",
        "        p=p,\n",
        "        n=occasion_count,\n",
        "        observed=sim_Y_det\n",
        "    )\n",
        "\n",
        "pm.model_to_graphviz(known)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with known:\n",
        "    known_idata = pm.sample(nuts_sampler='nutpie')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(known_idata, var_names=['g0', 'sigma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Trace plots for model where $N$ is known. The true parameter values are shown by vertical and horizontal lines.\n",
        "#| label: fig-known_trace\n",
        "\n",
        "az.plot_trace(\n",
        "    known_idata,\n",
        "    var_names=['g0', 'sigma'],\n",
        "    figsize=(8,4),\n",
        "    lines=[(\"g0\", {}, [g0_true]), (\"sigma\", {}, [sigma_true])]\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The trace plots show reasonable agreement between the true parameter values and the estimated values, although $g_0$ appears to be overestimated.\n",
        "\n",
        "## Ovenbird density\n",
        "\n",
        "Now, I estimate the density $D$ for the ovenbird population. Like distance sampling, SCR can robustly estimate the density of the population, regardless of the size of the state space. The difference between the model above and this one is that we use PX-DA to estimate the inclusion probability $\\psi,$ and subsequently $N.$ First, I convert the `DataFrame` to a `(n_detected, n_traps)` array of binomial counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_Y(ch):\n",
        "    '''Get a (individual_count, trap_count) array of detections.'''\n",
        "\n",
        "    # count the number of detections per individual per trap\n",
        "    detection_counts = pd.crosstab(ch.ID, ch.Net, dropna=False)\n",
        "\n",
        "    # remove the ghost nan individual\n",
        "    detection_counts = detection_counts.loc[~detection_counts.index.isna()]\n",
        "\n",
        "    Y = detection_counts.to_numpy()\n",
        "    return Y\n",
        "\n",
        "Y = get_Y(ovenbird)\n",
        "detected_count, trap_count = Y.shape\n",
        "\n",
        "# augmented spatial capture histories with all zero histories\n",
        "all_zero_history = np.zeros((M - detected_count, trap_count))\n",
        "Y_augmented = np.vstack((Y, all_zero_history))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As with the other closed models in this series, I will write the model in terms of the latent inclusion state $z_i$. Interestingly, ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Visual representation of the ovenbird model using data augmentation.\n",
        "#| label: fig-oven\n",
        "\n",
        "with pm.Model() as oven:\n",
        "\n",
        "    # Priors\n",
        "    # activity centers\n",
        "    S = pm.Uniform('S', (x_min, y_min), (x_max, y_max), shape=(M, 2))\n",
        "\n",
        "    # capture parameters\n",
        "    g0 = pm.Uniform('g0', 0, 1)\n",
        "    sigma = pm.Uniform('sigma', 0, U_SIGMA)\n",
        "\n",
        "    # inclusion probability\n",
        "    psi = pm.Beta('psi', 0.001, 1)\n",
        "\n",
        "    # compute the capture probability\n",
        "    distance = euclid_dist(X, S, 'pm')\n",
        "    p = pm.Deterministic('p', g0 * half_normal(distance, sigma))\n",
        "\n",
        "    # inclusion state\n",
        "    z = pm.Bernoulli('z', psi, shape=M)\n",
        "\n",
        "    # likelihood\n",
        "    mu_y = z[:, None] * p\n",
        "    pm.Binomial('y', p=mu_y, n=occasion_count, observed=Y_augmented)\n",
        "\n",
        "pm.model_to_graphviz(oven)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "oven_marginal = pmx.marginalize(oven, ['z'])\n",
        "with oven_marginal:\n",
        "    oven_idata = pm.sample(nuts_sampler='nutpie')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(oven_idata, var_names=['g0', 'sigma', 'psi'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Trace plots for the ovenbird model using data augmentation. Maximum likelihood estimates are shown by vertical and horizontal lines.\n",
        "#| label: fig-oven_trace\n",
        "\n",
        "g0_mle = [0.025]\n",
        "sigma_mle = [73]\n",
        "\n",
        "az.plot_trace(\n",
        "    oven_idata,\n",
        "    var_names=['g0', 'sigma'],\n",
        "    figsize=(8,4),\n",
        "    lines=[(\"g0\", {}, [g0_mle]), (\"sigma\", {}, [sigma_mle])]\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The estimates are quite close to the maximum likelihood estimates, which I estimated with the secr package in R.\n",
        "\n",
        "Finally, I estimate density $D$ using the results. As in the closed capture-recapture and distance sampling notebooks, I use the posterior samples of $\\psi$ and $M$ to sample the posterior of $N.$ This $N,$ however, has an awkward interpretation because I pooled across the years by combining all the detection histories. To get around this, I compute the average annual abundance by dividing by the total number of years in the sample. Then, I divide by the area of the state space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def sim_N(idata, n, K):\n",
        "\n",
        "    psi_samps = az.extract(idata).psi.to_numpy()\n",
        "    p_samps = az.extract(idata).p\n",
        "    p_samps_undet = p_samps[n:, :, :]\n",
        "\n",
        "    bin_probs = (1 - p_samps_undet) ** K\n",
        "    bin_prod = bin_probs.prod(axis=1)\n",
        "    p_included = (bin_prod * psi_samps) / (bin_prod * psi_samps  + (1 - psi_samps))\n",
        "\n",
        "    number_undetected = RNG.binomial(1, p_included).sum(axis=0)\n",
        "    N_samps = n + number_undetected\n",
        "\n",
        "    return N_samps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Posterior distribution of the density $D$ of ovenbirds. The maximum likelihood estimate is shown by the dotted red line.\n",
        "#| label: fig-density\n",
        "\n",
        "N_samps = sim_N(oven_idata, detected_count, occasion_count)\n",
        "\n",
        "# kludgy way of calculating avergage abundance\n",
        "year_count = 5\n",
        "average_annual_abundance = N_samps // year_count\n",
        "\n",
        "# area of the state space in terms of hectares\n",
        "ha = 100 * 100\n",
        "mask_area = (x_max - x_min) * (y_max - y_min) / ha\n",
        "\n",
        "# density\n",
        "D_samples = average_annual_abundance / mask_area\n",
        "D_mle = 1.262946\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "ax.hist(D_samples, edgecolor='white', bins=13)\n",
        "ax.axvline(D_mle, linestyle='--',color='C1')\n",
        "ax.set_xlabel('Ovenbirds per hectare')\n",
        "ax.set_ylabel('Number of samples')\n",
        "ax.text(1.4, 800, rf'$\\hat{{D}}$={D_samples.mean():.2f}', va='bottom', ha='left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes, the location of the activity centers is of interest. Below, I plot the posterior median for the activity centers for the detected individuals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Estimated activity centers for the detected individuals\n",
        "#| label: fig-activity\n",
        "\n",
        "s_samps = az.extract(oven_idata).S\n",
        "s_mean = np.median(s_samps[:detected_count], axis=2)\n",
        "\n",
        "# plot the trap locations\n",
        "plot_width = 3\n",
        "plot_height = plot_width * scale\n",
        "fig, ax = plt.subplots(figsize=(plot_width, plot_height))\n",
        "\n",
        "# plot the traps\n",
        "ax.scatter(trap_x, trap_y, marker='x', s=40, linewidth=1.5, color='C1')\n",
        "ax.set_ylim((y_min, y_max))\n",
        "ax.set_xlim((x_min, x_max))\n",
        "\n",
        "# plot the mean activity centers\n",
        "ax.scatter(s_mean[:, 0], s_mean[:, 1], marker='o', s=4, color='C0')\n",
        "\n",
        "# aesthetics\n",
        "ax.set_aspect('equal')\n",
        "ax.set_title('Estimated activity centers')\n",
        "ax.grid(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also look at the uncertainty around those estimates. Below, I plot the posterior distribution of the activity centers for two individuals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Posterior distributions for two activity centers.\n",
        "#| label: fig-activity-post\n",
        "one = 49\n",
        "one_samps = s_samps[one]\n",
        "\n",
        "two = 2\n",
        "two_samps = s_samps[two]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(plot_width, plot_height))\n",
        "\n",
        "# plot the traps\n",
        "ax.scatter(trap_x, trap_y, marker='x', s=40, linewidth=1.5, color='tab:cyan')\n",
        "ax.set_ylim((y_min, y_max))\n",
        "ax.set_xlim((x_min, x_max))\n",
        "\n",
        "# plot the distributions of the activity centers\n",
        "ax.scatter(one_samps[0], one_samps[1], marker='o', s=1, color='tab:pink', alpha=0.4)\n",
        "ax.scatter(two_samps[0], two_samps[1], marker='o', s=1, color='tab:purple', alpha=0.4)\n",
        "\n",
        "# plot the mean\n",
        "ax.scatter(one_samps[0].mean(), one_samps[1].mean(), marker='o', s=40, color='w')\n",
        "ax.scatter(two_samps[0].mean(), two_samps[1].mean(), marker='o', s=40, color='w')\n",
        "\n",
        "# add the label\n",
        "ax.text(one_samps[0].mean(), one_samps[1].mean() + 5, f'{one}', ha='center', va='bottom')\n",
        "ax.text(two_samps[0].mean(), two_samps[1].mean() + 5, f'{two}', ha='center', va='bottom')\n",
        "\n",
        "# aesthetics\n",
        "ax.set_aspect('equal')\n",
        "ax.set_title('Posterior of two activity centers')\n",
        "ax.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, I plot the posterior distribution of the detection function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Posterior distribution for the detection function. The line represents the posterior mean while the shaded area is the 96% interval.\n",
        "#| label: fig-func\n",
        "xx = np.arange(BUFFER * 2)\n",
        "\n",
        "sigma_samps = az.extract(oven_idata).sigma.values.flatten()\n",
        "g0_samps = az.extract(oven_idata).g0.values.flatten()\n",
        "\n",
        "p_samps = np.array(\n",
        "    [g * half_normal(xx, s) for g, s in zip(g0_samps, sigma_samps)]\n",
        ")\n",
        "\n",
        "p_mean = p_samps.mean(axis=0)\n",
        "p_low = np.quantile(p_samps, 0.02, axis=0)\n",
        "p_high = np.quantile(p_samps, 0.98, axis=0)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5,4))\n",
        "\n",
        "ax.plot(xx, p_mean, '-')\n",
        "ax.fill_between(xx, p_low, p_high, alpha=0.2)\n",
        "\n",
        "ax.set_title('Detection function')\n",
        "ax.set_ylabel(r'$p$')\n",
        "ax.set_xlabel(r'Distance (m)')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%load_ext watermark\n",
        "\n",
        "%watermark -n -u -v -iv -w"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "pymc_env",
      "language": "python",
      "display_name": "Python (PyMC)",
      "path": "/Users/philtpatton/Library/Jupyter/kernels/pymc_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}